{
    "0-20": " if the data on the stream is arriving too rapidly we may not want to or need to look at every stream element perhaps we can get by with just a small sample of the values in the Stream but we have to worry about two things first the sample better be unbiased but second the sampling process must preserve the answer to the query or",
    "20-40": " queries we want to ask about the data unbiased sampling is not hard but C carelessly choosing how we sample can distort the results of queries so after a discussion of the pitfalls we'll see a method that preserves the answer to a given query introducing only the noise",
    "40-60": " that comes from the fact that we're sampling at random let's take up an example Google has a stream of search queries coming in at all times we might want to know what fraction of search queries received over a period such as a month are unique",
    "60-80": " that is there's only one occurrence of that search query in the entire month many questions about the stream can be answered by sampling the stream suppose to be concrete that we randomly select on10th of the queries to be examined for example if we wanted to know what fraction of the search queries were single word queries we could",
    "80-100": " compute that fraction for for the sample and be pretty sure that it was very close to the fraction for the stream is a a whole that is over a month a on10th sample would be a billion queries of more statistically if those queries are selected at random the deviation from",
    "100-120": " the true answer will be minuscule however we want the fraction of unique queries and this query cannot be answered correctly from a random sample of the stream in fact as we shall see on the next slide there's not enough information to deduce the fraction for the entire stream",
    "120-140": " from the fraction for the sample let's do the math for the matter of unique queries first we know that there will be in the sample very close to 10% of the query occurrences of the original stream the problem is that the probability of a given query appearing to be unique in the sample gets distorted because of the",
    "140-160": " sample first suppose the query is unique in the Stream as a whole it has a on10th chance of being selected for the sample that's fine it says that the fraction of truly unique queries that make it into the sample is the same as for the whole stream if we could only count the truly unique queries in the sample we would get the right",
    "160-180": " answer however suppose a search query appears exactly twice in the whole stream the chance that the first occurrence will be selected for the sample is 10% and the chance that the second occurrence will not be selected is 90% multiply those and we have a 9% chance of this query occurrence being unique in the sample moreover the first",
    "180-200": " occurrence could not be selected but the second is selected and that's another 9% chance of a query that really occurs twice looking unique in the sample that's a total of 18% I'll let you do the calculation but a query that appears in the Stream three",
    "200-220": " times has a 24.3% chance of looking unique in the sample and in fact any query no matter how many times it appears in the original stream has at least a small chance of looking unique in the sample so when we count the number of unique queries in the sample it will be an overestimate of the true fraction of unique queries very possibly a substantial overestimate in fact there",
    "220-240": " could be no unique queries in the original and yet many in the sample and worst we just don't know from the sample whether we're looking at truly unique queries or not we may as well toss out the data and start over again if you think about it the problem was that we assumed we flipped a 10 cided coin every time a new element",
    "240-260": " arrived on the stream the consequence is that when a query occurs at several positions in the Stream we decided independently whether or not to add each one to the sample that isn't what we want we want to pick on10th of the search queries not one/ tenth of the instances of search queries in the Stream and we can make the random",
    "260-280": " decision the first time we see each search query if we kept the table of our decision for each search query we've ever seen then each each time a query appears we could look it up in the table if we F if found uh we do the same thing we did with the first occurrence of that query added to the sample or not but if",
    "280-300": " we didn't find the query in the table we flipped that tded coin to decide what to do with it and record the query and the outcome in the table that's kind of unappetizing it will be hard to manage the table and there is a lookup with each stream element",
    "300-320": " fortunately there's a much simpler way to get the same effect without storing anything okay let's pick a hash function from search queries to 10 buckets 0 through n when a search query arrives hash it if it goes to bucket zero then add it to the sample and if it goes to any of the nine other buckets do not add it to the",
    "320-340": " sample the cool thing about this approach is that all occurrences of the same query has the same bucket because the same hash function is always applied as a result we don't need to know whether the search query that just arrived has been seen before and we don't need to know what action we took we can be sure that if it did appear",
    "340-360": " before we'll do the same thing now that we did then the result of sampling this way is that one/ tenth of the queries are selected for the sample if selected then the query appears in the sample exactly as many times as it does in the Stream as a whole thus the fraction of unique queries in the sample should be exactly",
    "360-380": " as it is for the hole suppose now that we want our sample to be not a fixed fraction of the total stream but a fixed number of samples from the stream what we can do is Hash to a large number of buckets and accept for the sample not just one bucket but enough",
    "380-400": " buckets that the resulting sample just stays within the size limit if as more stream elements come in our sample gets too large we pick one one of the buckets we had been including in the sample we delete the s from the sample just those elements that has to that bucket organizing the sample Itself by bucket can make this decision process efficient",
    "400-420": " I won't go into the details so let's rethink the example we've been working with we still want a One a 10% sample of the search queries but we realize that eventually even the 10% sample will get too large so we want the ability to throw out of the sample",
    "420-440": " some fraction of its members and of course we want to do it consistently so that if one occurrence of a query is tossed then all occurrences of that same query are tossed hashing to 10 buckets is fine to get a 10% sample uh but we need to be prepared to deal with smaller fractions so we need to Hash to many more buckets",
    "440-460": " we'll pick a 100 buckets for our example but it could be a million buckets or even more as long as we're happy with a 10% samp Le then we'll accept for the sample those elements that H that hash to 10% of the buckets we could choose any 10 but let's choose 0 through 9 to be specific now suppose we're going along",
    "460-480": " and at some point the sample size gets too big so we pick one of the buckets to get rid of say bucket nine that is we delete from the sample all those elements that hash to nine while retaining those that hash to 0 through 8 implementation is simple if we",
    "480-500": " store the sample by bucket uh we just return bucket nine to available space now our sample is 9% of the stream in the future we only add to the sample new stream elements that hash to 0 through 8 now sooner or later even the 9% sample will exceed our Space Bound so we get",
    "500-520": " rid of those elements that hash to eight then seven and so on if we event If eventually even a 1% sample is too much we stuck but then we should have taken the opportunity to Hash to more buckets than a 100 uh probably lots more the idea we explained by example is",
    "520-540": " really an instance of a general idea we can see any form of data as key value pairs okay we can choose our sample by picking a random key set of the desired size and taking all key value pairs whose key falls into to the accepted set",
    "540-560": " regardless of the associated value in our example of search queries the search query itself was the key and the value is null it was not really there in general we select our sample by hashing Keys only the value is not part of the argument of the hash",
    "560-580": " function we pick an appropriate number of buckets for acceptance and we add to our sample each key value pair whose key hashes to one side of the uh to one of the accepting buckets so let's look at a simple example where picking the right key",
    "580-600": " makes all the difference okay imagine our data elements are tupos with three components an ID for some employee the department that employ employee works for and the salary of that employee for each department there is a salary range the difference between the max maximum and minimum salaries taken",
    "600-620": " over all the employees of that department again let's suppose we want to use a 10% sample of those tupal to estimate the average salary range picking 10% of the tupal at random won't work for a given Department we're likely to be missing one or both of the",
    "620-640": " employees with the minimum or maximum salary in that department thus the difference is between the Max and Min salaries in the sample for a department is likely to be too low that is we have introduced a bias toward the low side by our poor sampling strategy the right way to sample is to",
    "640-660": " treat only the department component of tupal as the key and the other two components the employee ID and salary as each part of the value in general both the key and Value parts can consist of many components if we sample this way what we're doing is sampling a subset of the departments but for each department",
    "660-680": " in the sample we get all its employee salary data and we can get the true salary range for that department when we compute the average of the ranges we might be off a little because we're sampling the ranges for some departments rather than averaging the ranges for all departments but that error is just random noise introduced by",
    "680-700": " the sampling process not a bias in One Direction or another another"
}