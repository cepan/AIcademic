{
    "0-20": " so what I haven't explained yet is how does CR select which rows and which columns to put in the to fit into the Matrix C or into the Matrix R and here's the algorithm to do this we call this algorithm the column select algorithm I'll first just explain how do we select a set of columns so the idea is the following right we are given the Matrix",
    "20-40": " a um it's some real valued Matrix um with the m rows and M columns and we are we want to select a set of C columns from our data and the way we will do this is the following first what we do is we will go and for every um we will go and we will compute for every column",
    "40-60": " X We Will We Will compute its frous Norm right so kind of um its length and we will divide that by the total length of all the columns so in some sense we are just asking how long is this column with respect to the length of all other columns and this now defines us a probability distribution right where long columns columns where the",
    "60-80": " the sum of the squared errors is Big have will have a higher P of X and then columns where the the their sum of squared values is small they will also have small P of X okay so this is now we basically compute a probability distribution over the columns and now what do we do in the steps three to five is basically we have a for loop from one",
    "80-100": " up to the C where C is the number of columns we want to select and what we do is um we pick um we pick a column um J with probability proportional to the to the length of that column and once once we select the column we we put it in the Matrix C by by taking it out and",
    "100-120": " multiplying it with a square root of c times the probability for that column being chosen okay so what does this basically mean is the following it means that all we need to do to select the columns is that we need to compute the length of every column and then we need to select C columns out of out of our Matrix where the probability of",
    "120-140": " selecting a column is proportional to the sum of the squares of the entries in that column what is important to note here is that the same column can be selected multiple times right so every time we select the columns independently from The Matrix so as the same column can be sampled multiple times the algorithm very nicely generalizes to",
    "140-160": " rows everything is the same so basically now we we um compute the length of every row uh create a probability distribution over the lens and now pick the row proportional to its length and put it into our Matrix R and we do this um in this case uh little r times and we are done so now that we have CNR U created",
    "160-180": " the question is how do we compute Matrix U and the way we will compute Matrix U is first we will Define The Matrix W to be the intersection of rows and columns that we sampled right so if I think of my columns c um and my row Matrix R so this is is C and this is R then what is",
    "180-200": " um what is Matrix W is basically the intersection of these rows right so I will only take rows and columns that appear both in CNR and this will generate me the Matrix W and then what I will do is I will perform the SVD of Matrix W this is called right so I have W and I will perform SVD on it I know I",
    "200-220": " can do this and I can do it uniquely so I will think that I'm getting Matrix X the singular value Matrix Z and the Matrix y transpose and then I will compute The Matrix U which is called the more penr pseudo inverse right and the way I do this is basically the",
    "220-240": " mathematical symbol for this is that I take my Matrix W and I want to compute this pseudo inverse so I denote that as um w+ and the way I do this is I compute this by taking my Matrix y multiplying it with z+ * X transpose where the Matrix of z+ is simp simply the",
    "240-260": " reciprocal values of nonzero singular values right so if a singular value in my Matrix Z is non zero then Z Plus is simply one over that singular value that I have there that is non zero and if the singular value is zero then I just leave the whole thing at at zero and this is called the pseudo inverse right and um",
    "260-280": " now basically we have everything we need we we took the intersection of C and R computed this pseudo inverse where basically we took the SVD of the intersection took the one over the singular values multiplied everything together and we obtained our Matrix U so now we have the complete uh CR decomposition just to",
    "280-300": " give you an example kind of more concretely I had a very kind of theoretical theorem or a very theoretical statement before here is a um a statement that's easier kind of easier to think about right so imagine the following imagine that I'm given my Matrix a i sample C columns where C is of order K * log K divided by some",
    "300-320": " little Epsilon squar I also select R rows where R is K log K ided by Epsilon squ right so I se I sample um columns I sample rows I compute my uh U to be the pseudo inverse of the W of the intersection then what turns out is that",
    "320-340": " the uh reconstruction error of CR is less than twice than 2 plus Epsilon times the reconstruction error of SVD right this is the SVD with probability 89% right so basically the theorem I had before now I simplified it putting in",
    "340-360": " some numbers and basically what this means is that SVD picks K singular vectors um CR will pick K log K so kind of just just a bit more than K and this means that its reconstruction error will be better or at worse will be two times worse than that of SVD and that will",
    "360-380": " happen with probability 98 98% right so we get near optimal right kind of at most twice as bad as SVD almost always which is a great result"
}