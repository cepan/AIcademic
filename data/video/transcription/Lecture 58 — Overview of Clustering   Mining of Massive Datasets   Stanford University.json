{
    "0-20": " hello everyone welcome back to mining of massive datasets in this lecture we are going to cover a very important topic clustering we're going to kick off our discussion of clustering by looking at some applications of clustering they'll tell you why we need clustering in the first place then we're going to look at the overview of the most common methods used for clustering the basic problem of",
    "20-40": " clustering is quite simple we have a cloud of data points here you see data points in two dimensions and we'd like to get a some understanding of the structure of the data points beyond just seeing them in the two dimensions for example it's intuitively clear by looking at these points that there are",
    "40-60": " three groups of points here there's a group of points that group together here there's another group here and there's a third group here and it looks like we have two outliers that don't fall into any of the groups the goal of clustering is to find groups like this except in much higher dimensional spaces than two dimensions",
    "60-80": " more formally given a set of points and a notion of distance between points we want to group the points together into a number of clusters or groups and you want to group them together so that members of a cluster are close or similar to each other using the notion of distance that you've defined",
    "80-100": " previously and members of different clusters are far away from each other or dissimilar from each other usually the points that we'll be dealing with will live in high dimensional space or space with thousands or hundreds of dimensions and similarity will be defined using a distance measure from from among the",
    "100-120": " recent measures that we have covered earlier such as Euclidian cosine shekhar or edit distance going back to our example here we have points in a two dimensional space and let's say our distance measure is Euclidean observe that points in the group that are highlighted are close to each other by",
    "120-140": " creating distance and are closer to each other then they are two points outside this group we will call this group a cluster similarly we have two more clusters based on the similar notion and finally we have these two outliers that are not close to any of the clusters now if that example looked easy",
    "140-160": " but in general clustering is a hard problem for example here we have a group of points and you can see that the different colors actually denote different clusters but the problem you notice is that unlike the previous example where the clusters were cleanly separated from each other in this case",
    "160-180": " the cluster is actually overlap with each other for example there are some blue points that are among the orange points here and some among the green points here and you can notice that the clusters are kind of smeared over and mix into each other it's hard to find clear boundaries between the clusters as in the previous example so these are the kinds of real problems that we have to",
    "180-200": " tackle when we deal with clustering in the real world so why is clustering hard clustering two dimensions actually looks quite easy clustering small amounts of data also looks easy and in most cases looks are actually not deceiving clustering in two dimensions or small amounts of data is actually very very easy the trouble starts when you",
    "200-220": " increase the number of dimensions many applications don't involve two but tens or hundreds or thousands or even tens of thousands of dimensions and high dimensional spaces are fundamentally different from low dimensional spaces the problem is that in high dimensional spaces almost all pairs of points are at",
    "220-240": " approximately the same distance from each other and so it's not intuitively clear how to group them together these problems will become apparent as we work with high dimensional spaces let's look at some real applications starting with skycat skycat is a catalog of 2 billion astronomical objects and each object is",
    "240-260": " represented by its radiation signature in 7 dimensions or frequency bands the problem is to take these 7 dimensional data points and cluster them into real-world objects such as galaxies stars quasars and so on which are more similar to each other than they are to other things",
    "260-280": " as a second example let's look at clustering CDs now intuitively music divides into categories and customers prefer a few categories or genres of music but what a category is really right we'd like to represent a CD by a set of",
    "280-300": " customers who bought it and we'd like to say that similar CDs have similar sets of customers and CDs naturally group or cluster together based on the customers they have for example there might be is a group of CDs that are you know there",
    "300-320": " are classical music and they have a group of customers who are classical music aficionados and there's another group of CDs that are punk rock that are bought by a different set of people another example is clustering documents we'd like to group together documents on",
    "320-340": " the same topic but what is the topic really a topic is just a set of words that appear together frequently now documents with similar set of words might be about the same topic right so we want to cluster documents based on their similarity in the space of words a",
    "340-360": " dual problem is to find topics instead of documents a topic is just a group of words that Co occur in many documents so we could instead look at words in the space of documents and cluster those rather than clustering documents in the space of words and when you do that we will find topics instead of document clusters let's talk briefly about",
    "360-380": " distance measures we've looked at various distance measures such as cosine jacquard and Euclidean and depending on the way we represent the objects of a clustering one or one or the other of these distance measures may be more appropriate for example let's look at our examples of documents or music and",
    "380-400": " different ways of representing documents lead to different distance measures we might represent a document as a set of birds or a bag of birds that appear in the document in this case the appropriate distance measure uses a jacquard distance since we are dealing with sets alternatively we can",
    "400-420": " think of a document as a point in the space of words here there is one dimension for each word and each document is an n-dimensional point a dimension I is one a word I appears in the document and zero otherwise since you've represented documents by points the natural distance measure here you see clearly in distance between the",
    "420-440": " points in space alternatively we can think of a document as a vector in the space of words a document is a vector from the origin to the vector x1 through xn where X is 1 if bird I appears in the document and 0 otherwise and when you think about documents as vectors the",
    "440-460": " natural distance measure is the angle between the vectors or the cosine distance notice that each different way of representing the same object leads naturally to a different distance measure and depending on the application a certain representation and distance measure might make more sense than",
    "460-480": " another now that we understand distance measures here is an overview of the important methods of clustering the two important methods of clustering are hierarchical methods and point assignment methods and within hierarchical methods we can either go bottom-up or top-down whatever methods or agglomerative methods start with each",
    "480-500": " point in a cluster by itself now once we have each point in a cluster by itself we repeatedly combine the two nearest clusters into a single cluster and we stop at some point and we have a set of clusters divisive or top-down methods initially place all the points in the",
    "500-520": " same cluster and then keep recursively splitting the clusters and to be end up with the desired number of clusters point assignment methods work differently in point assignment methods we always maintain a set of clusters let's say K clusters at any point in time and then we repeatedly assign a point to its nearest cluster and we proceed until each point is assigned to",
    "520-540": " some cluster you"
}