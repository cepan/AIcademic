{
    "0-20": " So, continuing the exploration of large-scale machine learning topics, today we will focus on a different algorithm called the decision tree algorithm. And the idea here is that instead of modeling the decision space between, let's say, positive and negative examples using a single line, we will learn much more complicated decision",
    "20-40": " boundaries. And this will allow us to, in a kind of much more fine-grained way, represent the differences between, let's say, positive and negative examples, if we are talking about classification. So just to remind you, what we are talking about or thinking about is that given a given attribute, let's say a wealth of a person, we want to predict the value of this attribute",
    "40-60": " by the means of some other features or attributes available to us, right? So in a sense, what we would like to do is we would like to figure out how wealthy is the person given various other characteristics of this person. And the way we can think about this is that we can think of input attributes as the features",
    "60-80": " that we know about the person. So for example, imagine that every person is described by a set of d features, x1 to x sub d. And then each feature, let's call it j, has a domain o sub j, right? When I say a domain, I mean what is the number of or what are all the different values that",
    "80-100": " this feature or this attribute can take? For example, if the feature x sub j is a favorite color of a person, this would be what we call a categorical attribute, right? It can take values red, green, blue, and so on. Another thing, for example, would be if I can have numerical features in a sense that, for example, if I ask what's the weight of that person or how old is that person, that",
    "100-120": " is nicely a number and a given variable, in this case a feature, would have a domain that is, let's say, non-negative real numbers if we are asking about a weight of a person. And now, of course, the same thing happens to what is the value of y, which is the value",
    "120-140": " that we want to predict. We will call this the dependent variable or this is the quantity we want to predict. So now our idea is the following, as kind of we started last time. We are given a set of data d, we are given a set of n, we will call them training examples,",
    "140-160": " x sub y comma y sub i, where basically the idea is that x sub i is a d-dimensional feature vector, so d dimensions from up here, and then y is simply the variable that we want to predict. And now what we want to do is we want to learn a mapping that maps the features of a given person to the wealth of a given person.",
    "160-180": " So that is our goal. And now the question is, how is this function that takes the features and transforms them into the value we want to predict, how does this function look like? So the idea will be that basically decision trees is a tree-structured plan that given a set of variables, it wants to test that set of variables and predict the output.",
    "180-200": " So to be a bit more concrete, here is an example of a decision tree. A decision tree is a tree hierarchical structure where I have two types of nodes. I have what we will call internal decision nodes, and I have prediction nodes, here denoted",
    "200-220": " as hexagons, and these three can be as deep as we want, as wide as we want, and so on. And the important thing here is that in this tree, we have the, as I mentioned, decision nodes that basically examine the value of a given attribute and ask, is this predicate satisfied, yes or no?",
    "220-240": " And then we have the internal leaf nodes, which we can call the decision nodes, which then say, OK, for this set of examples, let's predict the value. So the idea is that, as I mentioned, internal nodes have the split values. Leaf nodes make predictions. And in particular, what we will look at today is only decision nodes that are based on binary",
    "240-260": " splits. Basically, every node has at most two children, so there is a predicate or a condition, and then whether that condition is satisfied or not. So if condition is satisfied, we go to the left, and if it's not, we go to the right.",
    "260-280": " We will be talking about numerical attributes, and we will think about regression, which means y will be, let's call it, a real number. So this is the setting that we want to talk about. So the hard part here is, how do we build the tree? Now the easy part is, how do we make a prediction?",
    "280-300": " So making a prediction is very easy. So given that we already built the tree, given a training example, we want to predict what is the y value for that training example. So the idea is that we want to take this x sub i and drop it throughout the tree and",
    "300-320": " see which prediction node it ends in, and then predict that value. For example, imagine that I have a particular example that comes in here. I can first evaluate, is the first feature of this training example, does it have value less than v1? If the answer is yes, I move to the left.",
    "320-340": " I end up in the prediction node, and my predicted value is 0.42. If the condition is not satisfied and the answer is no to my condition, I would move to the right. And I would keep moving down the tree until somewhere I would hit my prediction node, and I would make a prediction for that case.",
    "340-360": " So the idea is that basically every node examines one individual feature, looks at it, and then makes a decision, is the condition satisfied or not, and then moves either to the left branch or to the right branch. So to have an idea of what decision trees are really doing and what kind of interesting decision surfaces they can find, let's look at this simple two-dimensional example where",
    "360-380": " I have a set of data points in this two-dimensional space, and imagine we want to do classification. We want to separate pluses from minuses. So the way the decision tree building procedure would start is that given this kind of training data set, we want to go and recursively kind of split this space into smaller and",
    "380-400": " smaller pieces such that each individual region in this space is uniformly populated by either all pluses or all minuses. So for example, what we could do is to say first we have our first node in the decision tree, and we want to decide what is the first split.",
    "400-420": " And imagine that we say the first split is at value v1. So we say is the value x1 of a given data point, imagine the data point here, is the value x1 less or more than the value v1? So I have v1. And now if the answer is yes, I go to the left, and otherwise I go to the right.",
    "420-440": " And now I could now go and find the second split. So I would take all the data that is down here, this is everything that kind of goes to the left, and I ask, OK, how can I now split pluses and minuses in this case? And maybe I find the new, decide to draw a line here.",
    "440-460": " So now I would have the value v2, and I can draw another decision node and ask, you know, is x2 less than v2? And then I say, is it or is it not? If it is, right, then I notice I have all the pluses. So if the value is less than v2, then here I say, yes, let's predict plus.",
    "460-480": " If the value is not less than v2, I still have this messy part. So for example, I would want to maybe split again along this dimension and split along that dimension. And similarly on the top, I would want to split here, and this way build the three throughout",
    "480-500": " to the end, where I have the prediction node. So this is kind of one idea how we can think about building a decision tree. What is now interesting here is that we have this complicated decision boundary that splits pluses and minuses from each other. And we see that we kind of have this area where there are minuses, and then we have this other area here where there are pluses.",
    "500-520": " And we see that we could never kind of separate these two classes out with a single line, but we can use decision trees to learn this more complicated decision boundary."
}