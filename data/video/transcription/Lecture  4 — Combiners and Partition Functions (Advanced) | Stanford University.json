{
    "0-20": " welcome back to mining of massive data sets in the previous lectures we studied the basic map reduce model and then we looked at how it's actually implemented in this lecture we're going to look at a couple of refinements to the basic map reduce model that can make it run a bit faster the first refinement we're going to look at is combiners now one of the",
    "20-40": " things that you may have noticed uh in the previous examples is that um you know a map task will produce many pairs of uh key value pairs with the same key for example a popular word like the will occur in millions and millions of key value pairs now remember that uh the map tasks",
    "40-60": " are actually happening in parallel on multiple worker nodes and uh the the key value pairs from each map node have to be uh shipped to uh to to reducer nodes um if you sort of Imagine a bird like the uh the uh on on node one map task",
    "60-80": " one is probably going to see uh a few thousand occurrences of the word there uh and map task 2 is going to see a few thousand occurrences of the word the and so on uh and so the output of map task one will have let's say a th tles uh with the key the and the value",
    "80-100": " one uh now all these tles will have to be shipped over to uh let's say to reduce task one now shipping a th tles over all of whose um you know keys are the and all of whose values are one uh it's a lot of network overhead and you can save some of this network overhead by doing an",
    "100-120": " intermediate sum uh in the in the map worker for example instead of sending thousand tles that each say that that each have the key the and the value one you can send a single tupple that has the key the and the value a th000 right and so you can save a lot of network bandwidth by doing a little bit of pre-aggregation",
    "120-140": " in the map worker here's a mapper um and the mapper um this is a word count example again uh the mapper it has B occurring once C occurring once D occurring once C occurring once D occurring once and B occurring once again um and now the the tle B occurs uh two times here uh in the",
    "140-160": " output of this mapper so like the combiner uh which is another function that is provided by the program um combines the two occurrences of B and produces a single tle B comma two uh which is then Shi shipped over to the",
    "160-180": " reducer so instead of two tles of the form B1 uh being shipped over to the reducer a single tle of the form B2 is shipped over to the uh reducer and this way much less data needs to be copied and and shuffled so um the the combiner is",
    "180-200": " actually also supplied by the programmer the programmer provides a function combine the input to the combiner uh is um is it's a key and a list of values um and the output is a single value so instead of a whole bunch of tuples uh with the key K being shipped off to a reducer just a single uh tupple with key",
    "200-220": " K and value V2 is shift off to the reducer um now usually the combiner is the same function as the reducer so if for example if the reducer adds up its input values uh the combiner does the same thing as well however we have to be careful because this trick of um using",
    "220-240": " the combiner uh Works only if the reduced function is commutative and associative uh let's look at a couple of examples to see what um uh what I'm saying here so for example let's say the uh reduce function is a sum function you want to add up all the input values uh as in the count example now the uh the",
    "240-260": " sum function actually is commutative and associative by which we mean um that uh A+ B is B is the same as B+ a um and a + b plus C is the same as a + B+ C this is",
    "260-280": " the first property is the commutative property and the second property is the associative property um and because some satisfies both these properties um uh some can be used as a combiner as well as a reducer what that really means is that if you have a lot of values that need to be summed uh all these values need to be",
    "280-300": " added up you can break it up into two pieces uh you you can sum up the first piece you can sum up the second piece and then you can sum up the the two intermediate results and you'll get the you'll get the proper you'll get the same same answer right so um and so this is the first combiner sums up uh the the",
    "300-320": " this first chunk of outputs the second combiner sums up the second chunk of outputs then you sum up the two intermediate values and you get the same result as if you had summed up all the original values to begin with so that trick works because sum is commutative and associative however there are some",
    "320-340": " functions that are not commutative and associative um an example might be average right let's say uh the red the reducer needs to compute the average um of of its set of input values so this so uh the set of input values uh consists of a a",
    "340-360": " key followed by a bunch of uh values um and the combiner the uh the reducer needs to find the average of the set of values now let's say we divide the set of values into two two sets",
    "360-380": " compute the average of this set compute the average of this set let's say that's average two and now we take the average of average one and average two that's the average of average one and average two now it turns out that",
    "380-400": " this is actually not the same as the average of all the values that are out there so the average function as you have seen is not commutative and associative and so you can't use it as a combiner but it turns out that you can still use the combiner trick if you're a little bit careful instead of using",
    "400-420": " average as your reduce function if the reduce function instead uh outputs you know outputs a pair which consists of sum and count okay then the average can be computed in one extra step it's just a",
    "420-440": " sum divided by the count so if if the combiner instead of sending the average of all its values let's say the key and values um and here are the",
    "440-460": " chunks now the combiner the first combiner comput the sum of this piece and the count of this piece the second combiner compute the sum of this piece and the count of this piece and the third combiner comput the sum of this",
    "460-480": " piece and the count of this piece and the finally all these U all these values the sums and the counts get shipped to the reducer and the reducer computes a final sum which is a sum of the the the intermediate sums it has has",
    "480-500": " received a final count which is a sum of all the counts that it has received and divides the sum by the count the final sum by the final count that in fact turns out to be the correct",
    "500-520": " average right so using this uh using this this trick uh of using sums and counts um it's it's sometimes possible to turn a function that's not commutative and associative break it down into functions that are commutative and associative like sum and",
    "520-540": " count and still use a combiner trick to save some Network traffic unfortunately it turns out that uh while uh While most functions are amenable to the combiner trick there are some functions that don't um work with a combin arric at all one example is uh is median right the median of a set of values is obtained by",
    "540-560": " uh sorting um you know is by obtained by sorting that set of values and then finding um the the middle the the middle value uh in that uh in that sorted list it turns out uh and it's can be proved mathematically that there is no way to um to split the uh the median",
    "560-580": " computation uh into a bunch of commutative and associative computations so you can't actually use the combiner trick if your goal is to compute the median of a set of values you just have to ship all the values to the reducer and compute the median at the reducer the next refinement we're going",
    "580-600": " to look at is is the partition function now remember that the map reduce infrastructure uses a hash function on each key in the intermediate key value Set uh and this hash function decides which which reduce node that key gets",
    "600-620": " shipped to right the map reduce system uses a default partition function which consists of hashing uh the key using a predefined hash function uh and then taking the result modulo R now this gives a number from 0 to Rus one which decides which reducer the key is sent to",
    "620-640": " now sometimes you may want to override uh this partition function with a custom partition function for example for example we might want to ensure that all the URLs from a given host let's say end up in the same output file and are therefore sent to the same reducer so",
    "640-660": " instead of hashing by key uh you might want to Hash by the host name of the URL and the map reduce uh framework allows you to specify a custom partition function that can do things like this the initial implementation of map R use was done at Google um and Google",
    "660-680": " first implemented a file system called the Google file system which is a distributed file system that provides table storage on top of its cluster and then implemented the map ruce framework on top of the Google file system Google's implementation of map ruce is not available outside of Google Hadoop is an open source project that's a",
    "680-700": " reimplementation of Google's Map ruce it uses a file system called hdfs for stable storage and it's implemented in Java Hado is an Apache project and you can freely download it from the Apache website it turns out that many use cases of Hadoop involve doing SQL like manipulations on data and so there are",
    "700-720": " open- Source implementations called Hive and pig that provide SQL like abstractions on top of the Hadoop map reduce layer so that you don't have to rewrite those as map and reduce functions let's finally wrap up by looking at map ruce in the cloud Amazon's um elastic uh compute Cloud for",
    "720-740": " example uh is one example of a service where you can rent Computing by the hour and Amazon also has an implementation of map reduce called elastic map reduce that you can run in the cloud this concludes our discussion of map ruce"
}