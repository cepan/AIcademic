{
    "0-20": " So now that we have defined our relaxed version of affiliation graph model, now the whole trick is to say, OK, how do we go and find our community affiliation matrix F, right? This is the matrix where for every node, we know what are the communities that a given node belongs to. So how do we go do this, right?",
    "20-40": " Here is our task. Given a network G, we want to estimate F, and we want to find F in such a way that it maximizes the likelihood, right? It maximizes the probability that F generated our graph G. So we have exactly what we had before. We want to find F that maximizes the probability of the graph, where of course these edge probabilities",
    "40-60": " are computed based on our matrix F, right? So probability of an edge is simply 1 minus dxE raised to the power of minus given two rows, one for node U and one for node V in our matrix.",
    "60-80": " Rather than working with this expression here, which is called the likelihood, we would like to work with a log likelihood. So basically, we take the logarithm of the likelihood, and the reason why we take the logarithm is because then all the products become summations, right? If we have a product of terms and we take the logarithm, then that simplifies to a sum",
    "80-100": " of the logarithms, right? And this is good for two reasons. First, it's kind of analytically nice to work with log likelihood. And the second kind of more important reason is that multiplying small numbers, the numerical errors start to add up and start to propagate.",
    "100-120": " If we are summing together small numbers, the numerical errors are not so severe. So working with log likelihood is always preferred over working with the row likelihood. So rather than working with probability of a graph given F, we will work with the logarithm of that probability. And given that logarithm is a monotone function, everything is still the same, OK?",
    "120-140": " So now we know what our goal is, right? Our goal is to find F that maximizes the log likelihood. What is the log likelihood? The log likelihood is simply the logarithm of this expression up here. So if I write it out, right, I have a summation over all the edges before I had a product.",
    "140-160": " Now I have a summation. This is probability of an edge, uv. And here in the second part, I have the probability of not seeing an edge. And what is nice is that minus 1 is canceled. So all I'm left with is the product of the two factors, right? So now we know the optimization problem.",
    "160-180": " Our goal is to find factor matrix F that maximizes the following log likelihood equation. Now how do we go and find the matrix F that maximizes the likelihood? What we can do is similar to what we have already seen in the classes, to basically",
    "180-200": " go and use optimization methods. In particular, we can think of this whole problem as being a continuous optimization problem. And one of the best methods to use when solving optimization problems that are continuous is to use the notion of gradient, right, to think of this as a gradient descent type of",
    "200-220": " problem, where basically what we want to do is we think of our function as having some kind of convex, smooth shape. And what we would like to do is we would like to compute the value of the gradient at the given starting point and then move into the direction of the gradient. So as you kind of ski down the slope to get to reach the minimum of that function.",
    "220-240": " In our case, we are not doing minimization, but we are doing maximization because we want to find the most likely or the best, the matrix F with the highest likelihood. So our picture looks like that. But everything still applies. We are just kind of walking up the hill, right? If you want to reach the mountain, one way to reach the top of the mountain is just to",
    "240-260": " always walk up, and eventually you'll be on the top. So that's kind of our strategy. So in order to say what is the slope at a given point, we need to compute the gradient or the derivative of the log likelihood simply. So here is the derivative of the log likelihood with respect to a given node.",
    "260-280": " This is now with respect to a given row. And it's very simple, right? I have a summation over the neighbors of a given node and a summation over the non-neighbors of a given node. And computing the derivative of the first part gives us the following expression. And then computing the derivative of the second part of the summation is even easier.",
    "280-300": " It's basically just the Fv that is surviving. One important thing here is n of u is simply the set of neighbors of a given node u. So what we could do now is to say simply, right, how do we solve this optimization problem? We can simply iterate over the rows of our matrix F. For every row, we can compute",
    "300-320": " the gradient of the log likelihood. And then we just update that given row by moving for a small direction in the direction of the increased slope, right? So the idea is that basically we compute the gradient, we compute the slope, and move in the direction of the slope.",
    "320-340": " One little detail here is that sometimes what can happen is that these membership strengths can become negative. If the membership strength kind of becomes less than 0, we just reset it back to 0. And we keep iterating this until the method stops changing F, which means we have converged to the top. What is important here to note, though, is that this is very slow.",
    "340-360": " Why is it slow? It's because computing the gradient for a given node takes linear time, meaning we have to go over all the data. And the reason for that is that we have this summation here that goes over all non-neighbors of a given node. So what this means is we have to go and we have to iterate every node in the network",
    "360-380": " to estimate the second part of the summation. So what we will rather do is to say, OK, is there a better way to compute, a faster way to compute this second part of the summation so that the whole approach can be much faster? So here is kind of a version 2.0 of this approach.",
    "380-400": " What we notice is that the summation over all the non-members, this is kind of the part that takes very long because we have to go over everyone who is not friend with our node U and sum over their factors. What we notice is the following. We can say the value of this expression is simply a summation over all the nodes, neighbors",
    "400-420": " and non-neighbors. Let's sum them together. But now, because we summed too many things together, we have to subtract the F for the node U. And we also have to subtract the F for the neighbors of U. And whatever is left are exactly the sum of the factors of nodes that are non-neighbors.",
    "420-440": " Why is this a big win? This is a big win because all we need to do is kind of compute this ahead of time. We compute this slow summation ahead of time. And then whenever we need to estimate the sum over the non-neighbors for a given node, all we have to do is subtract F of U from it and then subtract the sum of Fs over the",
    "440-460": " neighbors of a given node. So this means that rather than taking time linear in the size of the data, we need time linear in the degree of a given node. And in networks, nodes usually have relatively small degree or they connect to a small fraction of nodes in the total network.",
    "460-480": " So this makes our method much faster than the previous approach. But everything is still the same. The idea was we computed the gradient. Now that we have the gradient, we can do a simple gradient update to move up the hill and find the maximum likelihood solution, which is our matrix F.",
    "480-500": " And what the matrix F has in itself, it basically tells us for every node what communities the given node belongs to. Just to show you how quick this method is, here I'm showing you a graph where the x-axis is the size of the network, y-axis is the computation time. Here are some examples of other methods that are used today.",
    "500-520": " And you see how badly they scale, right? After a few thousands of nodes, basically the runtimes go very quickly increase, while for example, the method I was talking to you today, the BigClam method, you see that its runtimes increases much, much more slowly with the size of the network. So for example, in five minutes, we can process a network of around 300,000 nodes.",
    "520-540": " If you want a network of 100 million edges, you need to wait a day or so. And also what it turns out is that the method works great in practice. This is some of our latest research. So here is a set of papers, kind of follow-up works that you can read to know and understand",
    "540-560": " more details about this particular method. And in particular, the paper that we talked about is the paper here on the top, and it has kind of more details about the way actually the optimization procedure works and how we arrived to it."
}