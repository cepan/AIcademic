{
    "0-20": " And now, the question is, how do we compute the eigenvector or the solution to this problem, r equals n times r? So the way we proceed is the following. So the method is called the power iteration method, and it assumes that on the input, we are given a big web graph on n nodes,",
    "20-40": " where nodes are web pages, and directed links correspond to hyperlinks. And then, the power iteration is a very simple iterative scheme. The way the whole thing works is the following. We will start with our vector r. I have this subscript r of 0, which simply",
    "40-60": " means that this is measuring the time, how the iterations proceed. So our initial guess of our ranking vector r is simply that all the components of it are 1 over n, where n is the number of nodes. So naturally, the entries of r sum to 1.",
    "60-80": " So now, all we do is we iterate our recursive equation. So we say that values of r at time t plus 1 is the matrix m, the stochastic adjacency matrix, times our previous vector rt. And we keep iterating this.",
    "80-100": " And basically, all we are doing is we are iterating this r equals m times r, and we keep iterating this until r stops changing. So this means we keep iterating this until this sum of the, let's say, coordinate-wise sum of the differences between the r at the current time step and r at the previous time step is less than epsilon.",
    "100-120": " And this is really all there is to the page rank algorithm. We start with some guess of how our rank vector r is. Then, we multiply it with m, usually around 50 or 100 times, and we keep monitoring how much does r change from one iteration to another.",
    "120-140": " And when it stops changing, we stop. And what we get is the page rank scores. So of course, if we have this algorithm, the question is, how is this working? So let me just give you an example using our old web graph idea. So we have the three-node web graph.",
    "140-160": " We have our matrix m here. We have the algorithm here on the left, simple iteration, as I mentioned before. And let me show you how this would work. So for example, we start with r0, which is where the components of it are 1 3rd, 1 3rd, 1 3rd. We multiply it with m. And in the next time step, so this would be now r of 1,",
    "160-180": " we would get the new vector. And then, we now compute r at time 1 times m. We obtain r at time 2. And then, again, we would go multiply that again with m. We would get r at time 3. And we would keep doing this. And at the end, r would actually converge to a vector",
    "180-200": " that I show you here, where a and y nodes would have the importance of 6 over 15, and y would have the importance 3 over 15, which is exactly the same values as we got before when we were actually trying to explicitly solve our system of flow equations. 6 over 15 is the same as 2 over 5, and 3 over 15 is 1 over 5.",
    "200-220": " So we got to the same solution as we got before when we were trying to solve a system of equation. But now, we didn't really solve the system of equations explicitly. We simply did this vector matrix multiplication multiple times.",
    "220-240": " And the thing converged somehow miraculously to the values we wanted. So, so far, we looked at pagerank in terms of a matrix formulation. So we expressed this set of flow equations as a vector matrix product. And then, we saw that instead of solving the flow equations,",
    "240-260": " we can kind of find the eigenvector of matrix M, and this way find the pagerank scores. So what we will do next is we will look at an interpretation of what the pagerank scores mean. And this is called a random walk interpretation. So basically, we will see that pagerank scores",
    "260-280": " are equivalent to a probability distribution of a random walker in a graph. So before I tell you the details, here is a way how to think about this. We are thinking about the web graph as a giant graph, and we are thinking about the surfer.",
    "280-300": " So a surfer is simply a person who is basically randomly surfing this graph, which means that a surfer comes to a given web page, looks at all of the outgoing links, picks one at random, and moves to the next web page. And the surfer is kind of browsing this graph indefinitely. So the idea is that at some given time, the surfer is at some node i.",
    "300-320": " And what the surfer will do in the next time step, at time t plus 1, basically, the surfer will follow an outlink from i and choose this outlink uniformly at random out of all the outlinks of node i. And then now the surfer is at node j. So at time t plus 1, surfer is at node j,",
    "320-340": " and again, looks at all the outgoing links of node j and follows one of them at random. So now what we can also think about is we can think of this vector p of t. And this p of t, we think of this as a probability distribution over the nodes of the graph, which basically tells us with what probability",
    "340-360": " is a walker at time t at a given node. So we can think that every node in the graph has a value associated with it. And this value corresponds to the probability that at a given time t, the random walker is at that given node. So now that we have defined the process",
    "360-380": " and we have defined the notion of p of t, now the next question is to ask, where is the random walker going to be at time t plus 1? So given the probability distribution where the random walker is at time t, that is called p of t, the question is, where is the random walker going to be at the next time step?",
    "380-400": " And the answer to this is actually very, very intuitive. So we can ask, what is the probability that the random walker will be at node j at time t plus 1? So if we want to compute this, then all that for node j we have to look at is, what are all the nodes that point to j? What is the probability that the random walker",
    "400-420": " was at any of these nodes i that point to j? And at every node i, the random walker basically has to go and take this link that points towards node j. So this means that whatever was the probability that the random walker was at a given node,",
    "420-440": " now the random walker has to pick the outlink that points to node j. So which means that what we are basically getting is exactly our PageRank equation, if you want to think about it this way. So the probability that the random walker is at a given node is simply the sum of the probabilities",
    "440-460": " that the random walker in previous time step was at the neighbors that point to a given node. And from every given node, the random walker transitions to the node j with probability 1 over the out degree of that given node, which is exactly the PageRank formulation.",
    "460-480": " So this means that the probability distribution of where the random walker resides at time t plus 1 is simply our matrix M times probability distribution where the random walker was at time t. So now, let's suppose the following. Let's suppose that the random walk reaches what is called a steady state, which",
    "480-500": " means that probability distribution at time t equals the probability distribution at time t plus 1. This means that P of t is a stationary distribution. So probability distribution at time t plus 1 equals M times P of t equals back P of t.",
    "500-520": " So what we observe now is that this stationary probability distribution, P of t, is exactly what was our original throw formulation of a random walk. Before, we had r equals M times r. Now, we have P of t equals M times P of t.",
    "520-540": " So this means that our rank vector r is a stationary distribution for this random walk process. So think about this a bit. So basically, what PageRank scores correspond to? They correspond to the probability that this random surfer, that infinitely long kind of walks",
    "540-560": " the web graph at some given time t, resides at a given node. So this is what is called the random walk interpretation of PageRank, where we can think of a score or a rank of a given node to be the probability that the random walker is at that given node at some fixed time t.",
    "560-580": " So another important consequence of this random walk interpretation is that there is a rich literature on random walks. And random walks are really called Markov processes, or first order Markov processes, because basically, they have very little history. And the central result from the Markov processes or random walk",
    "580-600": " literature is that under certain conditions, basically conditions on the matrix M, the stationary distribution is unique. And it will eventually be reached no matter what is the initial probability distribution at time t equals 0. So what does this mean? This means that there are certain conditions",
    "600-620": " on the structure of our graph, on the structure of our matrix M. And if our matrix M satisfies these assumptions, then the stationary distribution is unique, which means there is only one unique PageRank vector, r. And this unique PageRank vector, r,",
    "620-640": " will be achieved regardless of how we initialize it, which means that our power iteration will always converge to the same vector regardless of how we initialize it."
}