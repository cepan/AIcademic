{
    "0-20": " so so far we discussed SVD and we will conclude our discussion of singular value decomposition by looking at an example of its usage and then talk a bit more broadly about in the method so here is the idea let's think about the following problem imagine we want to identify all the users who like our Matrix movie so the idea is we have this matrix now of values of users two movies",
    "20-40": " and now we would like to identify all the users who like the movie matrix and what we would like to learn from this task in a sense is that given that we saw that there are people who like sci-fi movies so maybe we would like to kind of find a person who didn't even",
    "40-60": " see the movie matrix yet but we may want to be able to say yes but given that they like this other sci-fi movies demel they may also like the movie matrix and the question is how can we do this using SVD the answer to this question is that basically we want to map map our query",
    "60-80": " point into the concept space we are given our data users to movies matrix a right we did we do the SVD of it so here's the SVD and what we want to do now is we are given our query point our query point Q is simply we say let's find all the users that liked the movie",
    "80-100": " matrix so let's create in some sense these queries query users this artificial user that likes the movie matrix and the idea is we want to find other users who are close to this given user in the in the concept space so what we will do is we have we have our movies space we have our data point Q here our",
    "100-120": " query point and we want to project it into our concept space the way the way we do this is that basically we simply do the inner product of our query point with each concept vector in vector V because vector V is movies two concepts vector right so if we if we go do this why is taking the inner product a good",
    "120-140": " idea because for example Q times the first singular vector will simply take take our position of the point Q and tell and will tell tell us its location along the the axis of the first singular vector the second singular vector is orthogonal to the first one so here it is the V 2",
    "140-160": " and when I multiply Q times V 2 we will basically now get the projection of the data point and its position on the second singular vector so that's basically what will happen so if we if we do our our projection so we take our vector Q multiply it with matrix V we do",
    "160-180": " the thing and here is what we obtain so we obtain that this for example this particular user now we are in this concept space in this two dimensional concept space where the first column of V is sci-fi and the second column of rivas Romans and now as we do the inner",
    "180-200": " product we basically see that Allah that our query point in some sense corresponds heavily to the sci-fi concept and very low has a very low coordinate value along the Romans concept so this is now how we monitor the query and kind of mapped it into the concept space so now for example imagine",
    "200-220": " I have some user some user D that we don't know what they think or they haven't told us anything about what they think about the movie matrix but they tell us they really like movies alien and serenity so now if I take this user again multiply them by our vector V and basically move them to the concept space here are the coordinates or the position",
    "220-240": " of that user in the concept space so what is what is a good thing that happened so for example if I now compare the positions of the original user and the query in the original space and the dead locations in the concept space I find the following so the similarity",
    "240-260": " between Q and D in our original space is zero right in a sense that for alien in serenity our query doesn't doesn't want them for our movie matrix the user didn't tell us anything so there is no similarity between these two vectors but if I go back to my concept space here I",
    "260-280": " see that both of these data points or both of these users Q and d they actually share high values on the sci-fi concept and they share low values on the Romans concept so in some sense I'm able to identify or I'm able to put together that Q and D are actually close together in our space even though in the",
    "280-300": " row represent data representation they don't share any coordinates together so in some sense even though Q and D have zero ratings in common we are able to identify that there are similar because SVD was a SVD was able to identify that kind of people who like alien and serenity also like the Matrix",
    "300-320": " movie so this basically is how we can make use of singular value decomposition what I want to do now now very briefly is to relate singular value decomposition to another type of decomposition of a matrix that is called eigenvalue decomposition so first I will tell you what is the relationship",
    "320-340": " between singular value decomposition and eigendecomposition and then we will conclude so what we know so far is that SVD is given a matrix we represent it as a product of three matrices u Sigma and V transpose what is I can value decomposition eigenvalue decomposition is kind of more constrained it says given some matrix a I want to represent",
    "340-360": " it as a matrix X times the matrix capital lambda times X transpose again so here I only have kind of product of two matrixes if like lambda and X so for eigen decomposition to to even exist what we have to do is first a has to be",
    "360-380": " symmetric which means that the values above the diagonal have to be the same as values above a below the diagonal while for example in SVD we did not have this constraint and then both in SVD and eigenvalue decomposition all the matrices are column orthonormal which means that columns are orthogonal to each other and have a unit length and in",
    "380-400": " both cases u sigma and lambda are diagonal matrices so now the question is what is the corresponding correspondence between singular value decomposition and eigen decomposition so let's consider this simple case let's consider what is a a times a transpose so we know that we",
    "400-420": " can take the matrix a and perform singular value decomposition of it so let's do that so what is a times a transpose is the singular well value decomposition of a time's the singular value decomposition of a transpose which is the same as singular value decomposition of a and then transporters",
    "420-440": " transposing that so now let's start thinking what do we what do we get next right what we get next is that because the multiplication is commutative we can kind of reorder reorder the terms right so for example we can take the original expression and now just reorder the terms of mood or the order of multiplication what we notice now is",
    "440-460": " that we get we get a multiplication of V transpose times V and given that our matrices are orthonormal this means that we a matrix multiplied with itself gives us an identity matrix an identity matrix is simply a matrix that has zeros of the diagonal and it has values of 1 on the",
    "460-480": " diagonal so what this means is that it's basically an identity matrix right so what this means is that I it can take a a transpose and transform it down if we take its SVD and see what happens it turns out to become u times Sigma Sigma transpose u transpose ok similarly the",
    "480-500": " same thing happens or a similar thing happens if I ask what is the singular value decomposition of a of matrix a times a transpose I do I do the same the same trick as before and here now I obtain that this equals V times sigma x sigma transpose times V transpose one",
    "500-520": " thing that I have to remember is that Sigma is a diagonal matrix so in some sense sigma x sigma transpose is nothing else than another diagonal matrix that has the squares of the values on the diagonal right so what do we learn from this is the following so if I take the a times a transpose which",
    "520-540": " is a symmetric matrix I do a singular value decomposition of it what I end up with is an expression like like this which basically means that in this case you is a set of the I can think of you as a set of eigenvectors right so as a part of the eigendecomposition and i can",
    "540-560": " think of sigma x sigma transpose as a set of eigenvalues so what is basically means is that if I have a matrix I can do singular value decomposition of it and from singular value decomposition of it I can do the eigenvalue decomposition where the relationship between eigenvalues and the singular values is that singular value squared are the",
    "560-580": " eigenvalues of the corresponding matrix so that's the that's the connection between the eigenvalue and the singular value decomposition so to to finish talking about singular value decomposition here is here's kind of the overview so what is good about singular",
    "580-600": " value decomposition is that it gives it gives us the optimal low-rank approximation right in terms of the Frobenius norm so it means that if I allow myself to take my data and represent it using a small number of dimensions then SVD will be able to identify the best possible number of dimensions that basically it was the best possible projection of the data into some small dimensional space in",
    "600-620": " such a way that if we go from the small dimensional space back to the original high dimensional space the sum of the squares of the reconstruction set or errors will be as small as possible so that's great what is problematic with SVD are two things first one is the interpretation problem what this means",
    "620-640": " that the singular vectors specify some linear combination of input columns or rows which means that many times singular vectors are very hard to interpret when I say singular vectors are hard to interpret in our cases of movies to users matrix I was able to interpret the first singular vector to correspond to",
    "640-660": " the sci-fi movies and the second one to the romance movies many times that is hard to do and the second big drawback of singular value decomposition is what is called lack of spares sparsity what this means is that the input matrix a is often very sparse which means it is full",
    "660-680": " of zeros and only has a few nonzero elements in it but when you when we do singular value decomposition the which is U and V transpose they will be dense what I mean by that is all basically all the values in this matrix will be nonzero so many times even though you env have a very small number",
    "680-700": " of columns or rows so in some sense in terms of the row column or row row size they are much smaller than the matrix a in terms of the data size may be bigger because a has very few nonzero elements or values and then matrix issue and we have a large number of nonzero elements so what we will do next is we will look",
    "700-720": " at the method that is much easier to compute much faster to compute than the singular value decomposition and also maintains the sparsity of rows and columns of U and V and this is what we are going to look at next"
}