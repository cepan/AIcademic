{
    "0-20": " so the next question is how do we really go and find the vector W that maximizes the margin so the question is how do we precisely compute the margin so now what we will learn is where the name of the support Vector machines really comes from and our idea right so far was to find a separating line that maximizes",
    "20-40": " the margin and if we think about it we can we can draw the following picture right we have our let's say negative training examples our minuses on the top and we have our pluses on the bottom and what we want to do is we want to find a line that maximizes the distance of the closest point to that line and the way we this line is simply defined it's",
    "40-60": " defined by a few points that are closest uh to it right what this means is that we could ignore all other data points if we would have these three uh Circle data points because they already uniquely Define the line and these three Circle data points they are they are called support vectors right the line the separating height hyper plane is",
    "60-80": " uniquely defined in this case by three supporting um support vectors so now the question will be how do we go find the support vectors and how do we find the line generally if our data is is kind of not degenerate then if we are in D dimensional space we need d+1 support",
    "80-100": " vectors to Define such line while we have already talked that gamma corresponds to the margin basically corresponds to the distance of the point from the hyper plane here is a here is a problem the problem is Imagine The Following imagine that I have a data point x I multiply it with W add this B",
    "100-120": " multiply with with the class and I call this um gamma which is the margin so now imagine that I take my Vector W but make it twice as long so I just kind of take twice twice the W plus plus X plus tce b * Y what this gives me now is twice as",
    "120-140": " big margin so it seems based on this simple um calculation is that the longer we make W kind of the bigger the W is the bigger our margin will be and this basically means that there is kind of very hard to optimize this because we can just make W as large as possible and um the margins will also get as large as possible so we are not doing anything",
    "140-160": " useful so the solution to this problem is that we need to work not with W but with a normalized version of w in a sense that we want to think of w as a vector that has length of one so what this means now is that we will change the definition of margin slightly we are still taking WX + B * y but now we are",
    "160-180": " working with a normalized version of w so we are also dividing by the length by the ukian length of w in a sense and the ukian length is simply the sum over all the coordinates taking the square of those coordinates and then the square root of the sum so now this is the first thing we do is right the first change is",
    "180-200": " that we will be working with a normalized version of w and this changes the notion of the margin a bit the other thing that will also work is not now that we will require support vectors X these are these three circled data points to be defined by the line W * x + B = plus or minus one right so going up",
    "200-220": " back to my picture our decision boundary is WX plus b equal 0 so now the the left support vector are WX + B = minus1 and the right support vectors are WX + B = + 1 right so we are in some sense requiring that this here is of",
    "220-240": " unit one so now the question is how can we put all this together and find the optimization problem that will allow us to maximize the margin so the goal is still to maximize the margin now the question is what is the relationship between data point X1 which kind of is here on the margin on",
    "240-260": " the other side and data point X2 which is our red data point here um what do we know is the following we know that X X1 the value the value of the data point here is simply X2 plus twice the the margin um times the the normalized version of the vector W right so if I",
    "260-280": " have my Vector W then what's the distance between X and Y I simply has have to take um this Vector W and multiply it by twice the margin because there's one unit on of margin gamma here and another unit of margin gamma at the bottom so that's the first kind of equation that we know the second",
    "280-300": " equation that we know is based on our assumption the previous slide that the left side of the margin is defined by WX + Bal minus1 while the right hand side of the margin is or the bottom of on the other side of the decision boundary is defined by WX plus b equals + one so I can also go and write out both of these",
    "300-320": " constraints so what I can do now is I can take this system of equations and try to solve it right so the I take I take the first um the first equation and I enter instead of and I substitute X1 um from the from the top equation to obtain the next one now if I go um",
    "320-340": " multiply this through I get that W * X2 + B = tce gamma and then I have W * w = + one what what we notice now is that I can use the the last equation and notice that wx2 + b equals minus1 that's the",
    "340-360": " first thing we notice and this already now solves the whole the whole equ the whole equation for gamma right what we see is that we can solve now for gamma so we see that gamma equals the length of W Times the uh W do product with itself what do what do we note now is that W um times the dot product with",
    "360-380": " itself that is simply the square of the length of w so the square root of the length divided by the square of the length is just one over the length so what we arrived to is that gamma our margin is one over the length of w so to",
    "380-400": " summarize what we know so far we started with the first optimization problem that simply says we want to find W such that the margin is maximized and what is the margin the margin is the distance of that all all the data points that we have have the classification or the confidence greater than gamma so that",
    "400-420": " was our initial um optimization problem what we noted then that this initial optimization problem can trivially be solved by making W as large as possible or arbitrarily large so there is nothing kind of useful in solving this problem so what what we did then was we said okay let's normalize uh our margin by",
    "420-440": " the length of w so we said maximizing the margin gamma is the same as maximizing one over the length of w which is the kind of the same or equivalent to minimizing the length of w which is the same as minimizing 1/2 time",
    "440-460": " the length of w ^ s and just for some technical reasons at the end our goal will be to minimize the one half the length of W Squared so now that we that we have uh transformed kind of maximization of gamma to the minimization of the length",
    "460-480": " of w we can now write down the support Vector machines margin maximization uh optimization problem so our goal right now is the following and equivalent to the optimization above module of the problems that we resolved we want to minimize the length of w so we want to find w such that it has the smallest",
    "480-500": " length while um the our classification um margin our confidence in classification of all the training data points is greater than one and this optimization problem that I wrote down here is called um svm or support Vector machine with hard",
    "500-520": " constraints"
}