{
    "0-20": " so having examined nearest neighbors as a very simple way of how we can use use them to predict categorical and as well as real valued outputs for example to do regression we will now talk about the state of the art method that is used for classification and the method goes under the name of support vector machines and",
    "20-40": " this method will really scale to large scale data and it will allow us to do what is called classification and the example application we'll be talking about is the application of email spam filtering so imagine we are a big email provider we are receiving lots and lots of email and for every email that comes through our servers we would like to decide whether to flag it as spam or",
    "40-60": " whether we kind of put it in our users inboxes so the way we can think about this problem is the following we can think that we are having a an email and we will represent each email as a long vector where it will be a simple binary vector where every entry or every component of this vector will correspond",
    "60-80": " to a different word and simply a value one will indicate that a given word appears in the document and value 0 will indicate that a word does not appear in a document so for example imagine that I have a set of 3 emails where the email number 1 has words viagra and in indem while for example the second email has",
    "80-100": " the words learning and the indem and the third document has Nigeria in in it and other words as well right and for each of these documents I also know their label right I know whether they are spam so for example 1st and 3rd documents are spam while the second document is not spam okay so I can think about this as",
    "100-120": " an instance space where access are instances of my data let's think that I have n data points in our case I will have what we will call them binary features right because the entries of X are binary values and imagine that every X has d dimensions which means that we",
    "120-140": " have d features the different characteristics that describe our data point in our case there is one dimension or one feature per word which means that we will have around 100,000 1 million features so every email now is a 1 1 million or 100,000 dimensional data point now having described the",
    "140-160": " instances X now the question is what is the class space Y and our class is very simple it's a binary variable so it's a classification problem where we want to decide whether a given email is spam or whether it's ham whether basically it's not spam and the way we will be using",
    "160-180": " our models to do this classification is that we will use what is known as linear models and we will think about a very simple linear model right so we will think of our function f of X to have a very simple form we will say we will predict that the email is spam if some",
    "180-200": " weighted combination of words that is that appears in the document is greater than some threshold otherwise we will predict minus 1 right so all we are basically doing is we are taking these binary values whether the first word appeared in the document whether second word was a appear in the document whether the third word appeared in the",
    "200-220": " document and doing a weighted summation where the weight of every word will basically tell us whether that word in some sense vote for the email to be part of the spam domain or whether for that email to be part of the non spam domain so simply right if we have x1 to be a word viagra then we would we would",
    "220-240": " expect it to have a high positive weight so that the whole sum will be greater than 1 and if we have a word learning or data mining maybe the x2 is a word learning then the its corresponding way to W we would expect it to be negative or hope for it to be negative because this is an indication that our email is",
    "240-260": " not spam ok so the way we think about this is that our input is a set of vectors x and y where x s are real valued or binary valued in our case we will now take these big binary vectors and we will normalize them so that the",
    "260-280": " Euclidean length equals 1 and our goal is to find vector W right with the same same length same number of components as X has such that W now kind of separates out spam from non-spam so the way we can think about our data now in this case is that we can think of our data instances as being embedded in this",
    "280-300": " D dimensional space where this is the first dimension and this is the second dimension and so on and then one part of these data points is not spam so let's think of this as minuses and the other part of the data is spam so we could think of this as spam we think of down here as hem so our goal now will be to find in some sense a line that separates",
    "300-320": " pluses from minuses that kind of separates spam from hem and to find this line we will be using tools from linear algebra so our idea will be that basically what we want to do is the following imagine that we have this w this weight vector this weights that that we are using when doing the inner product with",
    "320-340": " our features X then basically what we are saying is that we want to find align this blue line where W times X equals our parameter theta right so as I have it up here and now if this if this product is positive then we will be making one prediction and if the point is on the on the other side of the line",
    "340-360": " then we will be making the other prediction and kind of the whole goal or the whole the whole the hard part here will be how do we find this weight W right how do we find the W subject such that it represent this line that separates spam from non-spam so to be a",
    "360-380": " bit more precise this is an example of a linear classifier where basically the idea is that every feature every word in our case has a weight associated with it and then the idea is that our prediction is based on the weighted sum of the weights of the features and the corresponding feature values so simply",
    "380-400": " writing this out we can think of this as a dot product inner product between the weight vector W and a vector representing features of our data point and then once we have done this dot product this weighted summation of features and their weights then we simply say that the f of X gives up",
    "400-420": " again is a positive prediction if the W times X is positive and it gives a negative prediction or it gives a different class if the the F times X is negative so the way we can think about this is the following we can think that in our space every every word is a",
    "420-440": " separate dimension so now a document or an email is simply a point in this D dimensional space and our goal is to find a line such that all the all the spam emails fall on one side of the line and all the non-spam email fall on the other side of the line the question is of course how do we find this vector W",
    "440-460": " or equivalently how do you find the line"
}