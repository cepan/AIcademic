{
    "0-20": " in this unit I'm going to build some theory for mauce algorithms this Theory will eventually lead us to some lower bounds and that in turn lets us assert that particular algorithms are as efficient as a map reduce algorithm for a problem can be there are several ingredients to the theory first we need the notion of reducer size the maximum amount of data",
    "20-40": " that a that a reducer can have as input we also need the notion of replication rate the average number of key value pairs Genera by a mapper on one input one goal is to prove lower bounds on the replication rate as a function of reducer size that is the smaller the reducer",
    "40-60": " size the bigger the replication rate another essential ingredients the theory is the mapping schema a description of how outputs for a problem relate to inputs we characterize a problem by its mapping schema and we use the properties of that schema to put an upper bound on how many outputs can be be covered by one reducer with a given",
    "60-80": " reducer size that in turn lets us get lower bounds and replication rate as a function of reducer size we're now going to introduce a model that lets us discuss the best way to trade off communication for the number of reducers used the elements of the model are simple but the conclusions",
    "80-100": " are interesting there is a set of inputs in the drug interaction example we just talked about each drug and megabyte long record is an input and there is a set of outputs the output for a pair of drugs and is the conclusion whether there is a",
    "100-120": " statistically significant interaction between those two drugs and there is a many to many relationship between inputs and outputs each output requires certain inputs to compute its value in the drug interaction example each output pair say i j is related to two inputs I and J",
    "120-140": " it is common for outputs to depend on two inputs but there are important examples where outputs depend on many more than two inputs often the inputs are sent as values in key value pairs to the reducer where the outputs that need them are computed however in other cases the key value pairs could be computed in some",
    "140-160": " complex way from the inputs we don't need to distinguish these cases we connect an input to those outputs such that the mapper for that input creates some key value pair necessary to compute that output here's what the many to many mapping looks like between inputs and outputs for the drug interaction problem with four inputs in this case there are",
    "160-180": " six outputs each corresponding to one of the six pairs of drugs here's an example where outputs depend on very many inputs we want to multiply n byn matrices there are thus two N squared inputs one for each of the elements of the the two input",
    "180-200": " matrices and N squ outputs one for each element of the result Matrix okay look at a single output say in row I and column J if you remember your matrix multiplication you know that this one output is computed by taking the dotproduct of the I row and the jth",
    "200-220": " column thus each output is related to 2 N inputs there's a subtlety in our model that is not Apparent from the examples of drug interactions and matrix multiplication in these cases the input set is fixed and the output values depend on the",
    "220-240": " inputs but which outputs are made is also fixed but in some important examples what we call the inputs and outputs are really an envelope encompassing all those hypothetical inputs that might occur in some run of the algorithm and all the outputs that might be made depending upon what combinations of inputs are",
    "240-260": " present in an execution of the algorithm the actual inputs are a subset of the hypothetical inputs a hypothetical output is made if and only if it can be made from whatever inputs are available in some applications we can only make an output if all its inputs are present and",
    "260-280": " in others we can make an output if at least one of its input is present or there might be other options as well to see the difference consider the problem of taking the natural joint of two relations r with attributes or column names A and B and S with attributes B and",
    "280-300": " C if you are unfamiliar with the concept of the natural join we're looking for pairs of tupal one from R and one from s that agree on their B columns any such pair of tupal is spiced together to form a tupal with the a value from the r tupal the common B value and the C value from the s tupal that is we get those",
    "300-320": " tupes AB C such that AB that is this forms a tupal of the r relation and BC forms a tupal in the S",
    "320-340": " relation in principle the inputs are all possible tupal in R and all the possible tupal in s that is we assume the attributes a b and c have finite domains and any pair of values one from the domain of a and the other from the domain of B could be a tupal of R so",
    "340-360": " these are one group of hypothetical inputs also any value from the domain of B can be matched with any value in the domain of C to form a possible tupal of s so these are the other hypothetical inputs similarly the possible outputs are all those triples ABC such that each component is taken from the",
    "360-380": " corresponding domain in this problem each output ABC is connected to two inputs the inputs that justify the presence of that output in the join those two inputs are R of ab and S of BC in practice the actual relations RNs will have only a subset of",
    "380-400": " the hypothetical inputs so when we compute the join we want to make an output ABC only when both of the inputs that join to make the tupal are present on the input so the actual outputs will also be a fraction of the hypothetical outputs for example if 1/10th of the possible tupal of R are",
    "400-420": " present and 1/10th of the possible tupal of s are present then we would we would expect about one 100th of the possible outputs to be made the true number of outputs could be different depending on the particular Tuple chosen now we're going to talk about about two parameters that describe a map",
    "420-440": " reduce algorithm the first which we'll talk about here is the reducer size next we'll talk about replication rate which is a measure of how high the communication cost is the reducer size for an algorithm which we denote by q and what follows is the maximum number of inputs that we allow for one reducer remember a reducer",
    "440-460": " is a key and its list of values so we are really putting an upper limit on how long this list can be one reason we might want to put such a limit on reducer size is so that we're able to execute the reduce function for one reducer in main memory but there might be other reasons",
    "460-480": " for example we might want to put a smaller limit on reducer size to force there to be lots of available parallelism remember hypothetical inputs are assigned to reducers when we design the algorithm that is the map function must be designed to generate certain key value pairs knowing only that it's one",
    "480-500": " input exists we don't know in advance which inputs will be present however if we know what fraction of all possible inputs will be present we can use a value of reducer size that is larger than we want the actual number of inputs to a reducer to be for example suppose",
    "500-520": " that we decide we want no more than a million inputs to any reducer and that bound is necessary to enable the reducer to do its work in main memory suppose we also know that 10% of all the hypothetical inputs will actually be there when we run the algorithm then we might decide design the algorithm with a",
    "520-540": " queue of 10 million figuring that the 10 million values that might be sent to the reducer on average only one million of them will really be there and things will work well in main memory we have to be a little careful as long as the problem inputs are chosen independently then we can expect each reducer to have close to the average number of inputs",
    "540-560": " and will be okay maybe you want to set q a little lower than 10 million to account for the fact that there will be some statistical variation and we don't want any of the reducers to get more than one million inputs but what if the inputs are seriously skewed that is not selected randomly for example we're joining R of ab with s of BC and many of the actual",
    "560-580": " tupal have the same B value then it is possible that our strategy for sending inputs to reducer will send many more than average to some of the reducers and those reducers will cause problems for example they may not be able to run in main memory and the need for repeated dis iOS will make them take much more time than the others in that case we",
    "580-600": " have to lower Q well below its expected value now let's introduce the other important parameter of map reduce algorithms the replication rate formally it is the average number of key value pairs created by one mapper or put another way it is the average number of outputs to which an input is connect connected for many problems this fan out",
    "600-620": " will be the same for all inputs we'll use R throughout the discussion to stand for the replication rate and it helps to think of the replication rate as the communication cost per input the relationship between reducer size and replication rate depends on the number of reducers we need and the input",
    "620-640": " size if there are P reducers each receiving Q inputs and the number of inputs is capital I then the replication rate will be PQ / I I should point out that I'm going to make a simplification here to avoid a substantial amount of mathematics I'm",
    "640-660": " assuming that each reducer has the same number of inputs and that number is the maximum allowed that is Q if some of the P reducers got fewer inputs then the replication rate would be smaller but usually we get the lowest possible replication rate for a given Q by using reducers with a exactly Q inputs each",
    "660-680": " let's see how R and Q relate for the drug interaction problem we discussed earlier let D be the number of drugs in our example we use D equals 3000 but there's nothing special about that let's also suppose we divide the drugs into G groups of equal size so each group",
    "680-700": " consists of D over G drugs a reducer corresponds to a pair of groups so the number of inputs each reducer needs is twice the number of drugs in a group or 2D over G we can figure out the replication rate",
    "700-720": " directly since each drug is sent to G minus one reducers one reducer for each pair consisting of its group and one of the other groups we'll assume G is fairly large so we'll drop the minus one and just say the replication rate is G now we have R and Q in terms of G we can eliminate G and get r equal 2D",
    "720-740": " over Q This is interesting it says that the replication rate and reducer size are inversely proportional to each other that makes a lot of sense it says the more work we can throw on one reducer and therefore the less parallelism we get the smaller will be the communication",
    "740-760": " cost there might be some confusion with the relationship r equal PQ over I that we learned in the previous slide uh superficially it looks like R grows in proportional to Q not inversely but p is also a variable here",
    "760-780": " and P is inversely proportional to the square of Q we can see that the earlier equation for R holds as well if we substitute for PQ and I P is G ^2 / 2 Q is 2D over G and I is",
    "780-800": " D when you multiply these out you get Ral G which we knew from the analysis of what the mappers do actually it's not g it's G minus one but P isn't really g^ 2 over 2 it's really G choose to or G * gus1 /",
    "800-820": " two the approximation even out what we saw so far can be interpreted as an upper bound on the best possible replication rate for any reducer size in general when we give an algorithm and analyze its replication rate in terms of",
    "820-840": " Q we get an upper bound on the smallest possible R for that Q surely the smallest R cannot be greater than the actual R we get from the algorithm however to really understand map reduce algorithms we need Define lower bounds on r as a function of Q as well the analogy would be sorting on a Serial machine machine we have",
    "840-860": " algorithms like merge sort that take n log n time on input of size n that's good to know but it only becomes impressive because we also have an N log n lower bound on any algorithm that does General sorting so what we want to address now is how one could go about proving that R",
    "860-880": " cannot be less than a certain function of Q in order to prove lower bounds and replication rate for a given problem we need to be able to talk about every possible algorithm that solves a problem we can't do that in general but for many interesting problems we can abstract enough of what any map produce algorithm can do that it is possible to",
    "880-900": " make some useful claims about how much replication is needed so let's define a mapping scheme for a given problem and for a given reducer size Q to be an assignment of each input to a set of one or more reducers there are two conditions this assignment must obey in order for the",
    "900-920": " mapping schema to be considered as solving the problem with this reducer size first of course we can't assign more than Q inputs to any one reducer but in order for there to be any way of computing the outputs correctly using this mapping schema there is another condition that must be satisfied for each output there is at least one reducer that is capable of computing the",
    "920-940": " output in order to compute an output the reducer must have as inputs all the inputs on which that output depends when a reducer gets all the inputs and output needs we'll say the reducer covers that output there are two really important points I want you to bear in mind about",
    "940-960": " mapping schemas first from any map reduce algorithm we can find a mapping schema the mapping schema doesn't tell us about the algorithm for example it doesn't say how the outputs are computed from the inputs it doesn't tell us in a case where several reducers each have all the inputs needed needed to produce an output which of them actually produces",
    "960-980": " it it doesn't say how the mappers generate key value pairs so that the right reducers will get a given input but we can be sure that if there is an output such that no reducer gets all the inputs that output needs then the algorithm cannot correctly produce that output and I also want you to observe that it is the requirement for a mapping",
    "980-1000": " schema that makes map produce algorithms a special case of parallel algorithms in general we can't just take any parallel algorithm and implement it as a single map reduce job now I want to apply the mapping schema idea to the problem of drug interactions and get a lower bound on replication rate as a function of the",
    "1000-1020": " reducer size Q as we shall see the algorithm we already suggested for the problem is almost as good as possible we'll again assume that there are D drugs to compare and the limit on the reducer size is Q first the critical observation in all lower bound proofs is to put an upper",
    "1020-1040": " bound on how many outputs a reducer can cover in this case the reducer gets Q drug records as inputs the only outputs it can cover are the pairs of drugs that it has received there are Q choose two of these which I'm going to approximate as Q ^2 over 2 technically the exact number is Q * Q -1 over2 but for large Q",
    "1040-1060": " There is negligible difference but among all the reducers there are D choose two outputs that must be covered again I'll approximate the choose two by squaring and dividing by two so there are approximately d^2 over two outputs to cover",
    "1060-1080": " somewhere so if we divide the number of outputs by the number of outputs one reducer can cover that we get the minimum number of reducers that must exist in this case the number is d^2 over for Q",
    "1080-1100": " ^2 the replication rate is the number of reducers that's d^2 over Q ^2 times the number of inputs per reducer that's Q divided by the total number of inputs which is D that gives us R greater than or equal",
    "1100-1120": " to D over q that is there is are lower bound on replication rate for any algorithm that solves the drug interaction problem uh I should confess that I'm being a little sloppy here I'm assuming that the algorithm has every reducer getting all",
    "1120-1140": " Q of the inputs it is entitled to I should do a little more math and consider the possibility that different reducers get different numbers of inputs smaller than Q however it doesn't matter in the end we'll still get the same lower bound notice that the lower bound R greater than equal to D over Q is half",
    "1140-1160": " the replication rate that the algorithm we described using G groups uses if you check back to that discussion you'll find that we had R equals g the number of groups and also Q was 2D over G if you turn this equation around you",
    "1160-1180": " get G = 2D over Q or all r equal 2D / Q uh since R and G are the same"
}