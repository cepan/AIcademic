{
    "0-20": " next we shall look at the problem of finding news articles that represent the same story since the same story may be used by many different sources each with its own way of presenting it on a webpage the problem is not too different from finding similar documents in general but there is a special way of shingling that works well when the difference are mostly in the ads associated with the article we should",
    "20-40": " also talk about a simple bucketing method that works when the number of sets is not too great and the similarity expected for the underlying articles is quite high so we turned to the third interesting variant of LSH with another true story a while ago a group of",
    "40-60": " political scientists at Stanford asked for help from the cs department going through a large repository of news articles to identify those that were essentially duplicates the problem was that many of the articles really came from the same source but that can look quite different when published on the website of different news services they",
    "60-80": " wanted to group webpages whose underlying articles were the same or similar this by the way is the same problem faced everyday by services such as Google News although they do the grouping day by day rather than once and for all identifying the same underlying article is tricky",
    "80-100": " because each news source creates a page from the article that has unique elements on that page for example they will put the newspapers name and other text elements on the top and there will be links to ads on the page it's also common for one site to include links to",
    "100-120": " relate or own interesting stories on its own site in addition it is common for a site not to place the entire article on its pages especially if the article is long they will leave off the paragraphs at the end or even delete other paragraphs that the editor finds less relevant now the CS team had not heard of locality sensitive hashing or min",
    "120-140": " hashing however they invented a form of shingling that is probably better than the standard approach we covered for those web pages that are of the type we just described and they invented a simple substitute for LSH that worked adequately well for the scale of problem they were looking at they partitioned the pages into",
    "140-160": " groups of similar lengths and they only compared pages in the same group or nearby groups after they had done all this we happen to be talking in the hall and I mentioned min hashing at LSH they implemented these algorithms on that data and they found that min hashing plus LSH was better as long as the similarity threshold was less than 80%",
    "160-180": " actually that is consistent with what is known when you're looking for very high jacquard similarities like 80 or 90% then there are indeed more efficient algorithms and we're going to cover these before we leave the topic interestingly the first time they implemented min hashing they got it",
    "180-200": " wrong and decided that the method was terrible but the problem was that I forgot to remind them to do the min hashing row by row where you compute the hash value for each row number once and for all rather than once for each column remember that the rows correspond to the shingles and the columns to the web pages since their data was naturally",
    "200-220": " stored by columns that is by weight with pages they needed to sort their set of shingle web page pairs to organize them by row that is by shingle once they did that they got the positive results I mentioned on the previous slide before leaving this topic let me tell you about",
    "220-240": " the way these guys shingled web pages containing news articles the key observation was that they needed to give more weight to the articles themselves than to the ads and other elements surrounding the article that is they did not want to identify as similar to articles from the same newspaper with the same ads and other elements but",
    "240-260": " different underlying stories the trick was based on stop words the common little words that we need in order to construct sentences properly but that do not convey much meaning typically the set of stop words includes things like and the to and so on usually when analyzing text we ignore",
    "260-280": " stop words because they don't tell us anything about the subject matter but here are the key observations was that the stop words tell us whether we're looking at the news article from which we want to take our shingles or ads or other peripheral matter from which we do not want to take jingles for example ordinary pros would say something like I recommend that you buy",
    "280-300": " suds o for your laundry the likely stop words are shown in orange that you for your very common words but in an ad we would just find something abbreviated like buy suds o which has no stop words at all so then define the shingle to be",
    "300-320": " a stop word followed by the next two words in the sentence stop words or not thus in the sentence on the slide one shingle would be I recommend the hat the next would be that you buy and so on notice that there are relatively few",
    "320-340": " shingles and it is not guaranteed that each word is part of even one shingle the reason this notion of shingle makes sense is that it biases the set of shingles for a page in favor of the news article that is supposed to simplicity that all pages are half news article and half ads if you count by number of characters if we have a second page with",
    "340-360": " with the same article but different ads we find that most of the shingles for both pages come from the article because that's where the stop words are so these two pages have almost the same shingles and therefore have very high Jaccard similarity now consider two pages with the same ads and different articles these will have low Jaccard similarity",
    "360-380": " because again most of their shingles come from the articles and these shingles would be mostly different for the two articles"
}