{
    "0-20": " so given that so far we looked at how to build a decision tree kind of in theory and we we ignore the data size let's now talk about the algorithm that does the decision Tre building using map reduce so the problem we are looking at is how do I build a tree using a giant giant data set that that lives on top of map",
    "20-40": " preduce so the idea is given a large data set I want to build a decision three um what are some general considerations that I want to take uh into into account so the first one is if the the the the whole three will be relatively small what I mean by this is the data is big but the tree itself is relatively small so imagine it's like 10",
    "40-60": " levels deep if it's a binary tree this means it will have um 2,000 nodes or something so it's a small thing so three itself fits into memory but the data is very big so the data set is too large to keep in memory and the data set is too big to scan uh to scan over it on a single machine so we really want to have this data set be distributed across a",
    "60-80": " large number of machines and the idea is that because the data set needs to be distributed we'll be using the map reduce framework just to remind you how map reduce works the the idea is that we have our input file split into the chunks then we have a pump the input the chunks of the input file through a set of mappers mappers extract extract key",
    "80-100": " value pairs right we have a set of keys and we have a corresponding set of values then this data gets sorted and it gets passed towards the reducers where all the key value pairs that belong to the same key for example here everything that has the key B goes to the goes to the first reducer and everything that",
    "100-120": " has a key value a goes to the second reducer and for example also all the C's go to the first reducer right so the idea is that there is the shuffling step and then the reducers collect the values and each reducer produces its own output so the question is how can we use this kind of map redu computational inform infrastructure to learn how to build",
    "120-140": " trees so the particular approach we will be looking at is called planet um and this was um a p a research paper published by Google um a few years ago so three four years ago and what planet is it's simply a sequence of map reduced jobs that builds a decision tree our",
    "140-160": " setting in this case is the following uh we will kind of focus on on a subset of all possible um you know categorical attributes and so on we will only consider uh numerical attributes we can think of them as discrete meaning integers or continuous like real values but we have no categorical attributes we will um we will think of predicting the",
    "160-180": " target variable y to be to be a continuous value or a numerical value so we will think about regression and we will uh limit ourselves on binary splits right so our decisions will be of the Forum is X less than value V and then if yes go to to the left if no go to the",
    "180-200": " right um as I mentioned our decision three is small enough so mapper can keep it in memory but the data is too large to be kept in memory so our overall system architecture will be something like this right we will want to have a sequence of map reduced jobs that read the input",
    "200-220": " data and process it we will also have the master node that keeps track um of everything and uh we will also keep the separate ly in the memory the model that we have built so far we will keep some attribute metadata information um and we the master is also responsible for saving intermediate results what the map",
    "220-240": " produce will do is it will do in particular two things it will try to help us find the the best attribute and the best value to split on so we will call this find best split and then it will also kind of keep building the tree so the expensive part here or the part that is computationally very expensive is finding what is the best split and we",
    "240-260": " want to utilize map reduce to help us find the best split the way we will build the Tre is we will build it in levels so we'll first build the first level and then we'll build um the second level and so on and at every level for all for all the nodes we will want to decide what is the best attribute uh to",
    "260-280": " split upon so let's think about uh building the tree right as I said we are building the tree level by level where we will have one map reduce steps to build one level of the tree so if you want to build tree that is 10 levels deep we need 10 map reduce uh jobs or steps of iterations of it um and in particular",
    "280-300": " kind of at high level the mapper what it will do it will consider a number of possible splits um and on on a given subset of data and for each split it would kind of store the partial statistics so that then the partial statistic is sent to the reducer the reducer users collects the partial",
    "300-320": " statistics and determines the best split so basically the idea is that given the particular feature and a particular split value if I have our big input file that is split into chunks each chunk is sent to the to the to a separate mapper this mapper will compute some statistics of how this given attribute and this",
    "320-340": " given value how what is the quality of the split all all these partial statistics will be sent to the reducer who will then output the final score or the final quality of the of the given split and then Master then decides which split to take and grows the threee by",
    "340-360": " additional level so this is generally the idea the mapper will allow us to take this big data file split into chunks compute subsets uh statistics of it on subsets of the data reducer collects the thing and lets the master know to make the final decision so the in terms of the the overall infrastructure the mapper loads",
    "360-380": " the model into um the model we build so far into the memory and then um also information on which attribute splits to consider and then a given mapper only sees a subset of data um the mapper drop drops each um data point uh to the appropriate Leaf node L and for each",
    "380-400": " Leaf node L it keeps the statistics about what is the data that reach tell and what is the data that is in the left and right sub three under under the given split right so the mapper sees a set of splits s um sees a subset of the of our data set um and then for each",
    "400-420": " Leaf node it keeps track of which data goes to the left kind of left of the LEF node and what data goes to the right of that given LEF node and then as I mentioned reducer will take all this partial information um combine it and determine the best split for each node in the three at the given level that we",
    "420-440": " are currently building right so the idea is that we are currently looking let's say at the third level or fourth level of the three so the way now that I told you about what the mapper is doing and what the reducer is doing at one one level of the tree now how are we kind of building the whole threee the whole procedure breaks down into three types of map",
    "440-460": " reduce Jobs first is the map map reduce job that we call initialization we call this once at the beginning of the building procedure and basically the idea here is for each attribute to identify values to be considered for splits then we will have a sequence of map reduced jobs that are basically finding the best",
    "460-480": " splits uh so we have a one map reduced job for every level of the three here the idea is that we want to find the best split uh when there is too much data to fit in memory and then we also have this last part which we will call inmemory build this is the thing we want to run at least once where basically it's similar thing to the build sub",
    "480-500": " three function I already showed you and this idea the idea here is that once the data is small enough um we just want to go and build the whole sub three at once"
}