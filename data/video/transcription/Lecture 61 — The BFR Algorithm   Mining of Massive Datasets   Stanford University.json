{
    "0-20": " welcome back to mining of massive datasets we're looking at k-means algorithms for clustering and we have sort of seen that the k-means algorithm is you know produces good clusters and it's linear for each scan or each round of the k-means clustering but it can take a large number of rounds to converge the k-means clustering algorithm so our question is can we come",
    "20-40": " up with an algorithm that can do something like k-means clustering but in one pass over the data if you have a really really large data set that doesn't fit in memory the answer to that question is the bradley fire rhino or BF r algorithm which are going to cover in this segment BF r is a variant of k-means and it's sort of optimized for really large data sets that are actually",
    "40-60": " discussing there the data set is so large that it doesn't fit in memory and so you really want to just scan the data set one in disk and come up with the clustering however to accomplish this the B of our algorithm makes a very strong assumption it assumes not only that they have a Euclidean space which is an assumption that all k-means",
    "60-80": " algorithms make but it also assumes that each cluster is normally distributed around a centroid in Euclidean space let's see what this means let's start with the fresher on a normal distribution here's a variable X and here it's and here is its frequency distribution assuming that X is normally distributed",
    "80-100": " let's say X has mean 0 and standard deviation Sigma then if you look at the frequency distribution it's it's apparent that you know we sort of end up in this bell-shaped curve and most observations of X are going to cluster very closely around the mean zero in",
    "100-120": " fact about 68% of the values which is you know here for zero point three four plus zero point three four so that 68% of observations are going to lie within one standard deviation of the mean plus or minus 1 Sigma from the mean right and",
    "120-140": " about 95% of the values are going to lie at a distance of at most two Sigma from the mean plus or minus two sigma from from the mean and 99% of observations are going to be at a distance of about 3 Sigma for the mean right so this is just a refresher it",
    "140-160": " should be evident to you know most of you should recall this from your elementary statistics classes now the the nice thing is you if we assume that the that the data is normally distributed in each dimension let's say",
    "160-180": " X here is a dimension and the mean of X 0 which means that the centroid in X dimension for a given cluster is at 0 then we sort of know that about 68% of the points lie within one standard deviation of the centroid along X",
    "180-200": " dimension and we can say the same thing about each dimension and we assume that all the dimensions are independent of each other so in general we can quantify the likelihood given you know given a point and given that a point belongs to a cluster we can quantify the likelihood of finding that point at a certain distance from the centroid of the",
    "200-220": " cluster and this is the property that we are going to use in the bfr algorithm as you know as it will become apparent later on now recall that although in this case you know I've shown a single standard deviation Sigma we don't make the assumption that the standard deviation is actually the same along every dimension each dimension actually has its own mean and its own standard",
    "220-240": " deviation now the implication of the of this idea that each dimension has its own mean and its own standard deviation is that the the clusters actually looked like ellipses that are aligned along the axis for example here's here is a",
    "240-260": " cluster the pink luster and the notice that it's with sort of a flat ellipse that's aligned along the the x axis which means that the the standard deviation is higher along the x axis than along the y axis for this particular cluster but at the green cluster here has you know it's more",
    "260-280": " elongated along the Y dimension which means that its standard deviation the Y dimension is more than its standard deviation along the x dimension whether the blue cluster is like looks more like a circle which means that it has roughly the same standard deviation along both dimensions so start with an",
    "280-300": " overview of the bfr algorithm remember that the data set that you're looking at is very very large so and it's actually on disk now the data is so large that it doesn't actually fit all of it doesn't really fit into memory right so what we're going to do is that we're going to page the data one chunk of the data in you know at a time into memory right",
    "300-320": " when you bring a chunk of data in and and read it we also the same time in memory going to maintain metadata about all the clusters that you've created thus far right we're gonna bring a chunk of the data into memory we're going to update the metadata about the clusters",
    "320-340": " and then we're going to throw that chunk of data away and then we're going to bring the next chunk of data into memory and we're going to continue with this process until we've gone through all the chunks of data at which point we've ended up at our final clustering okay this is the this is the high-level overview of the bfr algorithm this will",
    "340-360": " summarize the points are read from disk one main memory full at a time right and most points from previous memory loads are actually summarized by simple statistics since you don't have space to store all those points we summarized all those points with simple statistics and we'll see what these statistics are going forward to begin with we are going",
    "360-380": " to when we load the first set of data or the first chunk of data into memory we're going to select the initial case centroids for the k-means algorithm from that chunk of data using some sensible approach for example we might use one of the techniques that we saw in the in the",
    "380-400": " k-means lecture prior to this now as we go through we'll see that points fall into three three different categories that we need that you need to keep track of the first category of points is called the discard set and the discard set of points are points that are close enough to the centroid of an",
    "400-420": " sting cluster so if a point is close to the centroid of an existing cluster then we just update the metadata or the statistics of that clustered account for the fact that we've added the new point of the cluster and then we throw the point away that's why this set of point is called a discard set because we discard points that are close enough to the centroid of a cluster so we don't",
    "420-440": " need to keep them around any longer the second set of points is called the compression set or the CS now these are points that are close to each other but are not close to any existing centroid right so these points form a separate mini cluster of their own but they don't",
    "440-460": " really naturally merge with any of our existing k clusters so what we do with these points is that we compress them into many clusters and summarize them but you don't assign them to any of the existing clusters and and the the resulting you know come summarize sets or what we call the compression set or the CS and we'll see more about this",
    "460-480": " later finally we have the third set which is called a retain set and the retain set contains points that are kind of isolated they don't fit into any of the existing clusters and they're not close close together to any of the other points so they can't be you know made part of any compress and they kind of",
    "480-500": " outliers that are on their own and these are called the retain set or RS these are isolated points and you know we might eventually few in the future as we loaded more points we might find that they get close to a you know a compression set but at the moment they are too far away to be",
    "500-520": " assigned to and they just kept isolated as part of the retain set of points so the retain set of points if the only set of points that we actually have to keep track of in memory both the discard set on the compression set we can throw away the actual points and only keep metadata about the points in memory so here's a picture that'll help you visualize these",
    "520-540": " three class of points remember there are three sets that we need to keep track of the discard set the compression set and the retain set so here's you know cluster one of the K clusters that that that we're maintaining and here is its centroid and any point in that's close",
    "540-560": " enough to to the centroid of this cluster is assigned to this cluster and becomes part of the discard set we don't need to keep track of the point anymore now here are a bunch of you know smaller mini clusters now these are compressed set these the points in in these mini",
    "560-580": " clusters are close to each other so we could actually cluster them into these mini clusters but these but they have too far away from any of the existing clusters so I can't really merge the points in this mini cluster with the with this cluster here but I can keep track of them independently as a mini",
    "580-600": " cluster so the points that are in these mini clusters are said to be in the compressed set so for these points we just need to keep track of the mini cluster we don't need to keep track of the each individual point and so these points are in the compressed set and finally we have isolated points that are not close to any mini cluster they're",
    "600-620": " not close to each other they are not close to any of the existing cluster so we just keep track of them independently as individual points and these are the point that are the retain set or RS let's look at how we summarize this sets of points that are in the in the discard set remember the discard said it's this",
    "620-640": " particular discard set pertains to one of the K clusters and we're keeping track off so for each of the K clusters we we keep track of the number of point in the cluster let's go let's say let's call that n we keep track of vector sum and sum is just the sum of the points",
    "640-660": " that are in the cluster so there are n points in the cluster and we sum them dimension wise and the sum vector is just the sum of all the points that are in the cluster so the eyuth component of the sum vector is the sum of the coordinates of the points along the IATA dimension and so",
    "660-680": " on and similarly the sum square vector you just a sum of the squares of the coordinates so the earth component of the sum square vector is the sum of the squares of the coordinates of all the end points along the iets dimension in a moment it will be apparent why we",
    "680-700": " maintain the sum and some square vectors it should be fairly apparent that we need to keep track of the number of points in a cluster notice that if these the number of dimensions we can represent a cluster by 2 D plus 1 values independent of the size of the cluster that the cluster can have 10 points or",
    "700-720": " 10 million points the number of values that we used to represent that cluster is 2 D plus 1 where these the number of dimensions why is this well we need one one point or one value for two to store n the number of the the number of point in the cluster the sum is a d",
    "720-740": " dimensional vector and the sum square is another d dimensional vector so overall we need two deep Loess 1 values to represent the cluster and this is independent of the number of points in the cluster now the centroid of the cluster is just the average along each dimension and the",
    "740-760": " the it's very simple to calculate the centroid we just have to go to the some vector and divide each component of the sum vector by n and the total number of points and then we end up with a vector that is just a centroid of the cluster we can also calculate the variance of the of the of the cluster along any dimension the variance of the cluster",
    "760-780": " along the eyath dimension is just given by this formula here you take the sum square values along the I at dimension divided them by N and then subtract the square of the sum values along that dimension divided by n this is from elementary statistics the the the the the formula for the variance from",
    "780-800": " elementary statistics and the standard deviation along the dimension I it's a square root of the variance along the dimension high right so you just take the the variance wrong dimension I and you take the square root of that and you get the standard deviation along dimension",
    "800-820": " high so now it should be apparent why we maintain the the sum and the sum square of vectors for for each cluster because using them and the number of values in the cluster it's quite easy to compute both the the centroid of the cluster and the standard deviation along every dimension next we're going to look at",
    "820-840": " how we actually do the clustering remember we're going to page in one chunk of data points at a time and we're going to update the cluster metadata and we just saw the metadata that we're going to that we keep track of for each cluster the end the sum of the sum square values so once we have a chunk of",
    "840-860": " points in in memory we could find the point that are sufficiently close to a cluster centroid and we'll see what sufficiently close means in a moment but let's just assume that that for now we know that given a point that it sits sufficiently close to a cluster centroid for one of the the K clusters which means that it is that which means at",
    "860-880": " that point is in the discard set we're going to merge that point in the cluster and throw the point of a and here's how we do that we we add that point to the cluster and when we add the point of the cluster we have to update the the n the sum and is sum square for that cluster to adjust n we just have incremented by",
    "880-900": " one to do indicate that we've added a value to the added a point of the cluster to increment some we just have to add that point to the sum value for the cluster and to update some square we said we'll compute the square of the the the values of the of that point along each dimension and add that to the sum",
    "900-920": " square vector for the cluster now you see the advantage of the n some and some square representation it's very easy to incrementally add value you know add points to a cluster and increment the statistics for a cluster without doing complex computations now the points that are actually close to this centroid of a cluster we've added to a cluster however",
    "920-940": " the points that are not close to any cluster we know we know how to deal with those points what we're going to do is to take these points the points that are this memory load of points but that are not you know that we've not added to the discard set and we're going to take those points and we're going to add in",
    "940-960": " the old retain set the those points from before that we've not done anything with and we're going to take these new points and the old retain set and we're going to use some main memory clustering algorithm to cluster them all you could use either k-means clustering or we could use hierarchical clustering it doesn't matter we use any memory",
    "960-980": " clustering algorithm and we cluster these points now once you do that clustering the clusters that that come out which we will call we will call these and mini clusters they go into the compressed set and the outlying points that don't fall in any cluster in this according to this main memory algorithm go into the new retain set now we have",
    "980-1000": " now we've added a whole bunch of mini clusters into the compressed set and it might happen that a two of the new mini clusters you know one of the new mini clusters a team that we're adding to the compressed set is actually very close to one of the existing mini clusters so we",
    "1000-1020": " might want to merge these these mini clusters in the in the compressed set and so that we will see in a moment what metric we might use to determine that mini clusters are close enough and and need to be merged right so we might consider doing that and and if this is",
    "1020-1040": " the last round you know this is not the last round we just stop at this point and we just read in the next chunk of data data points in the memory but if this is the last round then we take each mini cluster or each compressed set in the CS and we merge it",
    "1040-1060": " to its closest cluster centroid and we take each point in the retain set or the RS and we we assign it to to its near a centroid and that's the end of our clustering if this is not the last round we just keep the compressed set and the retain set as it is and we read in the next chunk of data points and we repeat",
    "1060-1080": " the whole process again so the end of the algorithm we've gone through the the entire data set exactly once we page the entire data set to memory exactly once and we produce K clusters and there are two questions",
    "1080-1100": " that we left unanswered during our discussion of the algorithm the first question is how do we decide if a point is close enough to a cluster that we will add that point in that cluster we handled a little bit about this during our discussion of the algorithm and now we're going to look at a way to determine if a point is close enough to",
    "1100-1120": " a cluster that we will add it to the cluster and make it part of the discard set the second question which we left unanswered is how do we decide whether too many clusters in the compressed set are close enough that they we deserve to be combined into one mini cluster or whether we should just leave them as separate mini clusters let's look at",
    "1120-1140": " that next let's start with the first question which is how do we decide whether a point that that we merely read in is close enough to one of the cluster centroids that we decide to add that point to that cluster the key concept",
    "1140-1160": " here is something called the Mahalanobis distance and the Mahalanobis distance quantifies the likelihood of a point belonging to a centroid let's and the Mahalanobis distance depends very crucially on our assumption about the",
    "1160-1180": " points being normally distributed about the centroid along each dimension let's say cluster C has centroids C 1 through C D remember there are D dimensions and the the value C 1 through C D represents the the mean along each of the D",
    "1180-1200": " dimension so the centroid of the cluster is C 1 through C D and let's say the standard event dimensions along those dimensions along standard deviations along those dimensions are Sigma 1 through Sigma D and let's say we have a point X 1 through X D now what we need to",
    "1200-1220": " determine is if this point P is close enough to the cluster C so that we can add the point P to the cluster C or is it too far away to be considered part of the cluster C we're going to define the normalized distance of a point along",
    "1220-1240": " dimension I from the centroid to be y I which is X I minus CI divided by Sigma I we take the the value of the point along the dimension subtract the centroid along the dimension and divide by the standard deviation along that dimension and this gives the normalized distance",
    "1240-1260": " along that dimension now consider the point along dimension I that is at a distance of one standard deviation away from the centroid what this means this that X I minus CI is equal to Sigma but if excitement of CI is equal to Sigma then the normalized distance and dimension I which is why I is equal to",
    "1260-1280": " one so why I measures the number of standard deviations away from the mean a long dimension I the Mahalanobis distance of point P from cluster C is the square root of the sum of the squares of the normalized distance a so",
    "1280-1300": " I I along each dimension let's see what how we use the Mahalanobis distance now suppose Point P is one standard deviation away from the centroid along each dimension now we saw that in that case each Y I is going to be 1 since Y i",
    "1300-1320": " measures the number of standard deviations away from the centroid long dimension I and since there are D dimensions the the Mahalanobis distance or the point P from cluster C is going to be square root of D where D is number of dimensions now going back to our bell curve from the Gaussian or normal distribution the the probability that",
    "1320-1340": " each Y I is equal to 1 or that the a point is one standard deviation away along each dimension one standard deviation or less away in each dimension is 0.68 at 0.34 plus 0.34 here and so",
    "1340-1360": " the probability that that a point is that a Mahalanobis distance of square root of d or less is 68 percent or 0.68 of those points are in this area for each dimension right and similarly the probability that the Mahalanobis",
    "1360-1380": " distance of a point belonging to a cluster is within 2 times square root of D from you is this 95% and 99% of points have Mahalanobis distance less than 3 times square root of D so this sort of",
    "1380-1400": " motivates us to formulate the Mahalanobis acceptance criterion for the bfr algorithm which is to accept point P into the cluster C if its Mahalanobis distance from the center occurs from the cluster centroid is less than some preset threshold for example you might pick the threshold be 3 times square",
    "1400-1420": " root of D which means that it's the probability there's less than 1% probability that the the point is you know does not belong to that cluster the second question that we left open during our discussion of the algorithm was whether two sub clusters or mini",
    "1420-1440": " clusters in the compressed set should be combined or compressed into each other the simple way to address this question is to compute the variance of the combined sub cluster the way we do that is to just add the N sum and sum square values of the two clusters to each other",
    "1440-1460": " and then we can compute the the centroid and the variance of a combined hypothetical combined sub cluster if you find that the combined variance is below some threshold then that might indicate to us that these two clusters actually",
    "1460-1480": " do belong together and we might decide to combine those two sub clusters in the compressed set into a single cluster now this is just one approach to to this to answer this question and they could they be there there could be other approaches for example some dimensions may be more important than others you know in the in",
    "1480-1500": " the data for example height may be more important than weight in and so we might want to weight dimensions differently or you might want to use a density based metric like the study when we looked at agglomerated clustering"
}