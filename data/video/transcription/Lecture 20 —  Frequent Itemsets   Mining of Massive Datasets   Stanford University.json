{
    "0-20": " the study of how we find frequent itemsets was one of the earliest directions taken by data mining researchers and the our priori algorithm is probably the most cited work in the data mining field the motivation for this study was to determine unusual sets of items of people purchased together in supermarkets a no discussion of finding frequent itemsets would be complete",
    "20-40": " without mentioning the possibly apocryphal story of beer and diapers and will tell that one soon enough but in a rough outline we're going to start with a model of data called the Market Basket model where data consists of sets of items and sets our code baskets for a reason we'll discuss and a",
    "40-60": " set of items is called frequent if it appears in some large number of baskets we'll talk about Association rules which are statements that when a certain set of items are found in a basket than it is unusually likely that another item will also be found there and finally we'll give the operatory algorithm for finding frequent itemsets this algorithm",
    "60-80": " while it has been improved upon over the years is the fundamental starting point for all of these improvements the market basket model is essentially a representation for many many relationship between two concepts which we call items and baskets on one hand there's a large set of items an example",
    "80-100": " is all the different items Walmart cells of which there are hundreds of thousands this was the original purpose of the model analyzing the things people bought at a store however the model has many other applications if a few of which we'll talk about shortly and then on the",
    "100-120": " other hand there's a large set of baskets each basket contains a small set of items the original picture was that a market basket was a shopping cart a physical that is not electronic and customers would wheel their market basket up to the checkout the cash register would group all the items in the basket together in one bill and that",
    "120-140": " bill would be stored in the stores computer system thus by minding the contents of the various market baskets the store could learn what people bought together a hamburger and ketchup for example that in turn with the store figure out some ploys to increase sales think about what a store might do to exploit this information we're going to",
    "140-160": " return to the subject shortly the most useful and also most basic question to ask about data in the form of market baskets is to find those sets of items that appear in some minimum number of baskets to find the support for an item set to be the number of baskets of which",
    "160-180": " that item set is a subset we can give the support either as an absolute number or as a percentage of all the baskets data mining problem called frequent itemsets involves a number or percentage s called the support threshold any set of items with support at least as is",
    "180-200": " called a frequent item set okay here's a very simple example of data in the market basket model okay there are five items milk Coke Pepsi beer and juice the support threshold will be three that's an absolute number not a percentage of",
    "200-220": " the baskets here are the baskets we're using M for milk C for Coke and you can probably figure out the rest okay what are the frequent itemsets well almost all the sequencing Alton's are frequent each of the items except",
    "220-240": " Pepsi appears in at least three baskets for example beer appears in b1 b3 b5 b7 b6 + b8 okay Pepsi itself is not",
    "240-260": " frequent because it appears only in b2 and b5",
    "260-280": " there are also some frequent double Tain's for example milk and beer appear together in the four baskets shown and beer and coke also appear in four baskets and finally coke and jolla and juice appear together in three baskets",
    "280-300": " but no other double Tain's are frequent for example milk and juice appear together in b2 and b6 but in no other baskets okay also there are no sets of three or more items that are frequent so we're done we",
    "300-320": " found all the frequent itemsets one might have included the empty set that is surely a subset of all eight baskets but the fact is uninteresting and we usually ignore the empty set as I mentioned the the original application",
    "320-340": " for this sort of analysis was looking at things people bought together in a store in this case the items really are the items one might buy and the baskets are sets of items bought together by one purchaser okay there is a story that the first interesting discovery of a",
    "340-360": " frequent itemsets was that diapers and beer were frequently bought together and once you think about it it makes sense if you're buying diapers you probably have a baby at home if you have a baby at home you probably aren't going out to a barn to drink so you bring the beer home I have heard several people claim that they themselves discovered this so",
    "360-380": " I suspect that really nobody did and it's just an illustration of something that you wouldn't think of without a way to analyze massive amounts of data but which proves to be true and explainable rationally once you see it okay the first thing marketers did with information like this was to reorganize",
    "380-400": " their shelves they would put the diapers in beer near each other but put another item that made sense between them like chips but there is a more subtle way to exploit the data mining discovery suppose we run a sale on diapers but we raise the price of beer people will come",
    "400-420": " in to buy the cheap diapers and while they're there in the store they're likely to pick up some beer not noticing the price is too high or even if they do figuring it doesn't make sense to drive to another store just to buy beer everybody wins the customer doesn't lose money and the store gets more customers and on average receives the same amount",
    "420-440": " from each customer only the competitors of the store who don't have their own data mining experts lose incidentally a subtle point here is that there is causality operating and it's very hard to deduce causality from data that is if you don't think about the causes",
    "440-460": " underlying the data you might imagine that you could just as well run a sale on beer and raise the price of diapers but people who come in for the sale on beer are not going to buy diapers if they don't have a baby I just want to point out that these techniques are appropriate mainly for brick-and-mortar",
    "460-480": " stores a brick-and-mortar store needs to know that lots of people buy diapers and beer or else they're wasting time and money optimizing the sale of something that is rarely bought anyway that viewpoint matches well with the idea that we're looking for high frequency rather than correlation between rarely purchased sets of items online stores on",
    "480-500": " the other hand do not need to rely on high frequency since they can tailor this store differently for each customer thus entirely different forms of analysis or leader needed for online stores and we'll cover that as a separate topic",
    "500-520": " here's another problem that uses the same data model with an entirely different interpretation suppose our data is a collection of documents think of the sentences that appear in one or more documents as a basket the items are the documents themselves and the basket corresponding to a sentence contains all",
    "520-540": " the documents in which that sentence appears now what does it mean if a set of items appears together in many baskets the item set is a set of documents and these documents or items appear in a lot of baskets together that means there is a collection of documents in which a lot of the same sentences",
    "540-560": " appear documents that look like that may well be an instance of plagiarism it is interesting to note that in this case the items the documents are not in the baskets which are sentences and in fact it's the other way around but as we mentioned items and baskets form of many-to-many relationship and you can",
    "560-580": " always view such a relationship from either side however when it comes to the algorithms we'll discuss there is an asymmetry we assume that baskets contain small numbers of items while items can be in very very large number of baskets the algorithms would take too much time",
    "580-600": " if baskets contain large numbers of items because the work done processing each basket is normally quadratic in the number of items it contains here's another application involving documents let the baskets now be the documents and let the items be words we can think of a",
    "600-620": " basket a document that is as the set of words it contains but remember what we just said we have to be careful that the average basket doesn't contain too many items if documents are books for example they would contain thousands of different words but if documents or tweets or emails we're okay because",
    "620-640": " these are typically short in the case of tweets there necessarily short we can cut down on the number of words in a document by ignoring stop words as well the common these are the common that usually don't carry any significant meaning now what does it mean if a set",
    "640-660": " of items is frequent that means we have a set of words that appear together in a large number of documents by the way that's another reason to get rid of all the words that are not pretty rare or will just discover the words like the and and appear together in many documents that's big deal right on the",
    "660-680": " other hand if rare words are relatively frequently found in the same documents then there might be some connection between the words I'm supposing so that Brad and Angelina might be two such words or is that old news probably is anyway just to remind you of the scale",
    "680-700": " of the sort of problem we're thinking about if we're dealing with real market baskets a big store like Walmart will sell a hundred thousand different items and stores billions of baskets in its database for analysis on the other hand the web has billions of different words most of them by the way of misspellings and many billions of pages often the",
    "700-720": " problem of finding frequent itemsets is characterized as the problem of discovering association rules these are rules that say if a basket contains some collection of items then it is also likely to contain another particular",
    "720-740": " item the notation for Association rules that we use is shown here informally if we assert an association rule that says I won through I K in plies J we mean that if a basket contains all of I 1 through I K and it is likely to contain J as well the degree to which this event",
    "740-760": " is likely is called the confidence of the rule it's the fraction of the baskets containing I 1 through I K at also contain J for example here are the eight baskets we saw earlier a possible Association rule is this milk and beer",
    "760-780": " imply coke let's focus on the four baskets that have both milk and beer but we see that b1 and b6 do have coke while B 3 and B 5 do not thus two out of the four baskets",
    "780-800": " with milk and beer do have coke and the confidence of this rule is 50% one reasonable thing to do with market basket data is to find all Association rules that have a minimum support s and a minimum confidence C for some values",
    "800-820": " of s and C that you decide on before you run the algorithm the support of an association rule is the support of the side to the left of the arrow that is it is the number of baskets containing all the items on the left the hard part of finding Association rules is really",
    "820-840": " finding the frequent itemsets if an association rule like this has support s then the set on the left will be frequent with support s that is the set by 1 through I K but if the countenance of the rule is also high that is the",
    "840-860": " confidence C is close to 1 then the set of items with J the item on the right thrown in will have support CS now if C is large then CS will be close",
    "860-880": " to s say perhaps 1/2 of s ok and here's a recipe for finding all the association rules with support as in confidence C start by finding all the item sets with support at least C s also find the item",
    "880-900": " sets with support at least s that will be a subset of the first set of item sets let's focus on an item set in the first collection that is one with support at least C s suppose it has K plus 1 items as members there are k plus",
    "900-920": " 1 subsets of size K each form by removing one of the items I've abused the notation a bit by singling out one of them is J the item to be removed leaving I 1 through I K but in fact we have to do this k plus 1 times 1 for each of the items having removed J look",
    "920-940": " at the support of the remaining set I 1 through I K if it is at least as we might have an association rule with the set o that's it on the left and the k plus first item J on the right suppose the set without J has support s1 and",
    "940-960": " with J the support goes down to s2 then the confidence of the rule is the ratio s 2 over s 1 because that is the fraction of the baskets with I 1 through I K that also contain J if that",
    "960-980": " confidence is at least C then we have an acceptable Association rule we're going to let our finding frequent itemsets in a setting where the basket data is kept in a flat file not any sort of database system as I try to argue on the previous slides it is finding frequent itemsets",
    "980-1000": " that is the hard part of finding Association rules so even if our goal is to get Association rules and in many cases we really want only the free itemsets anyway not the association rules what we're going to talk from this point only about the problem of identifying the frequent itemsets we",
    "1000-1020": " assume the data is so big that it has to be stored on disk since reading data from disk often takes more time than what you do with the data once it is in main memory our primary goal will be to minimize the number of times each disk block has to be read into main memory we're also going to assume the data is stored",
    "1020-1040": " basket by basket rather than by item or in any stranger way and we're going to have to find for each basket all its subsets of a particular size we can do that in main memory once the basket itself itself is there we can use K",
    "1040-1060": " nested loops to generate all subsets of size K since we assume baskets are small and often K will be only one or two anyway we're not going to worry about the cost of doing this generation however you should be alert to the possibility that if you're asked to generate all subsets of size a hundred thousand from a basket with a million items you just couldn't do it okay so",
    "1060-1080": " here's a picture of what we imagine the file looks like items have been coded as integers so the file is a sequence of integers we need to do a way to indicate where one basket ends and the next begins so we might use an integer like minus 1 which we suppose can't represent",
    "1080-1100": " an item as the separator for baskets as we mentioned we can focus on the number of times the disk block is moved between disc and main memory turns out that the algorithms we will study each operate in passes during a pass the entire file is",
    "1100-1120": " read block by block in order a surrogate for the the cost of the algorithm is thus the number of passes the number of disk i/os is is that number number of passes times the number of blocks that the file of the basket occupies",
    "1120-1140": " another non-obvious point is that for the algorithms we consider the limiting factor is usually how much main memory is available so for example it is common to have a pass where the file of baskets is read and as we read the baskets we count all the pairs of items contained within that basket we need to have a",
    "1140-1160": " place in main memory to count each pair so that means at least a few bytes prepare if we have a hundred thousand items then there are five billion pairs at four bytes per count that's 20 gigs of main memory okay we can do that by a",
    "1160-1180": " little extra for a machine or use several processors but if we have a million items that's a half a trillion pairs and we can't really manage all the counts in main memory and if it isn't obvious using disks to store the counts will not work the counts after we have",
    "1180-1200": " to increment or essentially random so if even half the counts need to be on disk at any time we have a 50% chance of needing to disk i/os with every increment I'm going to concentrate on how you find frequent pairs of items often finding",
    "1200-1220": " frequent items that is sets of size wonders or Singleton's is not too hard because there aren't so many items that we can't count them all in main memory as we make a pass through the baskets but is also common for the number of pairs to be too large to count them all in main memory you might think that",
    "1220-1240": " there are even more triples of items than there are pairs you'd be right however the algorithms will cover exploit the fact that once you have the frequent pairs you can eliminate most of the triples and not count them at all one might ask why there shouldn't be lots and lots of frequent triples the reason is that if we're going to bother",
    "1240-1260": " to do a frequent itemsets analysis we don't want so many answers that we can't even think about them all as a result it is normal to set the support threshold high enough that it is hard for a large item set to be sufficiently frequent as a result most of the sets that we meet that will meet the thresholds will be Singleton's or double - the bottom line",
    "1260-1280": " is that we're going to concentrate on algorithms for finding pairs the extension to larger item sets will be given once and can be used with any of the algorithms we discuss let's start by talking about what we might call the naive algorithm we want to read the",
    "1280-1300": " baskets in some number passes so why not use just one pass and count all the pair's in main memory we mentioned this briefly but just to make sure we understand what happens when we process a basket we use a double loop to generate all the pairs of items in the basket and for each pair we add one to",
    "1300-1320": " its count this algorithm actually works provided there is enough space and maintenance of items the number of bytes we need is roughly the square of the number of items in our data that is the number of pairs of items is the number of items choose to or approximately half the square of the number of items and if",
    "1320-1340": " we can count each pair in two bytes which is possible if the threshold is no more then two to the 16th then the number of bytes we need is exactly the square of the number of items if we need four byte integers to count then we need twice that Square and just to recall the",
    "1340-1360": " typical number of items if you're Walmart the number of items is about a hundred thousand so you might be okay but if you're dealing with items as webpages then you're definitely not okay before we proceed I need to talk a little more detail about how you organize main memory to do the counting",
    "1360-1380": " of pairs there are actually two approaches and which is better depends on whether it is likely or unlikely that two given items ever appear together in a basket one approach is to maintain a triangular matrix I'll talk about this on the next slide but the idea is to",
    "1380-1400": " maintain a two-dimensional array where a of I and J is only there if I is strictly less than J the second approach is to keep records",
    "1400-1420": " with three components IJ and C meaning that the count for the set of items I and J is currently C you organize this collection of records by indexing on I and J so given a pair IJ you can quickly find its record and increment its count or just read its count if that's what you want",
    "1420-1440": " the triangular matrix approach requires four bytes per pair of items I'm going to assume from here on that integers require four bytes even though if as we just mentioned it is okay to keep them small then fewer bytes could be okay it",
    "1440-1460": " is even possible in some circumstances that you need more than four bytes but I'm not going to worry about that case either on the other hand if we keep a table of triples then we need 12 bytes prepare for each four IJ and C but the advantage is that a pair only needs a",
    "1460-1480": " record if it appears in at least one basket as a result the space requirement for the tabular approach is 12 bytes per existing pair but not possible pair here's a picture of the difference for the triangular matrix you need four",
    "1480-1500": " bytes per unit area of the triangle for the tabular method you need 12 bytes times the fraction of the area that represents pairs actually present so if more than one third of possible pairs are present in at least one basket you prefer the triangular matrix if you expect fewer than a third of the",
    "1500-1520": " possible bill pairs to be present in the data then you should go for the tabular approach now let's look at how we construct the triangular matrix given the given that this structure is not exactly built into most programming languages first of all we'll assume the items are represented by consecutive",
    "1520-1540": " integers starting at one if items in your data are represented by their names or by integers that are not consecutive then you need to build a table to translate from an item's name the data to its integer a hash table whose key is the original name of the item in the data we'll do this just fine",
    "1540-1560": " okay we're counting sets of two items so we can think of each set as an ordered list of length two that is will count all I and J such that I is less than J I want to use an order for the pairs that look like this if there are n items in",
    "1560-1580": " total then first come the N minus 1 pairs whose small a member is one that's these these pairs are in order of their larger member then come the N minus 2 pairs whose smaller member is 2 again",
    "1580-1600": " these are ordered by the larger member then the N minus 3 pairs with 3 is the smaller and so on what we really have is a 1 dimensional array and we need a function that takes I and J where I is less than J and turns it into the position in the array belonging to this pair here's the magic formula I'll let",
    "1600-1620": " you figure out why it works but for example if N equals 10 let's look at the pair 3 5 I claim it is at position 2 that's I minus 1 I is 3 of course times",
    "1620-1640": " 10 that's n minus I over 2 that's three halves plus 5 that's J remember J is 5 and then minus 3 which is I you work that out it's 19 ok that makes sense",
    "1640-1660": " because there are 9 pairs ahead of the payer 3 5 that have a 1 as the lowest the lowest member of the pair there are another 8 pairs that have 2 as the lowest and then there's one more pair 3 4 that comes ahead of 3 5",
    "1660-1680": " the total number of pairs that are represented as n choose two or about N squared over two and we use four bytes per pair so the number of bytes needed is about two N squared if we use a table of",
    "1680-1700": " existing pairs then we need space 12 P for the triples where P is the number of pairs that occur in the data as we mentioned this amount of space is less than that of the triangular matrix as long as P is at most one-third of the",
    "1700-1720": " possible pairs or about a P less than N squared over 6 however that for this method we also need an index of the pairs of integers so that we can quickly find the record for that pair this",
    "1720-1740": " structure also requires space depending on how we implement it for example we might implement a hash table in which case we need pointers to link a list for the for each bucket so let's say here's the hash table here's a pointer to the first element of an i J and a C and then",
    "1740-1760": " a pointer to the next element and so on that would add another 4 bytes prepare on in particular a not counted in the 12",
    "1760-1780": " P is all the space for the bucket headers that's probably not too much but then another integer at least for each of the links that is going to lead to a cost of about 16 P rather than the the 12 P and and in addition again this is",
    "1780-1800": " the the cost for the bucket headers which is probably negligible"
}