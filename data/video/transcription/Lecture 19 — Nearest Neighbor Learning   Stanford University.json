{
    "0-20": " so we are starting a new module in this class the module is about large-scale machine learning so what we will talk about is a class class of methods for working with data that falls under the umbrella of machine learning where the idea is that based on we want to analyze the data so that based on the features of the data we want to predict certain properties of",
    "20-40": " items or data points that we haven't yet seen or that we are going to see in the future and the simplest method that we will talk about today is called nearest neighbor classifier and based on this idea of nearest neighbor classifiers we will then develop this into more complicated and more advanced machine learning models the idea of machine",
    "40-60": " learning or what is known also as supervised learning is that we would like to learn a function that is basically making predictions for us so in an abstract way we would like to estimate based on the data a function f of X so that given X we can predict Y now of course the question is water what is X and what is y most generally there",
    "60-80": " are kind of two way to separate things what Y can represent if Y represent a real number then this is what is known as a regression right based on some values of X I want to predict a real number Y so for example if I would want to given someone's age and someone's",
    "80-100": " ethnicity and so on maybe I would want to predict their life expectancy then this would be an example of a regression problem however Y can also be a categorical variable right which you can think of it as a binary variable or something like that right this predicting a categorical variable is",
    "100-120": " known as a problem of classification right so for example one one case of classification would be that I give you a document and I or an email and I want to ask you is this spammy email or not right so given a document you want to decide return a binary well variable 0 or 1 where 0 means not spam and 1 means yes this is spam let's discard this",
    "120-140": " email of course you can also predict kind of more complex objects for example you can sometimes why could be a ranking on ordering of things or it could be for example if you are working with sentences I could be a whole parse tree but what we will focus on in our lecture is mostly on classification so basically given a set of XS decide what is the",
    "140-160": " label the binary label of every X and what is why do we call this whole thing supervised learning is because we think of our data as labelled we are thinking that we are getting a set of many pairs x and y where access is x is the data that we are getting and Y is the",
    "160-180": " variable or the class that we want to predict so we can think of s as a vector of binary categorical or real valued features and Y as the class let's say plus 1 minus 1 or a real number if you are working with regression and if you think about this case basically we can think of X as a set of features",
    "180-200": " representing our data point and Y is the property of the data point we want to predict so if I want to predict spam then for example X could be a set of words in the email and Y is a binary variable that tells us whether that email is spam or not if I would want to for example model human diseases",
    "200-220": " I could X could be a set of symptoms or set of characteristics of a patient and Y could be whether that patient suffers from that disease or not so this is the first idea the idea of having a set of features and then having the dependent variable that we want to predict based on that set of features using our function f another important idea is",
    "220-240": " that we will think of our data in some sense as coming as this big matrix right where we can take our features feature vectors and stack them together in a matrix and then we can think of our dependent dependent variables Y so the class values the TVM to predict as a long thin vector and of course what we",
    "240-260": " can also do then is to say based on this what we will call training data we want to estimate our function f so that whenever we observe some new unseen data x prime we will be able to predict what are the Associated class values with this unseen data right so the idea is",
    "260-280": " that we will want to learn our function f based on the training data set that is labeled in a sense that we have both X and wise and then in the future our hope is that we will we will get this new data set this that we call it a testing data set where we only know access and from access we will try to predict these",
    "280-300": " wise so we will always kind of think about the training stage of our algorithm and then the testing or the application stage of our algorithm so in this module we will talk about several different machine learning methods where we'll be kind of focusing on large-scale data in particular we will talk today about K nearest neighbor which is an",
    "300-320": " which is something that our method in a class of instance based learning and then we will also talk about support vector machines and decision trees and kind of the main question when working with machine learning methods is how do we efficiently train or build the model based on the based on the data so in a sense the main question that arises in machine learning is how do I find this",
    "320-340": " function f that takes the input features and predicts the class variable right so learning our estimating this function f is the hardest part of of machine learning so an example of instance based learning the idea here is that we want",
    "340-360": " to use existing these instances or existing data points to make predictions about unknown or unlabeled data points so the example of such a method is called nearest neighbor right where the idea is that we take all our training data all our XY pairs let's say in memory or on the disk and then whenever a new new query example let's call it Q",
    "360-380": " comes we find other examples X prime that are that are similar to it and then based on the value labels of those examples we also predict the value Y Y star for the for the given 3d point Q so what is interesting about nearest a",
    "380-400": " button is that it works both for regression and classification and if we think about recommender systems in particular collaborative filtering collaborative filtering is an example of a nearest neighbor classifier right there there the idea was that when a user comes we find K most similar users to our given user q then we look at what this other",
    "400-420": " came or similar users what are the movies they like and based on the movies this other users liked we are making a recommendation to our query user q so this is exactly an example of nearest neighbor where a query arrives we find nearest data points based on the labels of the nearest data points we kind of",
    "420-440": " try to combine those labels to talk about the label of the query point the simplest of all nearest neighbor classifiers is what is called a one nearest neighbor classifier right where the idea is that whenever we want to make a decide on a label of a given of a given data point we simply find that the",
    "440-460": " point most similar to it and and use that label as a prediction so in a in particular when we want to implement a nearest neighbor classifier there are several things we have to decide on first is we have to decide on a distance metric right how do we measure the similarities or distances between data points so for example in the case I will",
    "460-480": " show you here is let's assume we are using Euclidean distance then another thing we have to decide is how many neighbors are we looking at are we looking at one year's neighbor five nearest neighbors how many nearest points do we want to examine so in our case let's look at one nearest neighbor another important thing is how are we",
    "480-500": " weighing these different neighbors that we are combining together right so in in this case that I'll show you we won't worry about this just yet and then another important question is how do I then take all these nearest neighbors and combine the values into a single",
    "500-520": " point that I will use as prediction and in our case because we are just using one year's neighbor all we have to do is just predict the same output as at least as is the value of the nearest neighbor for example now if I show you how this works here I have a simple two dimensional data set where I can think",
    "520-540": " of the x-axis as the input feature and the y-axis is the value I would like to predict and blue points are my data points and the black line shows the output of a new one nearest neighbor classifier you basically see that for around every point we exactly just predict the value of that point and then if our data set is noisy our our",
    "540-560": " prediction jumps around quite a lot if our data set is nice and smooth we are basically predicting this almost like a step function so this seems to be working quite well in this simple example but we are seeing what the method is suffering from it is making lots of very spiky or sharp decisions",
    "560-580": " because we are only looking at the one nearest neighbor so we want to generalize this and maybe try to kind of use more nearest neighbors to average better so let's look at how that works right so if we want want to generalize now our method and call it a K nearest neighbor then now the idea is I we will",
    "580-600": " still use the Euclidean distance now how many neighbors should we look at we will look at K where K some number chosen chosen by the user how and then how do we combine the labels of all the K neighbors into the into one label for example right now let's just say that we our output will be the every job the average of the classes of the K nearest",
    "600-620": " neighbors so very simply for example here is our data set from the previous slide we are using here K equals 9 which means we are averaging together the y-value of the nine nearest points to our query point and given the blue data this is the predicted value that we",
    "620-640": " would make for example notice that now our predicted function our function f of X in in some sense if you like is much smoother than what it was before while nearest neighbor is a very simple method one thing that we haven't yet discussed is actually how do we got go",
    "640-660": " and find nearest neighbors so our the task here is basically given a set of points in some in some space so this is basically our data set our goal is to find us another set of points that are close to our query point Q and there are two types of queries we may want to ask one type of query is to say give me",
    "660-680": " nearest K points to our point Q and another way to ask a query would be the range search where we would say give me all the points that are inside some distance of Q in both of these cases and if solution would require a linear pass over the data so it would take linear time but we already know how to do this",
    "680-700": " better for example using locality-sensitive hashing we could we could find nearest neighbors in your in your constant time so that would be a good way how to really make nearest neighbor classifiers scale to large-scale data"
}