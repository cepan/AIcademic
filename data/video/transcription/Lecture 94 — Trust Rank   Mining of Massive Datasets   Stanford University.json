{
    "0-20": " so now we will start talking about how to combat the web spam and the method we will talk about is called trust rank so as we mentioned before there are two types of web spam there is the term spam and there is the link spam term spam you can think of it as it's very similar to the email spam filtering right basically we would like to identify what are the",
    "20-40": " terms what are the words that correspond to spam and for every web page we would like to automatically say whether it contains a set of those words and filter out the web page combating link spam however is is more more ex more expensive or more tricky the idea here is basically that we would want to detector Blacklist a set of graph",
    "40-60": " structures on the web that look like spam farms in particular what is the problem here is that kind of this links this leads to another War where if it would have such a filter that would try to identify structures that look like spams Farms is that then spammers would try to hide themselves and we would have to update that back Blacklist and",
    "60-80": " everything would go back and forth so we need a more Universal solution so the more Universal solution is called trust Rank and in its basic idea trust rank is simply a topic specific pagerank with a Teleport set to a trusted set of web pages right so when I say for example trusted set of web pages this will be web pages to which to which we trust and",
    "80-100": " that are not likely to be hacked or farmed so for example dot edu domains or from non-us schools would be such a such a way to identify a good set of trust web pages so let's think about this a bit a bit more and see what is the idea so",
    "100-120": " basically the the idea behind the trust rank is this is the notion of approximate isolation where the idea is that it's rare for a good page to point to a bad page right it's very rare that a good legitimate page would point to spam so the idea is that we want to select a set of seat pages from the web that we trust that they are good and",
    "120-140": " then we would want to compute some kind of similarity or importance or proximity of all the other web pages on the web to this to this trusted set and now if on some other web page is legitimate then it will be close to the trusted set of web pages of course what we are depending on here is that we have some",
    "140-160": " Oracle right some human to identify good trustworthy set of web pages and um and of course that the idea is that web spam pages are not in this web in this set of course this is a very expensive task because a user would have to go or a human would have to go through and do this do these labels so",
    "160-180": " in some sense we want to make this set of seed trusted web pages as small as possible so the idea is the following what we basically will be doing is we are doing the trust propagation so basically we have a subset of good seed pages that we identify as good as we will call them trusted pages and then we perform as I",
    "180-200": " mentioned before the topic sensitive page rank with the teleport set to The Trusted Pages what this really means is that now basically we are propagating trust across the links of the graph right and the trust can take value between 0 and 1 the same way as the as the pagelank score so one possible way",
    "200-220": " to identify spam spam would be the following we take the set of trusted web pages we computed the personalized or topic specific pagering score with the teleport set being the set of trusted web pages and now what we do is we compute the pagerank score of every other page on the graph and if the if the patient score of some page on the on",
    "220-240": " the web graph is smaller than some given threshold then we go and cut that page away from the from our data set and we say that that corresponds to spam the problem with this method though is that basically we go and cut away all the web pages that have low page rank scores with respect to the trusted set and now",
    "240-260": " of course some web page on the web can have a low pagerank score because this web page is new and and has just been born or this web page can have a lot a page rank score because it's spam so the question is can we why is this a good idea and maybe can we later improve on this idea so let's first talk about why is this idea of personalized pagerank",
    "260-280": " score uh from a given trusted set of web pages a good idea to detect link spend so first this notion is that we have this notion of trust attenuation where the degree of trust conferred by a trusted page decreases with the distance from that page in the graph right so kind of farther away I've given pages from the from the trusted set of pages",
    "280-300": " the lower the lower the trust it receives will be and another important notion that also works in our favor is the notion of trust splitting right where the larger the number of outlings from a page the less scrutiny a page author gives to each outlink in a sense what this means is that if a page has lots of trust but then has lots of",
    "300-320": " outlinks then distrust kind of gets split into these small chunks and distributed over um over the target pages right and this is exactly what is happening in the topic specific page rank with the trust trusted set of web pages being the teleport set so now let's quickly discuss how do we",
    "320-340": " go in practice to pick a seat set picking a seat set we have kind of two conflicting considerations in some sense we would want to make the seat set to be as small as possible why as small as possible because um human labeling web pages are as trusted or not is very expensive so we want to use as little of human time as possible on the other hand we would like",
    "340-360": " in some sense to be the seat set to be as big as possible because we want to cover all the good pages on the web so in a sense ideally you would like to put every non-spam page into our seed set but that would mean the seed set is is huge so the question is how do we balance out these two kind of competing or conflicting goals",
    "360-380": " the idea is the following right um for example imagine that we want to select a seed set of K pages one idea would be for example that we pick the K pages on the web according to the pagerank and the hope is that this the top small fraction of web pages on the web are really the the truly important",
    "380-400": " pages on the web and we label those as the seed set another another idea that I briefly mentioned before is that we use a set of trusted domains whose membership is controlled by some um trusted organization so for example.edu DOT Milo or dot gov domains these are the these are the domains that",
    "400-420": " not just everyone can can register so all the pages in these domains we would trust them to be good pages and that are not spammy and don't spend don't point to other spammy pages so one way to create a trusted set of web pages would simply be to take all the web pages at these domains what is also interesting now is that we can actually take our",
    "420-440": " initial idea of identifying web spam and extend it a bit and we will extend it by creating this notion of spam mess right so the idea is the following we will use the trust rank as a model and start with a good a set of good trusted pages and propagate the trust but now we will flip",
    "440-460": " our reasoning and we will kind of view use two complementary views our goal will kind of be to ask what fraction of page page rank comes from the spam pages right so what we would like to do is not to ask how what is your proximity to The Trusted part of the web but we would like to say what fraction or estimate",
    "460-480": " what fraction of pagerank score of a given page comes from the span right so in practice we don't know the answer to this question but um we need to estimate it so the way we can think about this is the following we can think that we have our web page that we are interested in here as a red circle we",
    "480-500": " have a trusted set of web pages and we have the full web and our goal is to estimate what fraction of um pagerank score of our red node here comes from the spam pages so what we can do is proceed as follows we can first go and compute R sub P where p is our red",
    "500-520": " red node where RCP is simply the page rank of our node p and then we can also compute the r plus of P which is the page rank of the same node where the teleport was done to The Trusted set of web pages right and now we can say that the spam mass of a given web page is simply RP minus r r",
    "520-540": " plus P what does this mean basically is we say we will compute the pagerank score of a page using the simple page rank where the teleportation is uniform we will also compute the pageling score of a page where where we always travel from this trusted set of web pages and now if a web page is is really spam then",
    "540-560": " this difference will be high right basically there is a lot of other web pages on the web that we don't trust that really boost the importance of that page so this way we can we can Define the notion of a Spam s of a of a page which is simply the ratio of the overall page rank score of the page and the the",
    "560-580": " amount of spam mass that that page receives and this way we would we could go and all the pages that have this spams fraction High we would Proclaim these Pages as spam and remove them from the for from our web Corpus and this idea is is better than the first solution to our problem because here the",
    "580-600": " the question whether a page is Spam or not does not depend on the absolute value of its pagerank score but kind of it depends on the relative value when we compare how much pagerank score of a page comes from The Trusted part of the web and how much of its pagerank comes from the untrusted part of the web and the ratio between these two quantities tell us how spammy is a web page"
}