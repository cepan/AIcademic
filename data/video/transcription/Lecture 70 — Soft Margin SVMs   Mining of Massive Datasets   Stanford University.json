{
    "0-20": " so far we implicitly assumed that our data is linearly separable what this means is we kind of assume that it's always possible to find a line that will perfectly separate positive uh and negative training examples however most of real data sets that are noisy which means that there is no such sep separating hyper",
    "20-40": " plane so the question is what happens to our for formulation when we have data that cannot be nicely separated for example imagine my data set here where I have um these data points that are kind of are on the wrong side of the of the bound decision boundary and there is actually no linear decision boundary here that would allow me to to draw a",
    "40-60": " line and put all the pluses on one side and all the minuses on the other side so when we are dealing with such such data where finding a nice linear separator is impossible and this is basically this happens in every data data set what we have to do is we have to change our formulation a bit and introduce a",
    "60-80": " penalty so the idea will be the following what we want to do now is we still want to maximize the margin this is the first part of our objective function but what we want to do is we want to have some parameter we will call this parameter C and uh plus the number of mistakes right so what we are doing right now is basically saying we want to find W that has good margin while also",
    "80-100": " makes a small number of mistakes right so the idea in a sense now give minimizing W gives us the gives us the line that has high margin while um the second part of the optimization problem the the one on on the right basically",
    "100-120": " wants to control for the number of mistakes right and the idea here is that we will have the value of c and set it and the goal is to find a separating hyper plane to find a line that both has good margin and makes a small number of mistakes and now of course the question is how do we penalize mistakes because",
    "120-140": " not all mistakes are of same severity and the idea is that not mistakes are equally bad which means we will be using margin in order to penalize them so how do we use margin to penalize mistakes we introduce this notion of slack variables and the way we think of",
    "140-160": " slack variables are basically this additional constraints or this additional penalties that we get for misclassifying a data point so the idea is that we have our separating plane and then the the value of slack variable or the value of penalty will simply be what what is the distance from the other side",
    "160-180": " of the separate of the margin to the to the data point itself right so in this casei the value ofi is this much for example for the misclassification of this data point plus the value ofai is all the way from the other side right this is kind of the how much we are",
    "180-200": " Massif misclassifying because it would require us kind of to move the that data point plus uh to the other side of the margin if you if we would want to make it U be classified correctly so what this means is now we are arriving to a new optimization problem right we still say okay what is our goal our goal is to",
    "200-220": " find W and B and we want to also find the values of the slack variables such that the norm of w is uh small which means the margin is large plus the the sum of this penalties um side is as small as possible while what what do we also require we also require that the",
    "220-240": " confidence in our classification is at least one and if it's um if it's not one then we have to subtract the value of C right so this is basically whenever our example is correct correctly classified the our confidence will be greater than one and we can in that case we'll be",
    "240-260": " able to set the value of site to zero otherwise um if that is not the case we will have to set the value of side to some nonzero uh value which means um we will incur some penalty uh in the optimization problem and um the idea here is basically that if we take a data",
    "260-280": " point XI and uh it is on the wrong side of the classification margin then we incur some penalty for misclassifying it and this data this optimization that we set it so far this is called the svm with soft constraints why soft constraints because now we can also allow for",
    "280-300": " misclassifications so one more thing that would be good to um get some intuition about is what is the role of this parameter C we call this parameter the slack penalty why do we call it the slack penalty is because it controls between the size of the cost of the margin how much are we wishing to make the margin B big and how much are we",
    "300-320": " penalizing our misclassification mistakes so the way we can think of C is the following if we set C to be infinite right what this basically means is that we only want to find W that separates the data so for example in our case if I have a data set here and I would set C",
    "320-340": " to be very big then this is the this is the decision boundary we would find right it's a decision boundary that nicely separates the data for example if we would set C equals zero right which would basically mean that um we don't really care about misclassifications but we just want to make our our W to be as",
    "340-360": " as short as small as possible then basically what what this would do it would ignore the data and the whole decision boundary would just be something that goes through the coordinate origin so it could be this um um while L that I show here but however if we choose a good value of C then we are nicely trading off between our our",
    "360-380": " line nicely separating the data so having large margin while also not making too many uh mistakes and for a good or appropriate value of C this is the line we would like to find right we still have um relatively nice separation between pluses and minuses while making uh one small mistake so having discussed",
    "380-400": " the value of the slack penalty and the formulation of the support Vector machine here is now what we call the support Vector machine optimization problem in in its natural form so the way we can think about it is the following our goal is to solve the following optimization problem where we want to find B and W such that the 1",
    "400-420": " half Square um of the of the of the square of the norm of W plus the slack penalty times our misclassification costs um the whole thing is minimized what is what is this doing the way we are thinking about this we are thinking of the first part of the optimization problem was um maximizing the margin",
    "420-440": " right we want the the length of w to be as small as possible and um we think of c as a slack penalty which is something that we have to kind of set by hand and it tells us how much are we trading off between fitting the data and um making the margin large and then the the last part is we call it empirical loss right",
    "440-460": " because this is saying how well are we fitting the data right so the left part of the equation is trying to maximize the margin find a good separator and the second part is to trying to say let's try to feed the data as well as possible and the cost of how well are we fitting the data is called the loss on how we",
    "460-480": " can now think about machine learning is that basically machine learning is trying to trade off between finding a a good separation between um the two classes while also minimizing the loss and in particular the loss that we have written here goes under the name of the hinge loss so we can think of support vector Mach to be using or minimizing",
    "480-500": " the hinge loss the reason why we call it the hinge loss is the following what we would really like to do is is the idea is that if we have our classification and um on the y- axis we plot the penalty the idea would be that if we misclassify we obtain a penalty of one right if misclassification means",
    "500-520": " that we predicted one class and the the true class was was of the other sign so the product of the two signs is uh negative while if we made the correct classification we would like um to obtain zero meaning no penalty so an idea ideal 01 loss would be you obtain",
    "520-540": " penalty of one if you misclassify and obtain penalty of zero if you classify correctly what is the penalty that support Vector machine is using is called the hinge loss the reason we we call it a hinge loss because there is this hinge at uh one which basically means if we are classifying the point correctly",
    "540-560": " and the point is away from the margin it's basically away from the decision boundary for at least value of one then we obtain the CL the uh the cost or the loss of zero however if the if the point is inside the margin or inside the classification boundary so can still be classified correctly but is too close to the boundary or is actually on the wrong",
    "560-580": " side of the boundary then we are incurring the penalty and this penalty is proportional to how far away is our point from the uh from this decision boundary so this is what is called the hinge loss and support Vector machine is exactly optimizing uh this H hinge lost",
    "580-600": " in the in the loss part of the term"
}