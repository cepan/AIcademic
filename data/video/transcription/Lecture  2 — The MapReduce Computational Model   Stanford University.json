{
    "0-20": " welcome back to mining of massive datasets we are going to continue our lecture on method use and take a look at the MapReduce computational model so before we look at the actual MapReduce programming model let's do a boehmem task now imagine you have a huge text document you know maybe teto terabytes long and",
    "20-40": " you want to count the number of times each distinct word appears in the file for example you want to find out that the word appears 10 million times and the word you know Apple appears four hundred and thirty three times right and some sample applications of this kind of toy example in real life are you know if",
    "40-60": " you have a big web server log and you want to find out how often each URL is accessed that could be a sample application or you may be building term statistics for a search engine right so but for now let's just imagine that we have this one big file that a huge text document and our task is to count the",
    "60-80": " number of times each distinct word appears in that file so let's look at two cases the first case is that the file itself is too large for memory because remember we said it so it's a big big file but imagine that there are a few enough birds in it so that all the word count pairs actually fit in memory",
    "80-100": " right how do you solve the problem in this case well it turns out that in this case a very simple approach works you could just build a a hash table I'll be with the index by word and and the hash",
    "100-120": " table for each word will will score it will store the count of the number of times that word appears so you the first time you see a word you initialize you know you add an entry to the hash table with that word and set the count to 1 and every subsequent time you see the word you you increment the count by 1 and you you make a single sweep through",
    "120-140": " the file and at the end of that you have the word count pairs for every unique word that appears in the file so this is a simple program that all of us have written you know many times in some context or the other now let's make it a little bit more complicated let's let's imagine that",
    "140-160": " even the word count pairs don't fit in memory right the files too big it doesn't fit in memory - but there are so many words indistinct words in the file that even you can't even hold all the distinct words in memory right now how do you go about solving the problem in this case well you could try to write some kind of complicated code but you",
    "160-180": " know I'm lazy so I like to use UNIX file system commands to do this and so here's how I would go about doing this so this is a UNIX command-line way of doing this they you know here the the command of",
    "180-200": " you know birds is this is a little script that goes through dog dot txt which is C which is a big text file and it outputs the words in it one per line and once once those words are output I can pipe them to do a shot and the sort",
    "200-220": " sorts the you know sorts the output of that and once you sorted all these the all occurrences of the same word come together and once you do that you can pipe it to another little handy utility called unique and one of the one of the",
    "220-240": " nifty features of unique is this in my ASD it's a - C option and when you do unique - see what unique - C does is it takes run of the occurrences of the same word and it just counts the occurrences of the same word so the output of this is going to be bird count pairs right so",
    "240-260": " and you know I I'm sure many of you have done something like this and if you've done something like this you actually done something that's like MapReduce right so this case actually captures the sense of MapReduce and the nice thing about this kind of implementation is that it's it's very",
    "260-280": " very naturally paddle light parallelizable as we will see in a moment so so let's look at an overview of MapReduce using this example right so the the first step that we did was we took the document which was our input and we wrote a script called words that",
    "280-300": " output one word to a line right and this is what's called a map function in in I in main MapReduce the map function scans the input file record at a time and for each record it pulls out something that you care about in this case it was words",
    "300-320": " and and the thing that you output for each record you can record output one or multiple things for each records and the things that you output are called keys the second step is is to group by key and this is the what the the thought step was doing it grouped all the keys with the same value together right and",
    "320-340": " the third step the is the the unique - C step that the reduce piece of MapReduce and once the reducer looks at all the key you know all the keys with the same",
    "340-360": " value and then it then it runs some kind of function in this case it counted the number of times the each key occurred but but it could be something much more complicated and once it does that kind of analysis it has an answer which which then writes out okay so this is Map Reduce in a nutshell now the outline of",
    "360-380": " this computation actually stays the same same for any MapReduce computation what changes is is that I change the map functions reduced function to fit the problem that you're actually solving right in this case for the word count the map and the reduce function were quite simple in some other problems the",
    "380-400": " map of the reduce functions might be more complicated here's here's here's another way of looking at it you start with with a bunch of key value pairs and so here's K KB K stands for key and B stands for value and the the the map step takes the key",
    "400-420": " value pairs and maps them to intermediate key value pairs okay so for example you run the map on the first key value pair pair here Kb and it it actually outputs two intermediate key value pairs and the the intermediate key value pairs did not",
    "420-440": " have the same key as input key value pair they could be different keys and they could be multiple off them and the values although they look the same here they both say B the values could be different as well and notice in this case we started with the 1 input key",
    "440-460": " value pair and the map function produce multiple intermediate key value pairs so they could be 0 one or multiple intermediate key value pairs for each input key value pair now let's do it again for the second key value pair let's apply the map function and it turns out that in this case we have the",
    "460-480": " one key value pair in the in the in the intermediate key value pair in the output and so on so we run through the entire input file apply the map function to each input record and create intermediate key value pairs now the next step is to take these intermediate key value pairs and group them by key",
    "480-500": " right so all the intermediate key value pairs that have the same key are grouped together so it turns out that there are three values with with the first key two values with the second key and so on and they all get grouped together and this is done by sorting by key and then grouping together the value with you",
    "500-520": " know the values with the same key and these are all different values although I use the same same symbol B here now once you have once you have these key value groups then the final step is reducer the reducer it takes a look at a single single key value group as input",
    "520-540": " and it produces produces an output that has this you know that has the same key but it combines the the values all the values for a given key into a single",
    "540-560": " value for example it could add up all the values in the audit could oracle multiply them or it could do we could take the average or can do something more complicated with with all the values for a given key and finally you output it all puts a single value for the key right and so",
    "560-580": " when you apply the reducer to the second key value group you get you get another output and so on and once you applied the reducer to all the intermediate key value groups you get the final output so more formally the input to Map Reduce is",
    "580-600": " a set of key value pairs and the programmer has to specify two methods the first method is a map method and the map method takes an input key value pair and produces and into a set of intermediate key value pairs 0 or more intermediate key value pairs and there",
    "600-620": " is one map call for every input key value pairs the reduce function takes an intermediate key value group the intermediate key value group consists of a key and the set of values for that key and the output can consists of 1 0 1 or multiple key value pairs once again the",
    "620-640": " key is the same as the as the input key but the value is is obtained by combining the input values in some manner for example you might add up the you know add up the input values and that could be the output V double prime",
    "640-660": " here so let's look at the the word count example and run that through the Map Reduce process again here is our big document and I hope you can see the text of this you know cotton but it doesn't matter you",
    "660-680": " can see that there are birds in there and so we're going to take this big document and you're going to take the map function that's provided by the programmer the map function reads the input and produces a produces a set of key value pairs and the key value pairs in this case not going to be the key",
    "680-700": " each word is going to be a key and the value is going to be the number one right so for example the bird the and one crew and one and so on and the word appears again planned so there's another one here and so on so these are the",
    "700-720": " intermediate key value pairs that are produced by the map function now the next step is the group by key step which collects together all pairs with the same key so you can see that there are two tuples to",
    "720-740": " intermediate tuples with the with the key crew and they those are collected together here there's one with you know with the word space there are three with the word and so on and they're all sorted and collected together in this york in this place here and the final",
    "740-760": " step is the reduced step the reduce reduce step collects together all the values so the new step add adds together the two ones from crew and then figures out the direct to a you know two occurrences of the word crew space has one there are three tuples with two one further they are all added together and",
    "760-780": " the output is 3 and so on right so this is a schematic of the the MapReduce word counting example now of course this this whole example does not run on a single node the data is actually distributed across multiple input nodes so let's take that into account and see here's here's the data the data is actually divided here into into multiple nodes",
    "780-800": " let's say the read the first portion of the file is so it's one and it's on one node the second portion of the file here is Chung 2 which is on a different node the third portion is Chung three and the fourth portion is chunk four and each of these is on a different node now the map tasks",
    "800-820": " are going to be run on each of these four different nodes there's going to be a map task that's run on chunk one that just looks at this portion the first portion of the file map tasks with ROM and chunk two that the that just looks at the second portion of the file and so on and the the outputs of those map tasks will therefore be produced on on",
    "820-840": " four different nodes let like so so here the here is the first chunk of map output the second chunk of map output which is on another node the third check of map output which is on a third node and the fourth chunk of map which is on yet another node right now the output of the of the map functions",
    "840-860": " are therefore spread across multiple nodes and what the system then does is that it it copies the the map outputs on to a single node and then so you can see the data from all these four nodes",
    "860-880": " flowing into this single node here and once the data is has flowed to the single node it can then sort it by key and then do the final reduced step okay now it's a little bit trickier than this unfortunately because you know you may not want to use a you know to to move",
    "880-900": " all the data from all the map nodes there may be a lot of it into a single reduce node and sort it there that might be a lot of in a lot of sorting so in practice you use multiple reduce nodes as well and you know when you run a MapReduce job you can say you know you can tell the system to use a certain number of reduced node let's say you",
    "900-920": " tell the system in this case to use three reduced nodes so if you use three reduced nodes then the then the MapReduce system is smart enough to split the the the output of the map into into three three into three reduced nodes and it makes sure that for any",
    "920-940": " given in this case the all instances of the regardless of which map node they start out from always end up at the same reduce node right so all instances of the whether it started from map node 1 or map node 2 ended up at reduce node 2 in this case and all instances of the",
    "940-960": " word crew regardless of whether they started from map node 1 or map node 4 ended up at reduce node 1 and this is done by using a hash function right so the system uses a hash function that hashes each map key and determines a single reduce node to ship that couple",
    "960-980": " to right and this ensures that all tuples with the same key end up at the same reduced node and once once tuples end up at a reduce node they get sorted as before on each reduce node and and and the result is created now on multiple reduce nodes so for example the",
    "980-1000": " result for a crew is now on is now on reduced node 1 the result for the is now 1 reduced node 2 and the results were shuttled and recently around reduced node 3 so the final result is actually now spread across three nodes in the",
    "1000-1020": " system which is perfectly fine because you're dealing with a distributed file system which know knows that your file is now spread across three nodes of system so you can still access it as a single file in your client and the system knows to access the data from those three three independent nodes one",
    "1020-1040": " final point before we move on from the slide is that the all this magic the MapReduce magic is implemented to use as far as possible only sequential scans of disk as opposed to a random access if you think a little bit carefully about all the steps that I mentioned about how",
    "1040-1060": " the map function is applied on the input files record by record how the sorting is done and so on a moment's thought will make it apparent that you can actually implement all of this by using only sequential reads of disk and never using random access of risk now this is super important because sequential reads are much much more",
    "1060-1080": " efficient than random accesses to disk if you remember from your basics of of database systems it takes much much longer to do random seeks than to do a single sequential access of a file and that's why the map to do the whole MapReduce system is built around doing only sequential reads of files and never",
    "1080-1100": " random accesses so here is the actual pseudocode for for the word count using MapReduce remember the programmer is required to provide two functions a map function a reduce function and this is the the map function right here the map function takes a key and a value and",
    "1100-1120": " it's output has to be interning to me a set of intermediate key value pairs now the key in this case is a document name and the value is the text or the document and the map the map function itself is very simple in this case it scans the the input document and for",
    "1120-1140": " each word in the input document value remember is the input document for each bird in the input document emits that word and the number one so it's a tuple whose key is the word and whose value is a number one and here's the reduced function the reduce function remember takes a key and a set of values",
    "1140-1160": " the set of values all correspond to the same key and in this case they it just iterates through all the values and and sums them up and the output has the same key and the value is there is the sum we",
    "1160-1180": " look at a very simple example of bird count using MapReduce now let's look at a couple more examples here's here's here's another example suppose we have a large web corpus that we've crawled and for each and we have a metadata file for a for a crawl and each",
    "1180-1200": " record in the metadata file looks like this it has a URL the size of the file the date for scrawled and then various other pieces of data now the problem is for each host we want to find the total number of",
    "1200-1220": " bytes not for each URL but for each host remember there could be multiple many URLs the same host name in the crawl and we want to find the number of bytes associated with each host not with each URL right clearly the the number of bytes associated with the host is just",
    "1220-1240": " the sum of the number of bytes associated with all the URLs for the host and this is very easy to implement in in MapReduce the mapper in this case the map function just looks at each record and it looks at the URL of the record and outputs the host name of the",
    "1240-1260": " URL and and the size and the reduced function just sums the sizes for each host right and at the end of it you will have this the the size of each host here",
    "1260-1280": " is another example let's say you're building a language model by you have a large collection of documents and you want to build a language model and and this language model for some reason requires the count of every five word sequence every unique five word sequence occurs in a large cops document earlier",
    "1280-1300": " we looked at counting each unique word this example asked for each fibered sequence it turns out that the solution is not very different the just a map function differs the map function extracts you know goes through each document and outputs every fibered sequence in the document and the reduced",
    "1300-1320": " function just combines those counts and add them up and then you have the output so I hope these simple examples illustrate how MapReduce works in the next section we are going to understand how the underlying system",
    "1320-1340": " to the implement some of the magic that makes MapReduce work"
}