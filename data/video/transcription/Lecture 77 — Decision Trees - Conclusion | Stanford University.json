{
    "0-20": " let's conclude and kind of create an overview of what have we learned about decision trees decision threes are really the single most popular data mining or machine learning tool the reason why decision threes are so so popular is the following first it is very easy to understand the structure of the decision tree right it's even in a sense easy to explain to someone why did",
    "20-40": " we make why the tree made a given decision right we can show them a series of splits and say because of these conditions this is the classification that has been made decision trees are also very easy to implement both the training stage is not that complicated and also once we have the Tre it's very use easy to use it for",
    "40-60": " classification so trees are also easy to use another thing that is important is that they are computationally also very cheap in a sense that classification is easy one caveat with um decision thre is that it's very easy to overfit so it's very easy to create complicated trees that that are very deep and become come",
    "60-80": " to intricate and don't generalize well to unseen data set um and what is also nice about decision threes is that they can do both classification as well as regression so decision threes have many good um uh ex properties another thing that decision threes are very useful for is what is called learning ensembles",
    "80-100": " right many times it turns out that it is better to build multiple decision threes Let Let each of the individual decision threes create its own prediction and then for example you use these predictions as votes or you average them together to make the final prediction so this is what it means that you learn an ensemble of decision trees and then you",
    "100-120": " kind of take take the individual predictions average them up or somehow aggregate them to make the final prediction so one method that that does this um is called begging the idea here is that we want to learn multiple threes over independent samples of the training data um and then prediction from each three um is considered we average",
    "120-140": " predictions from all the threes that we have and make the final prediction it turns out that in practice these kind of bagging approaches work much better especially if the classification or the prediction task is very hard so the idea is very simple right we will have our uh initial data set we will create many",
    "140-160": " random samples out of it um right let's call them D Star One D star two this can simply be um where we are sampling from d with replacement this will give us a new set of training data sets and for each of these we then train a separate three 3 1 32 3 three and then when a new",
    "160-180": " example X comes in we we make each of the threes to to give a to give a prediction and um and then we take the let's say the majority vote or the average to come out um with a final prediction so that is basically the the",
    "180-200": " whole idea of bagging there are a few interesting things how we could how we could use our planet infrastructure that we just talked about to do bagging so let's look at this the idea is that here right the three induction be begins at the root um and that all all the threes are of the bagged model are pushed into the map reduce queue and then all the",
    "200-220": " controller kind of has to do is to do the three induction over over a given uh data set of samples right one thing that we need to discuss when the data set this star is very huge how do we create subsamples of it here basically the idea is to use hashing right so the idea is that I can take training records and I",
    "220-240": " can uh use hashing I can use multiple hash functions and for every hash function I this creates me a separate different uh subsets of data where the idea is that records that hash into a particular range uh we use them to learn a threee um right this means that this",
    "240-260": " way the sample the same sample is used for all nodes in a given tree and um the way I Define it right now is that this is sampling without replacement what we what we want to do in bagging is to use sampling with replacement so the way we can do this is by using uh multiple hash functions um to Contin to end our",
    "260-280": " discussion of decision threes and of the machine learning topics I want to also briefly com compare the support vector and decision trees and which of these two method should methods should be used in a given case so support Vector machines are generally used for classification meaning usually binary",
    "280-300": " classification is the most common case um they are used for real Val valued features right because we think about a decision boundary in this ukian space so they are not for example useful for categorical features like colors or um weather types or something right svms are also very good when we have hundreds",
    "300-320": " or hundreds of thousands of features right when we have lots and lots of features so for example for tax classification svms are really great they also work really well when we have sparse feature sets what this means is that most of our feature vector or most of our features take value zero and what is nice about svms is that they have a",
    "320-340": " very simple decision boundary the what we talked about is a simple linear decision boundary which means um it's very hard to overit so example applications for support Vector machines are like taex classification spam detection ex cases in computer vision when where we are doing um um classification and so on on the other",
    "340-360": " hand decision threes are for much in some sense more um comp building much more complicated decision boundaries so decision trees are both used for regression and classification having up to let's say 10 different classes if we talk about classification um decision threes can",
    "360-380": " handle both real valued and categorical features um the most serious limitation of decision trees is that they can handle um only hundreds of features right and only dense features right so when we think about decision trees we think that maybe we have 10 or 20 or 100 different features and for every training examples all these features are",
    "380-400": " are filled in all right um and svms allow sorry decision trees allow us to come up with um complicated decision boundaries this is good on the other hand we have to be very careful because otherwise we will overfeed to the data so we need to do what is called early stopping uh some example applications",
    "400-420": " for decision threes would for example be user profile U classification where we can have user we describe each user with a small set of features and now we we would want to classify these users maybe as fraudulent or not and this would be an example for um uh application for decision another application would be for example",
    "420-440": " predicting what is called on the web page bounce prediction right whether a person land coming to a given page are they just going to leave the page or are they going to make the next uh click in this case again we could describe every person with a small feature vector and uh make the",
    "440-460": " prediction"
}