{
    "0-20": " so Grindr theory is the theory of LSH families wouldn't be nearly so grand if there are only one example the min hash functions that we could use the start but in fact there are many interesting base families to start with and the same and then or constructions can be used with any of them here we are going to introduce first the random hyperplane",
    "20-40": " LSH family which is appropriate when the distance measure is cosine distance and then we'll see an LS H family for the conventional Euclidean distance recall it the cosine distance between two vectors is the angle between them as we are measuring angles and degrees the value of a cosine distance is a number",
    "40-60": " between zero and a hundred and eighty not between zero and one as it was for Jaccard distance or a number between zero and infinity as it is from any other distance measures such as Euclidean we're going to learn about an LS H family of hash functions that works for cosine distance in particular for any distances d1 and d2 with D 1 less",
    "60-80": " than d2 we can view this family as d1 d2 1 minus d1 over 180 1 minus d2 over 180 sensitive this is exactly the way we we view the min hash functions except for the scaling factor of 180 because of our decision to measure angles and units of degrees the LSH family is called random",
    "80-100": " hyperplanes here's how we construct the random hyperplane hash functions each of these functions is defined by some vector thee call the hash function associated with vector V H sub V and I should warn you that our notion of a",
    "100-120": " vector is a little non-standard we are really picking a direction for a vector but not a scale factor that is if you double the components of a vector V we regard that as the same vector and in fact if you reverse the vector by multiplying its components by minus 1 you again have the same vector the value",
    "120-140": " of H of V applied to vector X is either plus 1 or minus 1 it is plus 1 of the dot product of V and X is positive and it is minus 1 if this dot product negative notice I'm not saying what happens if vnx are perpendicular that is the dot product is exactly zero in a",
    "140-160": " vector space with real valued dimensions the probability of this happening is essentially zero but to be precise we should state something to do when the dot product is zero say put X in the plus one bucket the LSH family is all the functions H sub V remember that really H sub V is a function that takes",
    "160-180": " two vectors x and y and says yes or no in this sense H of these says yes if and only if x and y are in the same bucket that is the dot product of X and and y with V have the same sign notice that we're really okay thinking of all multiples of a vector V is the same vector even if that multiple",
    "180-200": " is a negative number like minus 1 because all multiples of V give the same yes or no answers about a pair of vectors x and y analogous the theorem about min hash functions in jacquard distance we have the following theorem for any of the random hyperplane hash functions the probability that H of X",
    "200-220": " equals H of Y is 1 minus 101 180th of the angle between x and y this picture explains why we can get cosine distance from the random hyperplane hash functions first let us look at the plane",
    "220-240": " containing the two vectors x and y those cosine distance we want to determine any vector V has a hyperplane normal to V that is the set of all points whose dot product with P is 0 this hyperplane is",
    "240-260": " what we are really choosing when we choose V which is why the method got its name random hyperplanes the dashed line is the intersection of this hyperplane with the plane of x and y the vector we show as V is really the projection of V into the plane of x and y the particular random hyperplane we chose as the",
    "260-280": " property that it's intersection separates x and y that is x is in the same direction relative to the dashed line while y is in the opposite direction thus the dot product of X with V will be positive in the dot product of Y with V will be negative now here's what it",
    "280-300": " looks like when the random hyperplane is such that both x and y are on the same side of the hyperplane the one shown is a blue dashed line note that what is important here is not that the blue vector V is between x and y that's a coincidence what is important is that x and y are on the same side of the dashed blue line",
    "300-320": " and as a result the dot product of both x and y with v have the same sign has shown that sine is positive but if we replace the V by minus V like this then the blue vector would be reversed",
    "320-340": " and the dashed blue line would be the same both dot products of X and and y with V would be negative but what is important is that they would still have the same sign so suppose theta is the",
    "340-360": " angle between x and y the hyper plane makes some angle with x and y we can think of all the possible angles of this hyper plane starting out with the direction that coincides with y it can then sweep out 180 degrees sort of like this until we come to the continuation",
    "360-380": " of Y now what is the probability that the situation will look like the red rather than the blue it's theta divided by a hundred and 80 that is the red case",
    "380-400": " is when H says no so the probability of saying yes which is any hyperplane in this area is one minus theta divided by in 80 here's how we use random",
    "400-420": " hyperplanes to find similar vectors in our data set where similarity now means that lo cosine distance first we pick a number of vectors at random the selection corresponds to picking some random permutations to serve as min hash functions but here instead of min hashing we compute the dot product of",
    "420-440": " each vector in our data set with each randomly chosen vector to get a sequence of Plus Ones and minus ones called a sketch the sketch is analogous to the min hash signature we can apply LSH to our sketches by thinking of them as a matrix dividing the matrix matrix into into bands and proceeding as we did from",
    "440-460": " n hash signatures however with the theory of LSH families available we have options we can start with the LSH family of hash functions defined by each possible random hyperplane and then apply the and the nor constructions in whatever order we desire well we shall",
    "460-480": " not go into this reasoning it turns out that you can avoid having to pick random vectors V from the space of all possible vectors of some dimension D it is sufficient to let each component of each vector be either plus 1 or minus 1 that makes the calculation of dot products easier since there is no multiplication involved just sums and differences of",
    "480-500": " the components of the vectors to which hashing by random hyperplanes is applied now let's switch our attention to an LS h family for the normal euclidean distance we'll pick lines in however many dimensions our space has each possible",
    "500-520": " hash function and the LSH family corresponds to a line the line will be partitioned into segments of some length a each segment corresponds to a bucket each point X in the D dimensional Euclidean space is hashed by projecting",
    "520-540": " it onto the line for this hash function that is find the hyperplane perpendicular to the line that goes through the point X and see where on the line the hyperplane intersects the line that is the projection of the point X onto the line the reason this works is an LS H family of hash functions is that",
    "540-560": " points that are close to the space must always project onto points that are close and on the line thus they have a good chance of falling into the same bucket although by bad luck their projections might be separated by a bucket boundary on the other hand if points are far apart they might fall into the same bucket if the line between them was almost perpendicular to the line that defines",
    "560-580": " the hash function but most likely they will be in different buckets so here's a picture of what is going on we're going to look at only the two-dimensional space for simplicity but similarity ideas will work in any number of dimensions we see a randomly chosen line divided into buckets of width a that's",
    "580-600": " this and here are two points at distance D here and here when we project them onto the chosen line the distance between them is d cosine theta where theta is the angle the chosen line makes with the line between the two points",
    "600-620": " notice that if D cosine theta is bigger than a there is no chance that the two points fall in the same bucket if D is very much greater than a theta must be almost 90 to 90 degrees for the points to land in the same bucket on the other hand if D is much smaller than a there",
    "620-640": " is a good chance that the two points land in the same bucket since the cosine theta is surely no bigger than D the probability that no bucket boundary separates the projections of the two points is at least 1 minus D over a so if two points have Euclidean distance at",
    "640-660": " least 2a then there is zero chance they fall into the same bucket unless theta is in the range of 60 to 90 degrees since theta is random in the range 0 to 90 there is at most 1/3 probability of sharing a bucket and thus at most 1/3 chance that a randomly chosen hash function from the LSH family will say",
    "660-680": " yes on the other hand if points are a distance at most a over two then they share a bucket with at least 50% probability that is the probability of a yes answer for these two points is at least a half we can combine these two observations to say that for any value of a we can have an a over 2 to a 1/2",
    "680-700": " 1/3 sensitive family of hash functions for Euclidean distance in two dimensions however there is something unsatisfying about that analysis form in hashing or random hyperplanes we were able to start with a DEP Q sensitive family for any D",
    "700-720": " and E as long as the U is less than E and we can then drive Peter 1 and Q to 0 using the and and or constructions here we don't seem to be able to get de to be as close as we want we seem to need D to be at least 4 times e but we were being",
    "720-740": " overly pessimistic in fact as long as D is less than E we know that the probability that points a distance d have a better chance of falling into the same buckets than dew point at a larger distance E as a consequence we know that",
    "740-760": " for any de with D less than E the hash functions based on on lines and bucket size a is a DEP Q sensitive family for Euclidean distance for some probabilities P and Q such that P is",
    "760-780": " greater than Q starting with this family we can amplify the probabilities to make be as close to 1 as we like and Q as close to 0 as we like while still allowing de to be as close as we like as long as II is strictly larger than D"
}