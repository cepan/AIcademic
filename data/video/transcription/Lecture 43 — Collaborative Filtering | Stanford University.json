{
    "0-20": " welcome back to mining of massive datasets we continue our discussion of recommender systems in the previous lecture we looked at content-based approaches to building recommender systems in this lecture we are going to look at another popular approach called collaborative filtering the basic idea behind collaborative filtering is very simple suppose we have a user x2 whom do",
    "20-40": " you want make recommendations what we're going to do is we're going to find a group of other users whose likes and dislikes are similar to user X for example suppose you are doing movie recommendations now this group of users you know like the same movies that X likes and dislike the same movies that X dislikes we call this",
    "40-60": " set of users the neighborhood of user X once we find the set N of users or the neighborhood of user similar to user X we find other movies that are liked by a lot of users in the set N and recommend those items to the user X so that's the basic idea behind collaborative filtering the key trick is to find the",
    "60-80": " set of users that are similar to user X the neighborhood of user X and to do that we need to define a notion of similarity between users let's see how to do this so here so here's a simple example with 4 users a B C and D and a bunch of movies ok a",
    "80-100": " HP one stands for Harry Potter one HP two stands for Harry Potter 2 and so on and you can imagine whatever names you want for the other movies as W stands for Star Wars so for example user a has rated three movies a Harry Potter one Twilight and Star Wars 1 and he's given",
    "100-120": " the ratings for 5 and 1/2 these movies imagine they are rating on a scale from 0 to 5 stars here and user B has rated three movies user C has rated three movies and user D has just rated to movies let's call the the set of ratings for a user the users rating vector for example user s rating vector is what",
    "120-140": " I've highlighted here now if you have two users x and y with rating vectors Rx and ry what we really need is a similarity matrix sim of XY that at the rating banker vectors Rx and ry now the interesting thing here is that there is lots of movies that a has not",
    "140-160": " rated and lots of movies that B has not rated so the key in defining the the similarity function is how we deal with these unknown values now once we define the rating vector we'd like to define it in a way such that it captures a very simple intuition it captures intuition that users with similar tastes have higher similarity than users with",
    "160-180": " disabilities for example in this case notice that a and B have rated only one movie in common Harry Potter one however they both rated the movie fairly highly whereas users a and C have activated two movies in common Twilight and star wars 1 but their ratings are very dissimilar",
    "180-200": " a seems to like Twilight while C doesn't like it while a hate star wars 1 ylc loves it so we'd like to you know it seems intuitively that users a and C at dissimilar while users a and B are similar and so we like to capture this intuition when we define the notion of",
    "200-220": " similarity we'd like the similarity of a B to be higher than the similarity of a Fe let's see how to do this so the first option that we try is Jaccard similarity which we are familiar with from previous lectures so just to just as a reminder the Jaccard similarity of a b is just",
    "220-240": " our a intersection RB divided by our a Union RB right we just take the intersection of the rating vectors and divide it by the Union it's this is and notices the Jaccard similarity this is nothing but 1 minus the jacquard distance of our a and our P right so",
    "240-260": " when we use in Jaccard similarity notice that a and B have one rating in common Harry Potter one and they've dated over all five movies and therefore the similarity of a and B is 1 over 5 and similarly the similarity of a and C since they related to movies in common",
    "260-280": " it's 2 over 4 notice though that when we compute similarities in this way the Jaccard similarity of a B e is actually less than the Jaccard similarity of a and c this is counter to the intuition that we wanted to capture that a and B are actually more similar than a and C so we'll have the abandon this notion of using jacquard similarities",
    "280-300": " the problem with Jaccard similarity that we like to fix is that it ignores the rating values it just notices that a and B have not watched one movie in common while a and C have watched two movies in common but it doesn't capture the fact that you know it doesn't take into account how they actually like the movies that they are they watched now a",
    "300-320": " way of capturing the rating values and using them to compute similarity is to notice that a and B and a and C are vectors and so we can compute the cosine between the vectors and use cosine distance and this is very this is very similar to the way we used cosine distances in in the case of content-based filtering so so we have",
    "320-340": " let's say we let's say we define the similarity of a B to B just the cosine of the angle between the rating vectors are a and harpy right and let's compute in this case compute the similarity of a and B and notice to compute the cosine similarity we have to insert some value for the unknown ratings and the simplest",
    "340-360": " thing to do is to treat them as zeros right suppose we treat all these unknown values here as zeros and compute the cosine of the angle between a and B and when you do that the cosine turns out to",
    "360-380": " be 0.38 and similarly when you compute the cosine of a and C the angles turns out to be 0.32 and notice in fact that this captures the fact that the similarity of a B is actually greater than the similarity of AC as we actually wanted but actually it's not they match these similarities are actually fairly",
    "380-400": " close to each other it just so happens that the the similarity of a B is actually marginally greater than the similarity of AC which we wanted although the this this still doesn't seem to capture that you know a and B are much much more similar to each other than a and C are the problem we have with cosine",
    "400-420": " similarities is that it treats the missing ratings actually as negative ratings what we've done is that we've used zero to fill in the blanks and in in our rating scale from zero to five zero is the worst possible rating so we've sort of assumed that if a has not rated Harry Potter - he'll give a Harry",
    "420-440": " Potter - a rating of zero which is actually a bad assumption given the fact that they actually like Harry Potter one so this is the problem that we have with cosine dating that we'd like to fix one way of fixing a cosine similarity to accomplish what we want is to use something called the centered cosine and the way we are going to do that is we are going to normalize ratings for a",
    "440-460": " given user by subtracting the row mean or the average rating of the user to illustrate what I mean let's go back to our example here are the ratings for the rating vector for a and notice that the average rating for a is 10 over 3 because there are you know",
    "460-480": " a has rated 3 items and the some of those ratings is 10 so the average rating of a is 10 over 3 and similarly the average rating of B is 14 over 3 and so on what we're going to do is we're going to go through the roof for a user a and we're going to subtract the row mean which is 10 over 3 from each of the",
    "480-500": " ratings except the zero ratings which are going to accept the blank ratings which are going to treat as zeros okay and when I do that here is a modified rating matrix that we end up with notice that a is rating for Harry Potter 1 which was 4 has now become 2 or 3 because we ended up subtracting the row",
    "500-520": " average is just 10 over 3 from each of the ratings and 5 has become 5 over 3 and the rating one for Star Wars has actually become a negative rating negative 7 over 3 right and of course we treat the blank values as zeros notice something interesting here if you sum up",
    "520-540": " the ratings in any row you're going to get 0 what we've actually done is that we've made we've centered the ratings of each user around zero so zero becomes the average rating for every user and positive ratings indicate that the user liked a movie more than average and negative ratings",
    "540-560": " indicate that the user like the movie less than average and the magnitude of the ratings also means you know shows how much he liked or disliked a specific item now once you've done this centering and we we can begin state we can compute cosines using these these centered ratings and when we do that you know the",
    "560-580": " similarity of a and B now becomes the cosine of the centered rating vectors RA and RB and that turns out to be 0.09 and the similarity of a and C using the centered cosine vectors actually turn out turns out a negative 0.5 6 which actually captures the fact that a and C are quite dissimilar users you know the",
    "580-600": " movies that a a like C doesn't like and the movies that a doesn't like C actually likes and notice also there's a big gap now between the similarity of a B and the similarity of a C it shows that a and B are much more alike than a and C are in fact a and C are very unlike each other so the centered cosine",
    "600-620": " captures the the our intuition of similar users much better than the simple cosine and this is because the missing ratings instead of being treated as the negative ratings are actually treated as average ratings and also this turns out to be a nice way to handle",
    "620-640": " tough raters and easy raters because we you know some data you know some people tend to be tough raters and and tend to you know give great movies on a scale really of zero to three while others they tend to be easy raters and tend to be much more liberal with a star star ratings by subtracting out the average rating of each user we've centered users",
    "640-660": " around an average of zero and we sort of normalized to some extent the tough raters and the easy raters and so the the centered centered cosine has these two advantages of of centering around zero and capturing our intuition better now it it turns out there's another name for the centered cosine similarity in",
    "660-680": " the world of statistics it's also known as the Pearson correlation so if you hear somebody talk about the Pearson correlation they just talking about centered cosine similarity great now we've come up with a way of estimating similarity between users once",
    "680-700": " we have this how do we actually make rating predictions for a user so here is a problem let suppose our X is the vector of user X's ratings and what we're going to do is we're going to use the notion of centered cosine similarity to find the set N of users which we'll",
    "700-720": " call the neighborhood and the neighborhood consists of K users who are more similar to X we're going to go through the set of all users compute the similarity between user X and every other user and select the top select K users with the higher similarity values and call that the set n but we also have",
    "720-740": " to be careful since we are trying to estimate the rating of item I by the user X we want to make sure that this set n consists only of users who've actually rated item I right so n is actually therefore the set of users our K users most similar to X who also happen to have rated item hi now once we",
    "740-760": " have this we can make a prediction for user X and item hi the simplest prediction is to just take the average ratings from the neighborhood remember the set n consists of users who also rated item I and are similar to user X so the simplest estimate is to just use",
    "760-780": " the average rating of all the users for item I in the neighborhood and that's and then take that as our estimate of the rating for user X and height fi now the option one is very very simple but it actually ignores the actual",
    "780-800": " similarity values between users now while neighborhood the neighborhood n consists of users who are similar to item I there might be a range of similarity values within the neighborhood it might you know contain users who are very highly similar to the user X and a few users who are not that similar to user X and so what we really like to do",
    "800-820": " - wait the average rating by the similarity values and that gives us option two option two is a weighted average we look at the the neighborhood n and for each user Y in the neighborhood n we wait wise rating for I by the similarity of X and x + y and",
    "820-840": " then we just normalize it by taking the sum of the similarities and that gives us a rating estimate for user X and item ID and you notice here that I just use a shorthand s X Y is there's a similarity of user X and user lie now the technique that we've used so far is called user to",
    "840-860": " user collaborative filtering because given a user we try to find other users that are similar to that user and find use the ratings of those users to predict the ratings of the user that they started out with a dual approach to user to user collaborative filtering is item two item collaborative filtering the basic idea is simple instead of",
    "860-880": " starting out with the user and finding similar user we're going to start out with an item I and find similar items to item I and then we're going to estimate the rating for item I based on the ratings for other similar items and we can use in fact the the same similarity metrics and prediction functions as in",
    "880-900": " the user user model we can use the center cosine distance to find out the center cosine similarity to find a neighborhood of items similar to our item I and then we can use the prediction the average or the weighted average prediction model from from the previous slide to actually make rating",
    "900-920": " predictions for for the item and when we do that here's the here's what the rating function looks like what we're trying to do is that they're going to predict the rating for user X and item I so we're going to start with the item I and we're going to find a neighborhood",
    "920-940": " of items the neighborhood of items and our X is just a set of items that are rated by that are both rated by the you Rex and are similar to the to the item I that we're looking at right so we just take the the item I compute the",
    "940-960": " similarity between item I and every other item in the set that we know about restrict our attention only to the items that have been rated by the user X and then take the top K of those as a neighborhood my X and and once we do that we can just use this same weighted",
    "960-980": " average formula that we had from the previous slide to predict the rating of user X and item I so let's work through an example to illustrate what this means let's say that the neighborhood size that we want to pick the size of n is actually took you're going to be looking at the two nearest neighbors of item I",
    "980-1000": " we're going to using item to item collaborative filtering here is a utility matrix and here the the yellows are the are the known ratings in the utility matrix we have movies on the on the y axis and we have users on the x axis and the the blanks are the unknown",
    "1000-1020": " ratings that we are trying to predict and let us assume that the rating values are between 1 and 5 remember our first step and let's say our goal is to estimate the rating of movie 1 by user 5",
    "1020-1040": " remember the first step is to take the take movie 1 and find other movies that are similar to movie to movie 1 we're going to use Pearson correlation as our similarity which is same as a centered cosine the distance that we computed so we're going to take every other movie",
    "1040-1060": " and they're going to compute its centered cosine distance from movie one and when we do that movie 1 is of course the centered cosine is is 1 point 0 movie 2 is somewhat dissimilar to movie 1 and so on and so this vector here",
    "1060-1080": " illustrates the centered you know list the center cosine distances of of all the different movies with respect to movie one and what we're going to do remember since since our neighborhood sizes - we need to find the two movies with the highest similarity to a two movie one and and that have",
    "1080-1100": " also been rated by by the by user five and those two happen to be movie three with a similarity of 0.41 and movie six with a with a similarity of 0.59 so we're going to pick those two movies as",
    "1100-1120": " a neighborhood for for movie five or sorry or for movie one once we do that if the similarity values are in this case zero point four one and zero point five nine and we're just going to use a weighted average estimate to predict the rating for movie 1 and user five and the",
    "1120-1140": " weighted average is just zero point four one times the rating of movie of movie three and zero point five nine times the rating of movie six which is three and and then divided by the divided by the normalizing value this gives us two",
    "1140-1160": " point six and two point six SR is our predicted rating for user 1 and and movie for for a movie 1 and user 5 you looked at two ways of doing collaborative filtering user to user and item two item now in theory user to user",
    "1160-1180": " an item to item are dual approaches and should have similar kind of performance now in practice though they don't perform similarly at all in fact in practice it's been observed that item to item collaborative filtering hugely outperforms user to user collaborative filtering for most use cases use cases",
    "1180-1200": " for example such as movies or our books and so on items item clearly outperforms user to user now we might wonder why this is the case and the answer turns out to be quite interesting items are inherently simpler than users",
    "1200-1220": " items belong to for example a small set of genres for example you know you can take a piece of music and then classify the classical music or pop or rock and so on whereas users tend to have very very varied tastes so the same user might like for example baroque classical",
    "1220-1240": " music and an acid rock and these two are very different genres but the same user might actually like both these genres but if very rare that an item would actually belong to both these genres of music and therefore it turns out that the notion of item similarity is inherently more meaningful than the",
    "1240-1260": " notion of user similarity and that's why item item collaborative filtering works much better than user user collaborative filtering for most use cases"
}