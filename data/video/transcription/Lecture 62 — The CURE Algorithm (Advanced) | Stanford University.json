{
    "0-20": " welcome back to mining of massive datasets we're going to continue our discussion of clustering by looking at the QR algorithm cure is an acronym that stands for clustering using representatives but before we get to the algorithm itself let's see why we need remember we look at the VFR algorithm or the Bradley fired reiner",
    "20-40": " algorithm in the last lecture for clustering very large data sets that don't fit in memory the bfr algorithm is great because you can scan the data in one pass and obtain clusters the problem though is that the VFR algorithm makes very strong assumptions about the data and about what the clusters look like the first assumption of the VFR",
    "40-60": " algorithm makes is that the clusters are normally distributed in each dimension that in each dimension there is a fixed centroid and a fixed standard deviation that the that each cluster follows along each dimension the second strong assumption that the VFR algorithm makes so the axes are fixed so the clusters then if you follow",
    "60-80": " both these assumptions the the clusters are normally distributed in each dimension and the axes are fixed the clusters that are discovered by the VFR algorithm have these the cigar kind of shape that you see here on the left it could either be a horizontal cigar shape or a vertical cigar shape or a circle",
    "80-100": " which is kind of a limiting case of an ellipse but if your trusters actually are not oriented along the X or the y axis in this case or along the axis in general in the multi dimensional case but are at an angle to the axis as as i show in the in the figure in the second picture here that's that's not okay the",
    "100-120": " B of our algorithm will not find a cluster that looks like a tilted ellipse it can only find clusters that look like either upright or horizontal ellipsis and all of your clusters actually look very very different like the picture on",
    "120-140": " the extreme right here where there are two clusters and the clusters look kind of like crescent moons except in opposite directions those will definitely not be found by the VFR algorithm because they don't look like cigar at all they don't look like ellipses at all in any dimension so that's the kind of cluster that will never be found by the VFR algorithm so the VFR algorithm",
    "140-160": " even though it's super efficient makes these strong assumptions the clusters are going to look like the the pictures on the extreme left and not like the other two and we'd like to avoid this assumption and try to find clusters regardless of what they actually look like because we don't control what the clusters look like in the ignition the data the cure algorithm tries to fix",
    "160-180": " this problem with the with the VFR algorithm the cure algorithm assumes a Euclidean distance I remember it clearly the distance metric means between any two points we can always find a midpoint of two points by taking average of those",
    "180-200": " two points however the QR algorithm unlike the VFR algorithm allows clusters to assume any shape whatsoever there is no restriction on the on the shape of the clusters so in the QR algorithm any of these clusters the the first the second or the third are perfectly fine the QR",
    "200-220": " algorithm works can find clusters of those shapes no difference between the QR algorithm in the VFR algorithm if there is a VFR algorithm we represented each cluster using its centroid whereas in the Q algorithm we are going to use instead of a centroid we're going to represent each cluster by a collection of representative points so instead of",
    "220-240": " representing a cluster by one point we're going to represent it by many points here is an example of a data set where the clusters don't look anything at all like ellipses or cigars so this data on the x-axis we have the age of faculty members at a university like Stanford and on the y axis we have their",
    "240-260": " salaries now these are this is not the actual data but more representation of what the data might look like although it's based on on real-world experience now the this the data points marked by H are salaries of faculty members in the humanities whereas the data points",
    "260-280": " marked with an e are salaries of faculty members in engineering departments and as you can see it's apparent from the from the graph here that in the humanities the the starting salary is somewhat lower than in engineering a humanities faculty member starts at a much lower salary",
    "280-300": " than an engineering faculty member but as overtime as their tenure increases the salary of a humanities faculty member keeps increasing and eventually overtakes the salary of a of an engineering faculty member but the salary of engineering faculty members increases a little bit with their tenure but then kind of flattens out it doesn't",
    "300-320": " increase beyond that and this is just a phenomenon that has been observed in in terms of salaries at most universities and presumably this is because in in the engineering departments the fields keep changing so much that there's a lot of",
    "320-340": " value in bringing in new faculty with with new interests and a new you know new expertise whereas in the humanities I guess you age better as you age so if you sort of look at the look at these two sets of salaries and you try to",
    "340-360": " cluster them what you really want in an ideal world is a stew is two clusters one that you know looks at the engineering salaries and one that looks at the humanities salaries and say and cleanly separate out these two data points into into two separate clusters now when looking at the data I remember",
    "360-380": " you don't know that some of these salaries correspond to engineering faculty members and some to humanities faculty members so so the clustering algorithm doesn't have access to this information but you'd like it to find these find these clusters in the data now it's too much to hope that a",
    "380-400": " clustering algorithm can actually find these exact clusters because these are overlapping clusters and most clustering algorithm cannot find clusters that actually overlap with with each other where a single data point is in two clusters but at the very least we can hope that the clustering algorithm find some approximation to these clusters for example we might want you to discover one",
    "400-420": " cluster there the cluster there and the third cluster is there so that would be a nice nice outcome for from from any clustering algorithm and the queuing algorithm can indeed find clusters of this nature the QR algorithm is a two",
    "420-440": " pass algorithm and let's look at the the first pass of the two pass algorithm in the first pass we sample a random set of points from the data set that's given to us I remember the data set is really large and doesn't fit in memory at all it's sitting on disk somewhere but they",
    "440-460": " go to randomly sample a point from this very large data set and we're only going to sample enough points that that fit in memory people we've covered a techniques for sampling in another lecture so you know how to do this already now once we have a lot of sample points that you know that even Adam Lee sampled we're going to use any main memory",
    "460-480": " clustering algorithm for example you can use a hierarchical clustering algorithm that we covered in a previous lecture and you can cluster the the sample points that are in memory using the hierarchical clustering algorithm to create an initial set of clusters right and because hierarchical clustering is is a complex algorithm it can actually",
    "480-500": " find clusters of the nature you know it doesn't have any kind of restriction on the kind of clusters that it can find so it can find clusters of any shape now once you've actually clustered the sample points and figure out the initial set of clusters for each of those clusters we are going to pick representative points to represent them",
    "500-520": " so what we're going to do is we're going to pick a number K and let's say for and we're going to pick K representative points for each cluster now our goal is to you know take a cluster like this let's say here is a cluster that that we found using hierarchical clustering and you want to find a representative points",
    "520-540": " that are you know as far away from each other to sort of get a good coverage of the cluster as possible for example but what if you find the first represented point there right to find the second there the third there and the fourth there so we have four points that are sort of well distributed in the cluster and if they cover as much",
    "540-560": " of the clusters you know volume as possible and the you know beep sort of in a previous lecture we discussed how to pick points that are as dispersed as possible in a cluster the technique is to for each cluster first pick the first point at random and then you pick the second point to be",
    "560-580": " within the cluster but be as far away from the first point as possible and you pick the third point to be still within the cluster but as far away from points one and two as possible and so on and we've covered this technique in a previous previous lecture so once you pick these representa points you know what you're going to do is that we're",
    "580-600": " going to create the synthetic points and these synthetic points are obtained by moving each representative point a certain distance towards the centroid of the cluster right so we have the cluster we know its centroid and we have these these K representative points now we're going to take each representative point and we are going to create a synthetic",
    "600-620": " point that is obtained by moving the representative point a fixed fraction maybe 20% towards the centroid of the cluster and the 20% here is a is a parameter to the algorithm so let's let's look at an example that should make this clear so here are",
    "620-640": " faculty salary data and let's say when you run a hierarchical clustering on a sample of the data let's say this is in fact a sample of the data and not the actual data when you run a hierarchical clustering on this let's say the the here the first cluster that's found here is the second cluster and here is a third cluster so we end up with these",
    "640-660": " three clusters that are found by the hierarchical clustering algorithm now once we found these clustering algorithms let's take one of these clusters and and let's say we want to find for a remote or representative point for each cluster and let's start with the cluster to the right and",
    "660-680": " there's the first representative point let's say the one that the that we've ended up picking now we want to pick the second reference into a point to be in this cluster but as far away from the first representative point as possible so that's the point we end up picking the third representative point is going to be still in the",
    "680-700": " cluster but as far away from these two points as possible so we end up with that a point there and the fourth represented point similarly ends up being that that point there now once you pick these four remote points for the cluster we are going to move these points towards the centroid of the cluster now beyond we",
    "700-720": " are actually going to affect the data itself but we are going to you know pick synthetic points that are closer to the centroid of the cluster than the remote points that we've actually picked now the centroid of the of the cluster is somewhere in the middle of the cluster there so each of these points is move going to move towards the center of the",
    "720-740": " circle here so when you move the points 20% towards the centroid we end up with these synthetic points and these synthetic points are manufacture points are going to be the representative points for this cluster and we repeat this process for the other clusters as well so for each cluster we end up with",
    "740-760": " K in this case for represent the points representing that cluster so that's the first pass of the cure algorithm in the in the second pass of the of the algorithm we scan the whole data set remember so far we've been working with a sample of the data set that fits in",
    "760-780": " memory now we go back to the whole data set and which is sitting on disk and we Li scan the whole data set and visit each point P and and we're going to take the point P and we're going to place it in the cluster that's closest to it and the definition of closest is very simple we're going to find the point you know you were going to take P and we're going",
    "780-800": " to find that the among the set of representative points of all the clusters we're going to find the represented a point that's closest to P and we're going to assign P to the cluster belonging to that representative point and that's it it's a very very simple algorithm where we just scan the data in",
    "800-820": " one pass and we place each point in its closest cluster which is the cluster with the closest representative point now the cure is a very very simple algorithm as as we saw it just requires a main memory sample and clustered",
    "820-840": " cluster finding some representative points and then one scan of the entire data set to place each data point into you know into the closest cluster but surprisingly cure really works well for a large number of use cases in finding complicated clusters of the of the kind that we saw with the faculty Saturdays",
    "840-860": " and so on our discussion of the cure algorithm wraps up our discussion of clustering which we've covered over the past several lectures remember that the clustering pointer problem is one way we're given a large set of points with a notion of distance between the point usually in a very high dimensional data space and we and a goal is to group these points into some number of",
    "860-880": " clusters with the points that are closer together being in the same cluster we look at the variety of algorithms to do clustering we started with agglomerated hierarchical clustering where we look at the notion of centroid and cluster ID we realize that hierarchical clustering while it's extremely flexible and can",
    "880-900": " produce clusters of any shape is really really complex and it doesn't really work well or scale well to large data sets that don't fit in memory thus we looked at other algorithms that scale well to large data set we started with the k-means algorithm then we looked at the BF r or the bradley fireline algorithm which is an",
    "900-920": " implementation of k-means for really large datasets but we found out that BF r has certain limitations so we looked at another algorithm called cure or clustering using representatives that overcomes these limitations"
}