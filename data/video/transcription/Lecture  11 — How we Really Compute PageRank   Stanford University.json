{
    "0-20": " so far we have been exploring page Rank and we have formulated it um as this IG value problem and we added the random walks um and teleportations to it so now the what we will look next is actually how do we really compute pag rank for the web scale graphs basically how do we comput it for graphs that don't even fit into the main memory of a machine so",
    "20-40": " let's look at what we know so far right what we know so far is that the key step in um Computing page rank is this Vector matrix multiplication where we are Computing the new vector R by taking the The Matrix M and multiplying it with the old Vector R um right this is very easy to do if we have enough main memory in the machine to to store both a the old",
    "40-60": " version of R and the new version of R now let's look at what is the structure of matrix a and we will notice one thing that that is kind of striking right so here I have an example from our previous previous slide with the the three three note graph where our Matrix a equals the",
    "60-80": " The Matrix M * beta plus 1 - beta * this Matrix of 1 / n where n is the the size of the size of the graph and the size of this Matrix is M byn right so notice down here what happens right we take our Matrix M multiply it with 08 we take this Matrix of n byn where every entry",
    "80-100": " is 1 / n multiply that with point2 and sum these two matrices together so what happens now is that our Matrix a is this big big Matrix with non- negative entries and what we notice is that every entry in this Matrix is non zero so initially our Matrix M had a number of zeros but now our Matrix a has no",
    "100-120": " nonzeros so what is what is important now is that um the amount of memory we will need to store a will be actually huge right a is now what is called a dense Matrix let me just give you give you an example so imagine we have a graph on 1 billion nodes so basically we",
    "120-140": " have 1 billion web pages and 1 billion web pages is a very tiny fraction of the web graph so this is a small graph from the web size point of view and let's assume that we need let's say four four bytes to save to save each entry of this graph right so to save a node ID so this means that for example to store 1 billion pages and um and edges this",
    "140-160": " means that we will need around 2 billion um two billion entries and this this means that we will need need around 8 gabt of memory uh to store Matrix M for example but for example if you would have to want to store all the entries of Matrix a now the size of the Matrix a is",
    "160-180": " n by n which is n squ which means that we would have to store n squ entries and the amount of memory we would need would be like 10 to 18 which is a huge number and we would never be able to store this in memory I I think there is not even if we take all the computers in the world there is not they don't have enough memory to store uh 10 to the uh 18 um bytes or integers in memory so the",
    "180-200": " question is what what can we do is there a way to get around this and kind of preserve the the sparsity of the Matrix M right basically the idea is that in real Matrix M will be extremely sparse meaning that an an average page only points to 10 other pages in the graph right so this means that rather than",
    "200-220": " storing the the whole the whole the whole Vector of zeros and then only a few non-zero elements we kind of only want to store non-zero elements so here is here is basically the idea and how we how we now can solve the the page rank problem so just to remind you we right we have the capital N number of",
    "220-240": " pages or number of nodes in the graph we are now defi we defined Matrix M to that's in such a way that the entry Mi J um is one over the degree of J if node J points node I and otherwise that entry zero and now let's think about how do random teleports um play with this uh in to this Matrix M right so basically the",
    "240-260": " way we can think of teleports is basically we take the our initial graph and add all this additional transition edges right so we can think that random teleports basically take our uh graph and transform it into a completely connected graph where we have two types of edges we have the edges that are that corresponds to the hyperlinks and then",
    "260-280": " we have the edges that correspond to random teleportations right whenever we can teleport from one node to another we add an edge and we say that kind of a random Surfer traverses this edge with a very small probability and this is exactly what we will do now right so basically adding a Teleport link from node J to every other um page in the",
    "280-300": " graph and we want to set this teleportation probability of such teleport link to be one minus beta right that's the probability of a Teleport um times um 1 / n right this is the number of the number of web pages so that's the that's the probability of traversing one individual teleportation link right this also means that what we need to do is",
    "300-320": " now that on the all the other Links of the web graph we have to reduce their transition probabilities right so the transition probabilities go from one over the out degree of node J to Beta times one over the out degree of node J right so now basically what what what we did is we we took our graph and we um",
    "320-340": " and we in some sense transformed it into this fully connected graph with different transition probabilities over the edges that corres to the hyperlinks and edges that correspond to teleports however what we notice is that this transformation is actually equivalent to saying that we will Tex the page rank of",
    "340-360": " every page by a fraction of uh one minus beta and then we will take this one minus beta fraction of the page rank scores and we will distribute at the imly right meaning that this random teleportation part of the page rank score gets evenly distributed among the web pages so this is the this is the intuition so let me now show you how the",
    "360-380": " mathematics works out we will start again by looking at our initial page rank equation R equals a * R where the way we Define m just to remind you um is to take every entry IJ of Matrix I is simply beta times the the corresponding entry of Matrix M plus 1 minus beta Over N where n is the size of the graph right",
    "380-400": " the way we think about this is basically to say the transition of a random Walker from from node J to node I is simply the beta time a transition due to the random walk plus um a small constant constant factor that happens due to a random jump probability transition right so now that",
    "400-420": " we have this let's unpack our page rank equation if we unpack this P page rank equation basically what we are saying is that entry ey of the rank V Vector R is simply a summation over the over the over the JS where J range from 1 to n AI J time RJ right what we will do now is we will take this",
    "420-440": " um a and expand it into the into the equation we have Above So if we do this what what we notice is the following right this is still simply a summation here is here I'm expanding the definition of a now into how we computed Matrix a and I have my entry uh from the rank Vector R so now I can distribute",
    "440-460": " the summation and here is what we what what happens right so we have now two summations um both both over the over the same range and what is in what we observe now is one thing that we know is that Vector R is is a probability distribution which means that the entries of vector are sum to one which",
    "460-480": " means that this second summation here the entries um the sum over J of r i j sums to one so the second summation basically simplifies to just this uh constant of 1 minus beta Over N so notice what what we did right now what we did is basically we expressed our Ral",
    "480-500": " a * R into a different into a different expression where we say that r equal beta * m * R plus some um some some constant right what this means is now basically that we never really need to explicitly Express Matrix a right we",
    "500-520": " never really need to materialize this big Den Matrix where every entry is non zero we can only work with Matrix M because Matrix m is full of zero Elements which we don't need to explicitly store so we can actually work with much much smaller Matrix so this is basically the the the good thing that",
    "520-540": " happened here so rather than working with a big N squared size size Matrix we are now working on a with a very small Matrix and let me just uh demonstrate how how much uh difference uh this makes in practice right so what we did um in the in the few last slides is basically we rearranged our page rank equation",
    "540-560": " into a into a different equation that doesn't depend on a but only depends on our Matrix M right so now what is important is that Matrix m is sparse right what do I mean by that is for example in The Matrix M if you think of it as a as a matrix in every in every",
    "560-580": " row even though this Matrix is uh let's say a billion times a billion like 10 to the 9 * 10 the 9 UM the individual the number of non-zero entries in every row on average will be around 10 what this means is that an average page has around let's say 10 outlinks or even 100 outlinks but the point being is that out",
    "580-600": " of 1 billion entries in every in every row of this Matrix there will be only um 100 nonzeros so this means that rather than storing the full N squared number of cells in this Matrix we will only store the non the cells that actually have the non-zero value so what what this means is that in in every iteration",
    "600-620": " of our page equation basically all we need to compute in some sense is that is the product of our matri Matrix M um with the the with our old rank vector and then add a constant value um to every rank score just because of the random um of the random jumps of course",
    "620-640": " here I was assuming that our graph M has no dead ends if our graph will have no will have de ends this means the vector R won't sum to one so after everything is done we will have to kind of expand it back to normalize to one I'll talk about this um a bit more in the next few",
    "640-660": " slides so here is now the complete algorithm for pag rank how one would go and actually implement it in practice so the way this works is on our input we are given a directed graph G and we are given a parameter beta this is the teleportation parameter beta right um and what we are assuming in this",
    "660-680": " algorithm is that our graph has both spider traps and dead ends kind of nothing will break the algorithm thrust regardless of whether these dead ends and spider traps actually happen so the output of our algorithm is now a page rank Vector R and the way the algorithm operates is the following at the beginning we set all the all the page",
    "680-700": " rank scores to be 1 / n to be equal and we set time equals 1 and then we have this Loop where we iterate until our page rank vectors will converge which means that the page rank Vector between time time step T minus one and time step Ste the the the individual entries do not do not change much but do not do not",
    "700-720": " change more than some some small value Epsilon which is and this Epsilon is again a user set parameter something small but um and depending on how small this value is this is the number of iterations the algorithm will need to converge okay so now we explain the outer Loop so now now let's look what",
    "720-740": " happens in the Inner Loop so in the Inner Loop first we have we have another a for Loop where basically we go and update um the paging score of every node by simply taking the paging scores of every node I that points to it and then divide that by our degree of I um this is the first part of course here we have",
    "740-760": " to be careful if node J has um um gets the rank zero if the ingree of it is zero right so if a node has no ingree then we set it page rank score to zero and then of course what happens now is that if we have dead ends the the page rank will leak out so now we need to figure out how much of the page rank has",
    "760-780": " leaked out and then reinsert the page rank in the missing page rank scores into our rank Vector so all we what we do here is the following we compute what is the sum of the components of our pag rank Vector R so far we call this s we want um this s will be less than one so what means is that 1 minus s amount of",
    "780-800": " page rank has kind of leaked out so we want to now take this and and evenly inserted to every entry of R so this is exactly what we what we do we say now we go over Vector R again we say the the true value of the page rank score is whatever we had before plus this missing part of the page rank score right 1",
    "800-820": " minus s Over N which means that um every node n gets one minus s uh times 1 / n fraction of the of the leaked out score so that now again our Vector R will sum to one so with this basically we obtained the new new",
    "820-840": " version of R we check for convergence and we keep uh we keep repeating until um until we converged"
}