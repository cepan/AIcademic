{
    "0-20": " we are now going to take up a class of problems where we are given a large collection of sets millions or billions perhaps and we are asked to find those sets that are similar the notion of similarity is quite specific and it's called jard similarity we'll learn this concept soon but the idea is roughly that the larger the fraction of elements that the two",
    "20-40": " sets have in common the more similar they are there is a fundamental problem of scale if we have even a million sets not a large number compared with the number of web pages or Amazon users the number of pairs of sets is half a trillion we don't have the resources to",
    "40-60": " compare them all so we need some magic to for Focus us on the pairs that are likely to be highly similar never looking at the vast majority of pairs when you learned about hashing you it probably seemed like a bit of magic you have a large set of keys and when you want to find some key k you go right",
    "60-80": " to it without having to look very far at all the technique we're going to learn locality sensitive hashing is another bit of magic uh here we are pointed right at the similar pairs without having to Wade through the morass of all pairs we'll Begin by looking at some applications where finding similar sets is very useful we then are going to",
    "80-100": " focus initially on finding similar documents meaning that they have a substantial amount of text in common for this problem we first study shingling which is a way to convert the informal notion of similar documents into a formal test for similarity of sets then we learned the remarkable technique",
    "100-120": " called minhing which allows us to replace a large set by a much smaller list of values the magic of minhing is that the similarity of the small lists called signatures predicts the similarity of the whole sets finally we take up the locality sensitive hashing technique itself and see how to find similar sets or similar",
    "120-140": " documents without doing anything that involves searching all pairs to begin let's look at some of the interesting data mining problems that fit the pattern of mining for similar sets for example we can view web pages as the set of words they contain if two pages have similar sets of words we might expect them to be about the same",
    "140-160": " topic for another example imagine a matrix of Netflix users where the rows correspond to the users and the columns to the movies the entry for a given user and movie is the rating that the user has given the movie blank if no rating uh we might see a user as the set of",
    "160-180": " movies they have rated four or five that is movies they like two users who have similar sets of liked movies probably have the same tastes and Netflix can use the movies one user said they liked to recommend movies to the other we can use the same idea backwards where we think of a movie as the set of users who like",
    "180-200": " that movie movies with similar sets of users can be expected to belong to the same genre of movie people create records of data about themselves at many different sites Google Amazon Facebook and so on we may want to figure out when two records refer to the same individual and",
    "200-220": " this need gives rise to the problem called entity resolution determining the set of records that refer to the same individual to see the problem many sites will ask for a phone number but you might give your landline at one site your cell phone number at another not give a number at all at a third site and mistype your number at a",
    "220-240": " fourth however we can often Wade through the errors and ambiguities by thinking of a record as a set of attribute value pairs pairs like uh phone is 555 whatever okay records with similar even if not identical sets of attribute value",
    "240-260": " pairs May well represent the same individual and these records can be merged to combine their information we're going to focus on a particular important application finding lexically similar documents in a large collection of docs such as the web note we are not talking about docs",
    "260-280": " on a similar topic we want them to have sequences of characters in common this question has a variety of applications for example the techniques we're going to learn were used to find mirror pages on the web mirror pages are typically almost the same but they will differ for example in the information about the host site for the page and",
    "280-300": " links to the other mirrors search engines use a technique like the one we'll learn so they don't show more than one of a set of mirror sites another application of finding lexically similar documents is the search for plagiarisms for example spammers will take your web page give it a new URL and place ads around around it the plagiarizer may be",
    "300-320": " clever taking only a part of the plagiarize document reordering Pieces perhaps changing a word here and there we still want to be able to find such pairs of documents in a collection as large as the web without having to compare all pairs of documents it can be done in fact it's much easier than it",
    "320-340": " looks and another application concerns sites like Google news that aggregate news stories an article may be written by the Associated Press and distributed to thousands of newspapers and online news sites each will make modifications perhaps truncating the story surrounding it with ads and so on it's important for an aggregator to realize that the two",
    "340-360": " web pages are really telling the same story because they came from the same original even if they have been significantly modified as we suggested in the introduction we're going to learn three important new techniques shingling is how we convert documents is the set so the documents",
    "360-380": " that have a lot of text in common will be converted to sets that are similar in the sense that they have a lot of members in common then we'll learn about Min hashing which is how we convert sets to short signatures the important property is that we can look at the signatures of two sets and tell approximately how similar are the sets that we obtained by",
    "380-400": " the shingling process and last but not least we'll learn the technique called locality sensitive hashing or lsh that lets avoid looking at most of the pairs of signatures that do not represent similar sets here's an outline of how we process documents to find those that are similar",
    "400-420": " without comparing all pairs at the outset I want to emphasize that there can be both false positives and false negatives that is the algorithms we use can sometimes fail to find a pair of documents that we would regard as similar that's a false negative but we can also if we're not not careful to",
    "420-440": " check the details of the documents sometimes have false positives pairs of documents we declare to be similar but they really aren't however by carefully choosing the parameters involved we can make the probability of false positives and negatives be as small as as we like okay so we start by shingling the",
    "440-460": " document okay that is we replace the document by the set of strings of some chosen length K that appear in the document that's how we convert documents to sets we then construct signatures for the sets of s shingles uh using the technique called minhing the result of minhing a set is a short Vector of",
    "460-480": " integers the key property which will prove is that the number of components in which the two of these vectors agree is the expected value of the similarity of the underlying sets incidentally the reason we want to replace sets by their signatures is that the signatures take up much less space if we're dealing with a large set of documents would like to",
    "480-500": " be able to work in main memory rather than with disk for efficiency and reducing the space of the representations makes it more likely that we can work in main memory but it seems we still need to compare all pairs of signatures and that takes time that is quadratic in the number of documents as we mentioned even",
    "500-520": " a million documents leads to half a trillion pairs of signatures to compare and that is too much so that's where locality sensitive hashing comes in we do some magic which we'll explain soon that allows us to look at a small subset of the possible Pairs and test only those pairs with similarity by doing so we get almost all the pairs that are",
    "520-540": " truly similar and the total time spent is much less than quadratic so let's begin the story by defining exactly what shingles are for any integer K A K shingle or sometimes called a KR is a sequence of K consecutive characters in The do document the blanks that separate the",
    "540-560": " words of the document are normally considered characters if the document involves tags such as an HTML document then the tags may also be considered characters or they could be ignored so here's an example of a little document consisting of the five characters a b c a b we'll use k equals",
    "560-580": " 2 for this little example although in practice you want to use a k that is large enough that most sequences of K characters do not appear in the document a K in the range 5 to 10 is you generally used uh but the two shingles for our little",
    "580-600": " document are AB it's that then BC then CA and then AB again since we're constructing a set we include the repeated T shingle AB only",
    "600-620": " once and thus our document ABC a b is represented by the set of shingles AB BC and CA we need to assure ourselves that",
    "620-640": " replacing a document by its shingles still lets us detect pairs of documents that are intuitively similar in fact similarity of single sets captures many of the kinds of document changes that we would regard as keeping the documents similar for example if we're using K",
    "640-660": " shingles and we change one word only the K shingles to the left and right of the word as well as shingles within the word can be affected and we can reorder entire paragraphs without affecting any shingles except the shingles that cross the boundaries between the paragraph We moved and the paragraphs just before and after in both the new and old",
    "660-680": " positions for example suppose we use k equal three and we correctly change the which in this sentence to that the only shingles that can be affected are the ones that begin at most two characters before which and end at most two characters after which okay these are G",
    "680-700": " blank W blank wh and and so on up to H blank c a total of seven shingles these are replaced by six different",
    "700-720": " shingles uh G blank t t is a blank t h and so on up to t blank c uh however all shingles other than these remain the same in the two sentences because documents tend to",
    "720-740": " consist mostly of the 26 letters and we want to make sure that Mo shingles do not appear in a document we are often forced to use a large value of K like k equals 10 but the number of different strings of length 10 that will actually appear in any document is much smaller than 256 to the 10th power or even 26 to",
    "740-760": " the 10th power thus it is common to compress shingles to save space while still preserving the property that most shingles do not appear in a given document for example we can hash strings of length 10 to 32 bits or four bytes thus saving",
    "760-780": " 60% of the space that are needed to show to store the the shingle sets the result of hashing shingles is often called a token thus we can construct for a document the set of its tokens we construct a shingle set and then hash each shingle to get a token since docu doents are much shorter than 2 to the",
    "780-800": " 302 power bytes we still can be sure that a document is only a small fraction of the possible tokens in its sets there's a small chance of a collision where two shingles hashed to the same token but that could make two documents appear to have shingles in",
    "800-820": " common when in fact they have different shingles but such an An Occurrence will be quite rare in what follows will continue to refer to shingle sets when these sets might consist of tokens rather than the raw shingles"
}