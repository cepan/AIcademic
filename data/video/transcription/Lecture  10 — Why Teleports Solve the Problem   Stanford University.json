{
    "0-20": " so now the question is why why why do why do teleports solve all our problems right so before here is the the equation we had from before and um in order to understand why the teleport solve our problems we have to go back to the theory of Marco chains that I just alluded to um um in the previous lecture",
    "20-40": " so the way we Define a Marco chain is the following so Marco chain is this abstract mathematical object that has the following ingredient or parts to it so first we think that we have a set of States X okay then we have a transition Matrix P where P J simply measures what",
    "40-60": " is the probability that if if we were at State I How likely are we to transition to State J um in a given time step right so P J means given that I was at J in previous step How likely am I transition to no to to state I okay and then pi is",
    "60-80": " a is a stationary probability distribution of being um at any of the states X and our goal right is to compute the value of this equation that pi equals P * pi right so this would be a pro stationary probability distribution of this Marco chain that is defined over a set of states and with",
    "80-100": " the transition Matrix uh p and again you iMed immediately see the the the correspondence to our initial page rank equation here we have pial P * pi and we had rals M * R so theory of Marco chains says the following it says that for any",
    "100-120": " start ve Vector the power iteration applied to this Marco transition Matrix P will converge to to a unique positive stationary Vector as long as this Matrix P has three properties it has to be stochastic it has to be irreducible and AP periodic so now what I will show is",
    "120-140": " that for each of these three conditions stochastic irreducible and AP periodic actually adding random teleports gives us a gives us a in some sense stochastic transition Matrix um that has these properties so what we will do now is convince ourselves that our Matrix Sam together with this random teleports",
    "140-160": " has all the three properties that we need for uh the for the pag rank Vector are to exist so the first question is why do teleports make M stochastic so a matrix is stochastic if its columns sum to one so in our case in case of dead ends when we have this no D",
    "160-180": " that has no outlinks the column for no D did not sum to one It summed to zero and the stochasticity condition for the Matrix was violated so now if we add this random teleportation we can that occurs with probability one we can basically think of this as adding the green edges",
    "180-200": " from node M to any other node in the network including the self Edge right this means that our Matrix M now got transformed and our column for M for node M now has this values of 1 over three in it so the column sums to one and we get the stochastic stochasticity property of",
    "200-220": " Matrix M the way we can think about this in terms of equations is that basically we say our we Define a new Matrix a where we take our previous Matrix M and now we we I introduced two pieces of notation here first I have this Vector a where the I component of vector a equals 1 if node I has out degree zero",
    "220-240": " if node I is a dead end and otherwise it has value zero and then this Vector e is just Vector of all ones so it's a vector where every component has a value of one so what we basic what this basically means is it we take Matrix M and wherever there is a column with in The Matrix Sam that has all zeros we replace",
    "240-260": " that with one over the out degree of of that given note exactly what we what we did in the case of M so this is what we did now is basically random random teleportations what they do they take our Matrix s that cannot be stochastic if the graph has dead ends and transform",
    "260-280": " that into a new Matrix a that is now stochastic by by taking the teleportation with probability one out of the nodes with zero out degree so that's the first property the second property is that M has to be a periodic so we say that a chain is uh periodic if there exists some value K such that the",
    "280-300": " interval between two visits to some State um s is always a multiple of K so for example if we were to have a graph on three nodes with with a directed cycle as I have it here then for example this is a this would this would be a a periodic periodic chain because the the",
    "300-320": " random walk here is the terministic um and every every two steps we return back to the to the same note so by adding teleports what this basically means is that um that at any time we will be able to jump out of of this kind of infinite infinite Loop and we can even think that",
    "320-340": " what we have is we have these self Loops so that the the random Walker can can get can spend some time at a given node and this way the periodicity is broken um and this is how basically random teleports sh the periodicity problem now the last property we need to talk about is",
    "340-360": " irreducibility and we say that um m is irreducible when from from any state there is a nonzero probability of going um to any other state in the in the network this means that basically we can never get stuck in a given state so the way for example we would make our given",
    "360-380": " uh graph here irreducible is to add all these other other possible links which basically means we would add um random jumps so this would mean that the that um there is a non-zero probability of going from any state to any other state uh in our graph so putting all this together this is this is um exactly what",
    "380-400": " random jumps do so basically Google's solution to to page Rank and to random Surfer uh interpretation of page rank was to introduce random jumps so the idea is that we stay we want to take Matrix m m make it stochastic a periodic in irreducible all this is achieved by",
    "400-420": " slightly modifying the our random walking process where at each step a random Surfer has two options with probability beta the random Surfer goes and follows a random outlink and with probability one minus beta the random Surfer jumps to some other page at random",
    "420-440": " so now what this basically means is that this now changes our page rank equation so if you think about the page rank equation now it's a bit different so here for example the score of node J can be computed as follows right so basically what this is saying is the following the importance of node J is first the sum of the importances of all the nodes I that point to it where um r",
    "440-460": " subj r sub I is the probability that random Walker is at node I then we divide that by the out degree of i as the probability that the random Walker actually Traverse the link towards J and this only happens with probability beta because the random Walker when they are at node ey has to decide to actually",
    "460-480": " follow a link and this happens with probability beta and then of course How likely is the random Walker to visit node J it either does it by a by by following a link or with probability one minus beta the random Walker decides to jump and if the random Walker decides to jump then it will land at a given note J with probability 1/ n where n is the",
    "480-500": " number of nodes um in the network right so basically we took our initial formulation of page Rank and now we change it a bit where we have the random walk part this is the part where we kind of multiply with beta and we have the random Jump part where we have the one minus beta so now the question is um",
    "500-520": " given this new random walk formulation is is power iteration still going to work right now we have a different more complicated um recursive equation so the question is how how do we compute this and the way we compute this is basically to to run to run our igen Vector finding method our power iteration again the way",
    "520-540": " the way we see that basically we have the same problem as before is to notice the following so we have this what we will call Google Matrix we will call it a and we will Express a as a matrix M plus some uh other matrices so we take our Matrix M and multiply with beta this",
    "540-560": " is the the part that comes due to random jumps and then what we want to do is we have this other part one minus beta that basically this is the probabilities or transitions due to random jumps and um simply the expression we get here is that this is 1 minus beta 1 / n where n is the number of nodes in the graph times the outer product of this Vector",
    "560-580": " of all of all ones called e okay so what this means is that even with this random jumps the page rank solution can be expressed exactly as we had it before that R equals a now this is the Google Matrix not the Matrix M anymore time times R of course one question that we need to answer is what is a good value",
    "580-600": " of beta right how often should the random Walker jump for example if beta would be zero then what would that mean is that random Walker jumps all the time so all the nodes in the network have um exactly the same Pro the same page rank score because the random Walker is not really walking over the graph it's just random jumping all the time if we set u",
    "600-620": " beta to be equal to one then basically there is no random jumps and this means that our Matrix a wouldn't be um stochastic anymore and and so on and we would have no random jumps and Page rank wouldn't wouldn't really work so what turns out is that the good value for beta is to set beta between 08 and 0.9",
    "620-640": " and usually people set beta to be 085 which basically means that for every five steps uh you do a random job so a random Walker in some sense in on the average would do five steps and a jump another five steps and a jump and so on so that's basically the uh the idea so let's now see how this page rank",
    "640-660": " formulation would work in in real world so imagine we have our old graph as we had it before three nodes uh and in this case node m is a is a spider trap right there is this self flop what I also have on this graph is I have these green edges and these green edges you can think of them edges that are there due to due to",
    "660-680": " random jumps so we have our Matrix M as we had it before with Matrix M everything is fine it's still it's still stochastic the only problem is that no D is a is a dead end oh sorry is a spider trap and um now what I also did in this graph is I labeled every every edge with",
    "680-700": " the with its transition probability so the way the way we do now is we take this Matrix M we take this other Matrix of 1/3 and multiply with 1 minus beta so in this case we are assuming beta is8 and this gives us the M The Matrix a and now if we do the M multiply our r with",
    "700-720": " Matrix a this is how the using power iteration these are the different versions of vector r as we keep um multiplying and at the end the pagent scores we would converge to are given here so basically the score for node y would be 7 over 33 for node a would be 5",
    "720-740": " over 33 and for node M would be 20 21 over 33 so what do we see we see that m is still the most important node in the graph because of this uh spider trap but we see that nodes Y and a have also nonzero score and actually node a is more important than node y so it seems",
    "740-760": " everything works and everything is fine"
}