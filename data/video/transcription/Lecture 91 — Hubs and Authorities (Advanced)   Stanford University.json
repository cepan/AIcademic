{
    "0-20": " So, we are continuing our investigation of link analysis algorithms, and in particular, now we will look at the hubs and authorities algorithm or the hits algorithm. Here the idea will be that every page now has two scores, not just one as in case of pagerank, but two, and one type of the score we will call a hub score, and the other one we'll call the authority score.",
    "20-40": " So let's look how this works. So the name hits or hubs and authorities comes from the hypertext-induced topic selection is the acronym, and basically the idea here is that we want to measure the importance of pages or documents in a similar way than what we did with pagerank. And this method was proposed around the same time as pagerank, this was 1999, 1998.",
    "40-60": " The goal or the motivation here is, for example, to say let's try to find a set of good newspaper webpages. And our idea is that we don't just want to find good newspapers, but in some sense we want to find experts, people who link in a coordinated way to a set of good newspapers.",
    "60-80": " So the idea, similar to pagerank, is that we will count links as votes, and here we will use a very similar formulation where we will say that page is more important if it has more links.",
    "80-100": " So the idea is the following. We will have for every page its hub and its authority score, and each page will basically have a quality of it as an expert, we will call this its hub score, and it will have also its quality as a content provider, so we will call this the authority score.",
    "100-120": " And the idea will be that the hub score is simply the sum of all the votes of the authorities that are pointed to, and the quality of the authority, so the authority score, will be the sum of the experts' votes that the page is receiving.",
    "120-140": " So the idea is that we will then apply the principle of repeated improvement to compute the steady state scores. So let me give you an example. So the idea is basically that we can think of the web as this bar-pie type graph where we have a set of hub scores here and a set of authorities.",
    "140-160": " Authorities are the web pages that provide good content, and hub scores or hub pages, we can think of those as web pages that have useful links to the good authorities. So the way we will think about this is that generally pages on the web fall into two classes. There is first a class of authorities where these are the pages that contain useful information",
    "160-180": " or useful content. So in our case of newspapers, this would be newspaper web pages. If we would want to identify good course web pages, this could be course home pages, things like that. And then we also have a class of hub pages, which are basically pages that link to good",
    "180-200": " authorities. So we can think of this as lists of good things on the web, right? So we could think of a good hub to say, here are my favorite newspapers, and that page would link to them. Or if we want to think of a good hub for the course domain, we could think of a course bulletin as a good hub web page for the courses.",
    "200-220": " Or if we were interested in car manufacturers, then a list of US car manufacturers would be a good hub that would then point to individual car manufacturer home pages. So the way we can think about this in terms of the graph structure is that we have the authorities, which are then pointed by good hubs. So hubs are, in some sense, indices that point to good content.",
    "220-240": " So let me give you a small example of how hub and authority computation is performed. So as with page rank, we will think of links as votes. But now, every web page has two scores. It has an authority score and a hub score. And the authority score is simply the sum of the hub score of everyone that points to",
    "240-260": " it. And then the quality of a hub is the sum of the qualities of authorities that the hub points to. So let's think now, just for a moment, of the web graph as being this kind of bipartite graph where good hubs point to good authorities. And let's think about our newspaper example.",
    "260-280": " So we have our set of newspapers here on the right. And we have a set of authorities here on the left. And our first goal is to compute a set of authority scores. So what this means is that every newspaper page or every page here in this graph will go and collect its in-votes.",
    "280-300": " So for example, the San Jose Mercury News gets two votes, Wall Street Journal gets two votes because it receives two in-links, New York Times gets four, and so on and so forth. So now, what we did is, given the hub scores on the left, and we assumed everyone has the same score, has the score of one, we collected the authority scores on the right.",
    "300-320": " What we do now is actually go and propagate the authority scores back to the hubs. So now, what we do is that every hub goes and sums up the authority scores of everyone they point to. So for example, the first node up here has the score of 8 because 2, 2 plus 2 plus 4",
    "320-340": " equals 8. And the second node has a hub score of 11 because, again, the authority scores of everyone they point to sum to 11. So now that we have the new hub scores here on the left, we can go back and update the authority scores on the right. So the idea now is that, for example, San Jose Mercury News has the authority score",
    "340-360": " of 19 because 8 plus 11 equals 19. So now you see how basically we are pushing hub and authority scores back and forth in this graph. Of course, what we see here now is that this hub and authority scores can go arbitrarily large.",
    "360-380": " So we will want to keep them normalized. But this is basically the idea, is that we compute the authorities based on the hub scores. And then we update the hub scores based on the new authority scores. And we keep doing this iteratively. So what we did right now is that we have this mutually recursive definition where basically good hub links to many good authorities.",
    "380-400": " And a good authority is linked from by many good hubs. And now that we are having this intuition, we model this, as I mentioned, using two scores. Every node in a graph will have a hub score, will have an authority score. We can denote these scores using a vector, h for hub scores and vector a for the vector",
    "400-420": " of authority scores, the same way as we did it with PageRank. So now, how does the hub and authority algorithm work? It operates in stages or in iterations, the same way as we had it with power iteration.",
    "420-440": " The idea is that we will initialize our authority vector and the hub vector to be of unit length, so to have Euclidean length 1. So this means that we set every coordinate or every entry of it to be 1 over the square root of n, where n is the size of our graph.",
    "440-460": " And then we have the iteration, which we keep doing until our hubs and authority vector converges, where the notion of convergence is that the individual authority scores don't change too much between different iterations. And similarly, the individual hub scores don't change too much between iterations. And what we are doing is very simple.",
    "460-480": " We are updating the authority scores by summing the hub scores of the nodes that they point to it. And we also update the hub scores by looking at all the authorities this hub node points to and summing their authority scores. And then, of course, because these things can get arbitrarily large, we make sure and",
    "480-500": " normalize the authority scores and normalize the hub scores so that their sum of squares equals 1. So here we are assuming that these vectors are unit length, that they have Euclidean length of 1, which means the squares of their coordinates have to sum to 1. And we keep repeating this in a similar way than what we did with the power iteration.",
    "500-520": " What is interesting is that I can now show you how to express this in the matrix formulation. And what we will see is that hub and authority scores, again, correspond to the eigenvector of a given matrix. So in the case of PageRank, the PageRank scores correspond to the leading eigenvector of the matrix A. And in this case, the hub and authority scores will also correspond to the leading",
    "520-540": " eigenvectors, but to a different matrix. So let's now figure out what is that matrix. So the way HITS works is the following. As I mentioned before, we have the vector A, which is a vector of authority scores. We have the vector H, which is a vector of hub scores.",
    "540-560": " And then we have the adjacency matrix A. We will now use this to just denote the adjacency matrix, where Aij equals 1 if node i points to j, and 0 otherwise. And then the first question is, how can you write the hub update equation?",
    "560-580": " The hub update equation just says that hub score of node i is simply a summation of the authority scores of everyone that node i points to. So if we rewrite this in terms of the summation with our matrix A, we can write this as a summation over j, Aij times ij, which in turn can be rewritten simply as H, our vector",
    "580-600": " H, equals adjacency matrix A times our vector of authority scores A. And analogously, we can now take the update equation for the authority score and the same way rewrite it in a sense that authority score A equals A transpose times H.",
    "600-620": " So having expressed the hubs and authorities equation in terms of the vector matrix product, we can now also express the whole hubs and authorities algorithm in terms of the vector notation. So the idea is very simple. We have still these vectors A and H. We initialize them to have values 1 over square root of",
    "620-640": " n, where n is the size of the graph. And then the hubs and authorities algorithm, what happens? It simply repeats the for loop until convergence, where at the first step, we are updating the hub scores, where we are taking the adjacency matrix and multiplying it with our authority",
    "640-660": " vector A. This gives us the new hub scores. And then we take the hub scores, and we want to update the authority scores. So what we do is we take the adjacency matrix A transposed, multiply it with the hub scores H to obtain the new version of A. And then we keep repeating these two steps until we converge.",
    "660-680": " And what I mean by convergence is the individual coordinates of vector A or individual coordinates of vector H do not change too much. So now, given that we have rewritten the algorithm in this way, let's look at what is the updated equation for A.",
    "680-700": " So the updated equation for A works the following. So little vector A equals the transpose of the adjacency matrix capital A times the adjacency matrix A itself times the authority vector A. Why is this the case? This is the case because the capital A times vector A is simply the new version of our",
    "700-720": " vector H. That's the first update equation. And now, what we do is we replace the second H here with the updated version of it. So this means that we are basically arriving to the recursive definition for authority",
    "720-740": " score A. What this means is that authority score A equals adjacency matrix transpose times the adjacency matrix times the authority vector A. And then in similar way, hub scores can be computed in terms of hub score equals the adjacency matrix times the transpose of the adjacency matrix times the hub score vector itself.",
    "740-760": " So what does this mean? Basically, we are arriving to the same type of equation that we had with PageRank, where we had R equals M times R. Here we are seeing that H equals A times A transpose times H. So this means that H and A are both eigenvectors of two different matrices.",
    "760-780": " One is adjacency matrix transpose times the adjacency matrix. And the second one is the adjacency matrix times the adjacency matrix transpose. So what this means is that our hub vector H is a principal eigenvector of the matrix",
    "780-800": " A times A transpose. And our authority vector A is a principal eigenvector of the matrix A transpose times A. So again, basically what happened, similar to PageRank, is that this hub and authority scores correspond to the principal eigenvector of our transformed adjacency matrix.",
    "800-820": " And the way we transform the adjacency matrix here is that for the authority score, we multiply it with itself. So we multiply A times A transpose. And for the authority scores, we multiply A transpose times A. Just to give you an example how hub and authority scores would work, I have an example of my",
    "820-840": " three-node web graph, Y, A, and M. We have a version of the adjacency matrix and a version of adjacency matrix transpose. If we were to run our iteration and keep updating the hub scores H here and authority scores A down here, then after a while, we would converge to two different vectors.",
    "840-860": " For example, where we would see that, in our case, Yahoo is a very good hub. But for example, we see that Yahoo and Microsoft are really good authorities. So this is just a different way, a different intuition, and a different way to compute",
    "860-880": " consequences of nodes in a graph, where now every node has this idea of a hub score and the idea of the authority score. What is interesting, however, is that PageRank and HITS are two solutions to the same problem. They both try to answer the question of, what is the value of an in-link coming from node",
    "880-900": " U to node V? In PageRank model, we value the value of a link to be dependent on the number of links that point to it, while in the HITS model, it depends on the value of other links that are pointed out of a given node. And what is interesting, for example, is that in terms of web search industry, PageRank",
    "900-920": " got widely adopted and is being used in ranking search results even today, while HITS method by itself hasn't really achieved wide industrial adoption."
}