{
    "0-20": " to see a problem that comes up when you're not careful about communication cost I want to talk about a real project in Stanford's data mining project course the data being used was anonymized information about a million patients over 20 years the medications they were given and their medical conditions the records were a pre processed to keep",
    "20-40": " information about each drug in a separate record that record contains the relevant medical history of each patient who took the drugs the average drugs record was about a megabyte long so it doesn't look like a really big database a few gigabytes of data so what could go wrong well the goal is to find drug",
    "40-60": " interactions so for example you could calculate the fraction of people people taking a drug a who subsequently got a heart attack and the same for drug B then look at the intersection of the two sets of patients and see if they were significantly more likely to get a heart attack so we need to look at each pair of drugs and do a statistical analysis of their two records there are 4.5",
    "60-80": " million pairs of drugs in this study so that's a lot of computation but as we shall see if we're not careful the communication cost is so high that it makes it impossible to do the study using MapReduce okay here's what in a sense is the obvious way to do the comparisons using",
    "80-100": " MapReduce will have a key for each pair of drugs I and J you can think of the set as a sorted list that's the same thing really and the value will be the megabyte long record for one of these two drugs so what does the MapReduce say given drug I on its record the mapper",
    "100-120": " needs to generate 2999 key value pairs one for each other drug J the key value pair for J has the key that is the set containing I and J this and the value is the record for I which will represent",
    "120-140": " that weight notice that the mapper doesn't know the record for J it only sees eye and its record and what does a reducer get say the reducer is the one for key IJ this reducer is going to get to key value pairs one from the mapper for I and the other from the mapper for J",
    "140-160": " formally the input to the reducer is the key I J and the list of the two values associated with this key the records I and J well look at a baby example to see the idea in this example there are three drugs rather than three thousand there",
    "160-180": " will be three mappers one for each of the three drugs there are also three reducers one for each of the pairs of drugs it is a coincidence that the number of mappers and reducers is the same in the real problem there were three thousand mappers and 4.5 million reducers and for any number of drugs greater than three there will be more reducers than mappers okay so the mapper",
    "180-200": " for drug one produces a key value pair where the key is the set 1 2 and the value is the megabyte or so of data about the patient's taking that drug the mapper for drug 1 also produces another key value pair the value is the same the record for drug 1 but the key is",
    "200-220": " different here it is the set 1 3 in general the mapper for drug 1 will produce a much larger number of key value pairs the number of key value pairs is 1 less than the number of drugs each key value pair has a key that is the set containing 1 and one of the other drug member numbers the value is",
    "220-240": " always the same the record for drug 1 and the other 2 mappers do essentially the same thing but with their drug number and with the record for that drug once these six key value pairs are generated by the mappers they are sent to the reducer for their key and there",
    "240-260": " they go and once they arrive at the reducer they are combined into a key in its list of value for example the input to the reducer for the set 1/2 has the key 1/2 and the list of values that were",
    "260-280": " associated with that key and the various key value pairs would be that the total computation time is not insignificant there were 4.5 million pairs and each takes some work to process even in main memory suppose a hundred milliseconds of computation was required for each pair",
    "280-300": " that's about 120 hours of work shared among all the cores of all the compute nodes not insisted not in substantial but you can get it done in an hour by using ten sixteen core compute nodes the problem is that there are 3000 drugs and",
    "300-320": " the mapper for each drug created 2999 key value pairs one for each of the other drugs and for each key value pair a megabyte had to be communicated from mappers to reducers you multiply that together and you get nine terabyte or ninety terabits you try to squeeze that",
    "320-340": " through a one Gigabit Ethernet or anything with that speed and it means Oh ninety thousand seconds about 30 hours of network use and that's assuming there are no other jobs competing for the network so let's see how we can reduce",
    "340-360": " the communication cost without increasing the computation cost the problem is that each megabyte long record gets replicated almost three thousand times if we don't replicate it at all that everything has to be done by one reducer and therefore by one reduced task that means no parallelism at all in the reduced part and the wall clock time",
    "360-380": " is too great even if the communication is really small but there are compromises that can be made we can group the drugs into several groups the more groups we use the more parallelism we can get but the greater the communication cost in a sense the original attempt group drugs into 3,000",
    "380-400": " groups of size one which gives the maximum parallelism but also the maximum communication cost will focus on an example of 30 groups each with one drugs to be specific the first hundred drugs will be in Group one the next hundred are in group two and so on I've used the notation G of I to mean the",
    "400-420": " number of the group to which the I struggle belongs so here's the new map function now a key is a set of two groups rather than a set of two drugs keys actually really look the same they're a pair of numbers such as looks",
    "420-440": " like that but now N and M are interpreted as numbers of groups rather than drugs and of course in this case the range of the numbers is 1 to 30 rather than one to 3,000 now for each",
    "440-460": " drug I we produce 29 a value pairs and for each group number besides G of I the group drug I belongs to we have one key value pair whose key consists of G of I and the other group number and then all",
    "460-480": " 29 key value pairs the value is the record associated with the drug i coupled with a number i itself the reason we need to attach the drug number to its data is that unlike in the earlier case the reducer will not know what drug where the records represent unless the drug itself is attached to it or embedded as a component of the record each reducer corresponds to a key",
    "480-500": " consisting of two group numbers the lists associated with that key consists of 200 drug records 100 for each of the two groups the reducer for the pair of groups is responsible for comparing each pair of drugs that are are one from each of the groups there is a tricky matter",
    "500-520": " of who compares drugs in the same group 29 different reducers get all the drugs from group in and we don't want them all to do it a simple rule is to compare pairs of drugs and group in if M is 1 more than n in the end around cents that is for n from 1 to 29 M is 1 more than n",
    "520-540": " for n equals 30 M is 1 as a result each pair of drugs is compared at exactly one reducer the total computation cause doing the comparisons is thus the same as it was when the reducer was responsible for only one pair there might be a small amount of overhead as the reduced function organizes the 200 records and moves its attention from one",
    "540-560": " pair to another but it is small compared to the real work of doing the statistical tests in fact since a good part of the statistics gathering involves only one of the two members of the pair working in large groups actually saves some computation but the computation time is not the big story what changes is the communication cost",
    "560-580": " we still have three thousand drugs and each has a 1 megabyte record but now the mappers make 29 copies of each of these records rather than 2999 and that's a big difference the actual communication cost goes down from nine terabytes to eighty-seven",
    "580-600": " gigabytes still substantial but manageable on a student's computing budget"
}