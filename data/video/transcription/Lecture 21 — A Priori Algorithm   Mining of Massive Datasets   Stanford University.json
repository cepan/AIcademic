{
    "0-20": " we're now going to learn the basic op priori algorithm uh later we'll see some improvements to this basic idea but the fundamental Insight is monotonicity the idea that an item set cannot be frequent unless all its subsets are frequent uh the aiori algorithm uses one pass for finding the frequent items then another pass through the data for finding",
    "20-40": " frequent Pairs and if we want frequent triples we need another pass and and so on um each pass after the first can be thought of as having identified a small number of sets of the relevant size that might be frequent and therefore require a count but the power of our priority comes from the fact that for many data sets we can eliminate almost all sets",
    "40-60": " from candidacy and thus greatly reduce the number of counts we need to maintain in main memory we can think of the op priority algorithm as a two-pass algorithm since that is what it needs to find frequent pairs but as we just said if you want to go past pairs to larger item sets then you need cas passes to find frequent",
    "60-80": " item sets of size up to K the monotonicity property which a priori exploits says that if a set of items appears in at least s baskets then so does each of its subsets that should be obvious since there are s baskets that contain all members of the set so surely these s baskets also contain any",
    "80-100": " of its subsets and there may be more baskets that contain the subset but not the full set now we actually use use this definition in its contrapositive form for example when we're looking for frequent pairs the key observation is that if an item I does not appear s Times by itself then no set containing I",
    "100-120": " could be frequent because if say the set of I and J appears in s baskets then surely I appears in all those baskets uh and maybe others uh this is all reason really obvious but it's also essential and I want to make sure everyone is on board",
    "120-140": " on the first pass we count the number of times each item occurs we want to do this in main memory and and unless the number of items is beyond billions we can set up an integer count for each item in main memory after the first pass we see which items appear at least s times these are",
    "140-160": " the frequent items incidentally if all we want is to tell whether or not an item appears s and more times then it is sufficient to count up to S and not add ones beyond that so if say s is 10,000 then we only need to keep two bytes per count regardless of how many times items might appear in the",
    "160-180": " data now let's look at pass two where we read all the baskets from disk again and as in the naive algorithm we're going to try to count pairs in main memory but now we use monotonicity so we only have to count those pairs of items both of which are among the frequent items so for example if only half the items are",
    "180-200": " frequent we need to count only a quarter of all pairs okay the main memory we need depends on the square of the number of frequent items only but there's a small amount of additional main memory we need for a table that lets us know which of the items are in fact frequent as we",
    "200-220": " read a basket from disk we look at all its items and ignore any that are not in the table of frequent items from what remains we gener at all pairs and increment each of their counts so here's a picture the first of a series we're going to use to compare different algorithms the rectangles each represent",
    "220-240": " uh main memory and how it is used on each pass on the left we see the first pass of the op priori algorithm we need space and Main memory only for the counts of the items and we show it as occupying only a small fraction of main memory because typically we will need much",
    "240-260": " more main memory for the second pass in the second pass we distill the item Counts from the first pass down to a list of frequent items this list is probably implemented as a hash table or similar structure so given an item we can quickly tell whether it is frequent the rest of main memory is available for counting all the pairs of frequent items",
    "260-280": " for our priority to work in a reasonable amount of time that these counts must all be able to fit in main memory if there's still to many counts to maintain in main memory we need to try something else a different algorithm splitting the task among processors or even buying more",
    "280-300": " memory we might suppose that when you're counting only pairs of frequent items you have to use the tabular method to store counts uh since we hope that only a small fraction of the possible pairs need to be counted uh since the numbers associated with the frequent items are not likely to be consecutive it looks like we can't use the Triangular Matrix with with counts for only the pairs of",
    "300-320": " frequent items or if we did we'd use just as much space as if we were counting all pairs and we might not have that much space uh fortunately uh there's a a a simple trick we can use uh we renumber the frequent items starting at one and on the second pass we store a table that translates the original integers used for all items into the new",
    "320-340": " numbers for the frequent items only this table also tells you whether an item is frequent or not since it will not appear in the table if it is not frequent so here's the picture of our priori again we show the table on the second pass is taking more space than before since it stores two numbers per",
    "340-360": " frequent item uh and needs to store each item even those that are not frequent so we can index into the table given an old number and find either its number among the frequent items like one which has new fre number one three has new number two",
    "360-380": " uh or as in this case we can find it's not frequent there are better ways to organize the table that saves space if the fraction of items that are frequent is small for example we could use a hash table in which we stored only the frequent items with the key being the old number and the associated value being the new",
    "380-400": " number what we're we're not showing is a table that all algorithms may need one that translates from the representation of items in the Raw data to the old numbers uh that is the consecutive integers that we used for all the items the idea used on the second pass",
    "400-420": " extends to later passes that construct larger sets let's use the term k set for an item set with K members uh then there are two collections of K sets associated with their effort to find all the frequent K sets C sub k is the candidate case sets",
    "420-440": " these are the sets that based on what information we have from previous passes might be frequent at least we can't rule out the possibility of that being frequent by using monotonicity so we have to count them and then the result of the Ki pass we'll call L subk this is the subset of",
    "440-460": " C subk consisting of those case sets that are found on the kith paths to be really uh frequent now here's a picture of the full op priori algorithm including not only pairs but a suggestion of the process for larger item sets in fact this is the picture for a whole family of related",
    "460-480": " algor algorithms uh where each algorithm is characterized by a different way to construct the set of candidate pairs C2 each pass consists of a filter step where we look at the candidate sets for the pass and select only the frequent sets that is CK is turned into LK",
    "480-500": " k each pass also has a construction step where the candidates for the next pass are constructed from the frequen sets for the current pass that is on the k pass CK is constructed from L subk minus one we start with C1 the set of candidate Singleton item sets these are",
    "500-520": " all items since we have no way of eliminating any items without looking at the data the filter step for the first pass count counts the items and finds those that are frequent so the set L1 is just the frequent",
    "520-540": " items from L1 we construct C2 the set of candidate pairs for the second pass in this case we don't actually do anything the set C2 is defined implicitly from the list of the list of items in the set L1",
    "540-560": " okay the filter step for the second pass counts all the pairs in C2 and the result is the truly frequent pairs of items we can proceed like this from the frequent pairs we construct C3 the candidate triples by technique we'll",
    "560-580": " describe on the next slide then we filter C3 to get L3 from that we construct C4 and so on we can describe the our priori algorithm for item sets of all sizes and as an induction on K the size of the item sets we construct there there is one pass for",
    "580-600": " each K the basis is that C1 is the set of all items strictly speaking C1 consists of all the Singleton sets each of those sets containing one of the items given the set CK we construct LK by making a pass through the data and",
    "600-620": " Counting each set in CK those sets whose counts get up to the threshold s become members of LK the other part of the induction is how we construct CK + one from LK we look for sets of size k + one each of whose subsets of size K those you get by dropping one element are in",
    "620-640": " LK you have to be a little careful how you organize the search for example you wouldn't want to enumerate all sets of size four and for each one test if each of its four subsets of size three or an L3 a better idea is to start with some set set in LK for example assume k equals 3 uh we",
    "640-660": " might find the set let's say 135 is in LK now look at each item whose number is higher than any in the set say six okay uh this set might be in C4 we",
    "660-680": " already know its subset uh that is the subset of uh well we're talking about really 135 6 now we already know that its subset 135 um is uh is in L3 so we have to test uh three other sets we have to test um",
    "680-700": " well 356 and um 156 and I guess 136 and if all three of these uh are an L3 then we would put",
    "700-720": " this guy 1356 uh in C4 uh then uh we're not done with starting again with 135 uh we might then then throw in the element seven and so on then eight and and nine uh all all the time searching",
    "720-740": " for uh sets of size four each of whose uh subsets of size three uh are are known to be frequent the space needed on the on the the Ki pass is proportional to the number of sets in CK in principle uh there could be as",
    "740-760": " many as n choose K uh candidate sets of size k uh if there are K if there are are n items so the space requirement could grow painfully each time K increased but in cases where this method is used in practice the support",
    "760-780": " threshold is high enough that it K increases Beyond Two the number of candidate sets that can be formed from the frequent sets on the previous pass actually decreases doesn't increase uh thus the memory requirements Peak at k equals 2 and that's why we concentrate on finding frequent pairs and why the more advanced algorithms we'll see in",
    "780-800": " the next unit differ from our priori in how they handle the pairs"
}