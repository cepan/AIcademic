{
    "0-20": " welcome back to mining of massive data sets in the previous section we uh studied the map reduce model and how to solve some simple problems using map ruce in this section we're going to go under the hood of a map ruce system and understand how it actually works um just to refresh your memory um um a map ruce system um has a simple map",
    "20-40": " ruce process has three steps in the U map step you take a big uh document which is divided into chunks um and you run a map process on each chunk um and the map process uh goes through each record in that chunk uh and it outputs",
    "40-60": " intermediate key value pairs uh for each record in that in that chunk um in the uh second set step which is a group by step um you Group by key you U you bring together um all the values for for the same key um and in the third step which is a reduced step uh um you apply a",
    "60-80": " reducer to each um intermediate key value pair set and you create a final output now here's a schematic of how it actually works in uh in in a distributed system uh the previous schematic was how it worked in a in a in a centralized system uh in a distributed system you",
    "80-100": " actually have multiple nodes uh and map and reduce tasks are running uh in parallel on multiple nodes um so um the uh a few chunks of the file input file might be on um on node one a few chunks on node two and a few chunks on node three and you have map tasks running on on each of those nodes um and uh and",
    "100-120": " producing uh producing intermediate key value pairs on each of those nodes um and once the inter once the um intermediate key value pairs are produced um the underlying system uh the map R system uses a partitioning function which is just a hash function um so so the uh the um map system",
    "120-140": " applies a hash function to each uh intermediate key value and the hash function will tell uh the map reduce system which reduce node to send that key value pair to right this ensures that uh all all the the same key values whether they're in map Task 1 2 or three",
    "140-160": " end up being sent to the same reduce task right so in this case uh the key uh K4 regardless of where it's started from whether in 1 2 or three always ends up at reduced task one and the key K1 always ends up at reduced task two now once uh once the reduced task has uh A",
    "160-180": " reduced task has received uh input from all uh from all the map tasks uh all the map tasks have completed then you can start the reduced tasks um and the the reduced task's first job is is to sort um its input uh and group it together by by key uh and so in this case there are",
    "180-200": " three values associated with the KE key K4 they're all grouped together uh and once that is done the reduced task then invokes the reduceed function which is provided by the programmer um on each um each such uh group uh and creates the final output okay so uh remember the",
    "200-220": " programmer provides two functions map and reduce and specifies the input file uh the map reduce environment take has to take care of a bunch of things uh it takes care of partitioning the input data uh scheduling the program's execution on a set of machines figuring out where the map tasks run where the reduced tasks run and so on um it",
    "220-240": " performs a the intermediate Group by step uh and while all this is going on uh some nodes May Fail um and the environment makes sure that the uh node failures are hidden from the uh from the program um and finally the uh map R use environment also manages all the required intermine communication a so",
    "240-260": " we're going to take a look at exactly what's what's going on in a little bit so let's look at the data flow that's associated with uh with with map Ru um now the uh the input and the final output of a map reduc program are stored on the distributed file system um and U",
    "260-280": " the scheduler tries to schedule the map task close to the physical storage location of the input data what that means um is that recall the input uh uh data is is a file and the file is divided into chunks and there are replicas of the chunks on different uh chunk servers the uh map Ru system tries to schedule",
    "280-300": " each map task on a chunk server that holds a copy of the corresponding chunk so there's no actual copy um data copy associated with the map step of the map ruce program now the intermediate results are",
    "300-320": " actually not stored in the distributed file system but stored in the local file system of the map and reduced workers um what what are intermediate results intermediate results an intermediate results could be the output of a map step um an intermediate result could be uh something that that's emitted while while in the process of computing a reduce now why why are such intermediate",
    "320-340": " results not stored in the distributed file system it turns out that there's um some overhead to storing data in the distributed file system uh remember there are multiple replicas of the data that need to be made um and so there's a lot of um copying uh and network shuffling involved in in storing new data in the distributed file system so",
    "340-360": " whenever possible uh intermediate results are actually stored in the local file system of the map and redu workers instead of being stored in the distributed file system to avoid uh more Network traffic and finally um as you'll see in future example",
    "360-380": " uh the output of a map reduced task is often the input to another map reduced task now the master node uh takes care of all uh the coordination aspects of a map Produce job uh the master node keeps um you know Associates a task status",
    "380-400": " with each task a task is either a map task or a reduced task um and each task has um has a status flag and the status flag can either be idle in progress are completed um the master U schedules idle tasks whenever workers become available",
    "400-420": " whenever there is a free a node that is uh that's available for for scheduling tasks uh the master goes through its queue of idle tasks and schedules an idle task on that on that worker um when the when a map task completes it sends the um the the master the location and sizes of its uh the r intermediate files",
    "420-440": " that it that it creates now uh why R intermediate files there's one uh intermediate file that's created for each reducer uh because the data the output of the mapper has to be shipped to each of the reducers depending on the on the key value uh and so there are R intermediate files one for each reducer",
    "440-460": " so when when a map task completes it uh let it it it stores the r intermediate files on its local file system and it let the master know what the names of those files are the master pushes this information to the reducers once the reducers know that all the mappers uh map tasks have completed",
    "460-480": " then they copy the um intermediate files from each of the map tasks um and then they can proceed with their work now the master also periodically pings uh the workers to detect whether a worker has failed and if a worker has failed the master has to do something and we're going to see what that",
    "480-500": " something is if a map worker fails then the all the map tasks that were scheduled uh on that U on that map worker may have failed so the uh the tricky thing is that the output of a map task is written",
    "500-520": " to the local file system of the of the map worker so if a map worker fails the the node um fails uh then all the intermediate output created by all the map tasks that have run on that worker are lost um and so the uh what the master does is that it um resets to idle the status of every task that was either",
    "520-540": " completed or in progress on that worker right um and so all those tasks need to be eventually redone and they will eventually be rescheduled on other workers in due course if a reduced worker fails on the other hand uh only the in progress tasks are set to Idol the tasks that have actually been completed by the reduced",
    "540-560": " worker don't need to be set to idle because the output of the reduced worker is a final output and it's written to the distributed file system and not to the local file system of the reduced worker since the output is written to the uh distributed file system uh the output is not lost even if the reduced worker fails so only in progress tasks need to be set to idle while completed",
    "560-580": " tasks uh don't need to be redone right and so the and once again the idle reduced tasks will be restarted on other workers eventually what happens if the master fails if the master node fails then uh the map reduce task is aborted the client is notified and the client um can then do something like restarting the map reduce task so um this is the",
    "580-600": " one scenario where the task will have to be restarted from scratch because the master is typically not replicated uh in the map reduce system now you might think that this is a big deal that uh that uh the master failure means that the uh the map reduced task is aborted and then the task has been started but",
    "600-620": " remember node failures are actually rather rare node fails as you recall once every 3 years or once every Thousand Days um and the master is is a single node uh and therefore the chance of a master failing is actually quite um you know it's it's quite an uncommon occurrence uh the the problem that you have is that when you have multiple",
    "620-640": " workers associated in um in a map ruce task it's much more likely that uh one of many workers failed rather than the master failing so the final question uh to think about is how many map and how many reduced uh jobs uh do we need okay suppose um you know Suppose there",
    "640-660": " are M map tasks and R reduced tasks um our goal is to determine M and R this is part of the input that's given to the map redu system uh to let it know how many tasks to uh tasks it needs to schedule um the rule of thumb is to make m much larger than the number of nodes in the",
    "660-680": " cluster you might think that it's sufficient to have one map task per node in the cluster but in fact it the rule of thumb is to have one map task per DFS chunk the reason for this is simple imagine that there is one map task per node in the cluster and during um you know during processing uh the node fails",
    "680-700": " if a node fails then that map task needs to be rescheduled on another node uh in in the cluster when it becomes available now since all the other nodes are processing uh you know one of the map tasks has to one of those no has to complete uh before this map task can be",
    "700-720": " uh scheduled on that node and so the entire um computation is slowed down um by the time it takes to you know complete this map task the failed uh redo the failed map task so if instead of one map task on a given node there are many small map tasks on a given node and that node",
    "720-740": " fails then those map tasks can then be spread across all the available nodes and so the entire task will complete much faster on the other hand the number of reducers R is usually smaller than uh M and is usually even smaller than the total number of nodes in the system U",
    "740-760": " and this is because the uh the output file uh is um spread across um spread across R nodes where R is the number of reducers um and if um it's usually convenient to have the output um spread across a small number of nodes rather than across a large number of nodes and so usually R",
    "760-780": " is set to a smaller value than m"
}