{
    "0-20": " in this unit we're going to look at several algorithms that improve upon our priori One Direction is to use the portion of main memory that the a priori algorithm doesn't need on the first pass because all is doing is counting Singleton item sets that is the PCY or Park chin U algorithm uh we'll talk also about",
    "20-40": " further extensions called multi-stage and multi has all these extensions to AR priori have the goal of minimizing the number of pairs that actually have to be counted on the second pass then we'll go in another Direction we'll accept that the result might not be accurate that is we'll allow the algorithm to fail to identify some of the frequent item sets but we want to",
    "40-60": " finish quickly preferably in only one pass or perhaps one pass and a little more several approaches of this type will also be dis discussed now the essence of the PCY algorithm is to take advantage of the fact that if the set of items is modest in size we have lots of space during the",
    "60-80": " first pass that we don't seem to need we can in fact do some counting that will give us a smaller candidate set of pairs that will help us on the second pass where the main memory is often the critical resource since we don't have to store as many counts okay what park Chen and you",
    "80-100": " suggested was that we could hash all pairs into a large number of buckets however these buckets do not hold the pairs themselves just the count of how many baskets contain pairs that hash to that bucket thus the space needed for a bucket is small four bytes is surely sufficient and we might get away with two bytes if the support",
    "100-120": " threshold is less than 2 to the 16th notice that it is not possible to store in main memory all the pairs themselves we're trying to avoid having to use some space for every possible pair in main memory so in the first pass when we read a basket we not only increment the count for each of its items as we did for the",
    "120-140": " first passive a priori we also hash each pair that is contained in the basket and for each resulting uh bucket we increment its count by one now let's call a bucket frequent if after the first pass we find that its count is at least equal to the support",
    "140-160": " threshold a bucket could become frequent simply because many pairs each contribute a little to its total count we can't tell if that's the case because we only had room to remember the total not the count of each individual pair that contributed to the bucket but in the opposite case we win",
    "160-180": " if a bucket is not frequent then no pair that hashes to it could possibly be frequent so on the second pass we only need to need to count pairs that hash to a frequent bucket and of course we you the usual op priori trick works too we only have to count a pair if both it its members are are frequent items",
    "180-200": " so here's the picture of PCY that i' would like you to have okay on the first path we use a little space to count the items and the rest of the space is devoted to these bucket counts uh on the second pass we keep a table of frequent items that's that's that just just as we",
    "200-220": " did for op prior uh but we also summarize the results of hashing pairs on the first pass by what we'll call a bit map that's that's this for each bucket the bit map has one pair that says",
    "220-240": " whether the bucket Is frequent or night or not so if this is a bucket and if it has a count let's say that it's s+ one it's it's frequent uh then it corresponds to some bit here that's a one okay another bucket that has a count let's say s minus",
    "240-260": " 100 uh that's not frequent so it would get its corresponding bit would be a a zero okay if the buckets are say four bytes then we get a 32.1 uh to 32 to one uh compression uh and yet we have all the information we need for the second pass which is is the",
    "260-280": " bucket frequent or not thus we have almost as much space on the second pass to count candidate pairs as we would for our priori that's of course this this areaa uh but we hope that because the bucket counts will eliminate most of the candidates that our prior would have to handle there will be many fewer pairs to count on the",
    "280-300": " second pass if that is the case then the PCY algorithm might be able to operate in main memory on the second pass while a priori would run out of space or have to use dis on pass one we use what space we need to count the occurrences of each",
    "300-320": " item uh just as our priori does typically four bytes per item will do nicely but the difference is that whatever space we do not need for counting items is given over to as large a hash table as we can where each bucket is a count typically a 4 byte integer as",
    "320-340": " well so here's the pseudo code for the first pass of the PCY algorithm we read each basket one as we would uh on any pass of any algorithm we look at each item in the basket and add one to its count okay",
    "340-360": " that of course is just what AR priori does the additional work on the first pass of PCY is we have to look at each pair of items and hash the pair to some bucket and increment the count for that bucket I want to remind you of two important observations about the buck buckets okay uh first if a pair Is",
    "360-380": " frequent then the bucket it hashes to is is surely frequent thus on the second pass the bit in the bit map for that bucket will be one and we will surely maintain a count for this pair unfortunately there may be many infrequent pairs that hash to the same bucket these might be counted on the",
    "380-400": " second pass as well okay we can hope that at least one member of the pair is not a frequent item because that will inhibit us from counting the pair on the second pass uh but sometimes you will have a pair that is not frequent but both of its its items are frequent and it hashes to a frequent bucket that pair will be counted on the second pass even",
    "400-420": " though it is not frequent it is also possible that a bucket will be be frequent but there is no frequent pair hashing to that bucket uh remember each buckets count is the sum of the counts of all the pairs that hash to it but the only time that we have to count a pair on the second pass",
    "420-440": " and that PA pair turns out not to be frequent is when the pair consists of two frequent items and for one of the two reasons I've just mentioned it's its bucket turns out to be frequent however we get a lot of Leverage when a bucket is not frequent",
    "440-460": " in that case we do not have to count any of the pairs that hash of that bucket even if the pair consists of two frequent items after pass one we set up pass two by doing the following uh first uh we construct the bit map from the bucket uh uh this bit map has one bit for each bucket in order so we can easily look up",
    "460-480": " the bit that corresponds to a given hash value one means the bucket was frequent and zero means it was not and as discussed if the buckets are 4 byte integers we get it 32 to1 uh compression when we uh replace the buckets by the bit map and as in the AR priori algorithm we",
    "480-500": " need to create a list of the frequent items to use on the second pass a draw on on pass two is to count the candidate pairs in the PCY algorithm in order to be a candidate pair JJ must satisfy two different conditions okay first of all both I and J must have been found frequent on the first pass for our",
    "500-520": " priority this is the only condition and all pairs satisfying it are candidates but the new condition is that the pair itself must hash to a bucket that was found frequent in the first pass it is easy to see that if the pair IJ is really frequent then both these conditions will be satisfied so this pair will be counted condition one",
    "520-540": " follows for monotonicity and condition two is satisfied because the count of a bucket is the sum of the counts of all the pairs that hash to it that count cannot be smaller than any of the individual counts uh we expect that each bucket requires only a small number of bytes four is Almost sure to be enough and in",
    "540-560": " many cases we can get by with two bytes per bucket Okay the reason is that it is sufficient to count up to the support threshold s even though we said we should always increment the count when a pair hashes to the bucket what we really want to do is if the current count is less than S and increment the count otherwise leave the count at s the number of buckets will be some",
    "560-580": " reasonable fraction of the size of main memory typically almost half or quarter the only time that would not be true is if there were so many different items that we needed all or most of main memory just to count them in that case we could not use the PCY algorithm when we count pairs on the",
    "580-600": " second pass we have to use the tabular method the problem is that the pairs that are eliminated because they hash to an infrequent bucket are scattered all over the place and cannot be or organized into a nice triangular array as a result we should not use PCY unless we can eliminate at least 2third of the candidate p",
    "600-620": " when compared with a priori we're now going to introduce an improvement on PCY called multi-stage this algorithm actually uses more than two passes to find frequent pairs uh we'll concentrate on the three pass version uh the benefit of the extra pass is that on the final pass when we",
    "620-640": " have to count the candidate pairs we have eliminated many of the candidates that PCY would count but that turn out not to be frequent the three-stage version begins with the first pass of PCY the second pass however is a repeat of the first pass of PCY with a different hash function the",
    "640-660": " third pass has a stringent requirement for candidacy the pair not only must hash to a frequent bucket on the first pass but must also hash to a frequent bucket on the second pass and on the second pass we don't increment the bucket counts for just any pair rather we only increment the count when the pair meets the criteria to be a",
    "660-680": " candidate on the second pass of PCY that is it must consist of two frequent items and it must have hashed to a frequent bucket as a result on the second pass we get two benefits first the total count of all the buckets will be less because their sum excludes some of the pairs that appear in the data that means it is",
    "680-700": " less likely that a bucket will be frequent on the second Pass unless it includes a frequent pair but also in the case that a pair is not frequent it will only become a candidate if it accidentally hashed to a frequent bucket on both passes not just one pass as for PCY so here's the picture of the",
    "700-720": " three-stage algorithm as I said the first pass is just like PCY okay the the second pass involves features from both passes of PCY on this pass we store the frequent items that's this uh and we summarize the buckets in",
    "720-740": " a bit bit map as before we use both these features to establish which pairs are candidates on the second pass but in the multi-stage algorithm we don't count pairs just because they are candidates we just hash the candidates to another hash",
    "740-760": " table using a different hash function this this hash table is approximately as large as the hash table in the first pass depending upon whether the bit map plus the list of frequent items takes up more or less or less space than the counts of all the items on the first pass okay for the third pass we we",
    "760-780": " maintain the list of frequent items and the first bit map but we also summarize the buckets of the second hash table by a second bit map and now we count all the candidate pairs where to be a candidate you have to satisfy three conditions on the next slide",
    "780-800": " on the third pass the the candidate pairs are defined as follows first both items are frequent by themselves this is the op priori condition for candidacy and then according to the hash function used for the first pass the pair hashes to a frequent bucket these",
    "800-820": " first two conditions are the PCY conditions for candidacy and finally according to the hash function used on the second pass the pair also hashes to a frequent bucket here are two points about multi-stage that we should bear in mind okay first the hash functions used at",
    "820-840": " each stage need to be independent of one another otherwise each would report the same frequent buckets and we wouldn't get any benefit from the additional stage a common question about this algorithm is why we need the second condition that the pair hashes to a frequent buck on the first pass after all unless a p a",
    "840-860": " pair meets the second Condition it's not hashed to a bucket on the second pass but if we don't enforce the second condition along with the other two conditions on the third pass then we can get another an unnecessary candidate for the third pass uh so consider a pair I",
    "860-880": " J where I andj are frequent but the pair is not frequent okay moreover the pair hashes to an infrequent bucket on the first pass now it is possible that J hashes to a frequent bucket on the second pass true in the multi-stage algorithm JJ is",
    "880-900": " not hashed on the second pass and doesn't contribute to the count of its bucket but there's still some bucket that it would have hashed to had we hashed it on the second pass and that bucket might be frequent for reasons that have nothing to do with I andj uh so we must test condition two along with",
    "900-920": " the other conditions next we're going to look at an approach called multi has this algorithm is similar to PCY except that on the first pass the available space is divided into several hash functions now this idea could be insane",
    "920-940": " remember that PCI works if the average count of a bucket is much less than the support threshold s that way most buckets are infrequent then we could eliminate a lot of pairs from the candidate set for the second pass if we have the number of buckets we double the average count so suppose the average count for both hash tables is going to",
    "940-960": " be over the threshold then almost all buckets are frequent and you eliminate almost nothing on the second pass uh but let's be optimistic okay if PCY gives you an average count of on10th of the support threshold then you can use five hash tables instead have an average count of half the threshold and each hash table",
    "960-980": " will have mostly infrequent buckets but now you have five chances to eliminate a pair that is not actually frequent and the chances are very high that at least one of the hash functions will send it to an infrequent bucket in that case we get much of the benefit of five stages with a multi-stage algorithm but we only",
    "980-1000": " use only two passes and not six so here's a picture of multi has will used two hash tables on the first pass and the general idea where more than two tables are used should be obvious uh on the second pass the item counts is summarized as a list of",
    "1000-1020": " frequent items just as in PCY or multi-stage uh but also the hash table from the first pass is summarized by a bit map as in PCY notice that the number of buckets created on the first pass is the same whether we use one hash table as in as in PCY or two or any number thus the sum of the lengths for the bit Maps is the",
    "1020-1040": " same for multi has as it is for PCY and less than it would be for multi-stage on the second pass we count the candidate pairs as always the requirements for a pair IG to be a candidate pair is what you should expect I and J must both be frequent items and",
    "1040-1060": " for each hash table the pair must hash to a frequent bucket"
}