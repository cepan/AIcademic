{
    "0-20": " so the the central question when working with decision trees is to construct them or learning them this is basically the harder part once we have the tree classification is very easy the hard part is creating the tree so let's now look at how would we want to go about creating a tree imagine we are given a training data set of 100 examples right",
    "20-40": " just as an example and we want to figure out um how to build a tree the way we are building the tree is in this kind of recursive fashion for from top to bottom so the idea is that first we decide on a decision node we then decide on a condition on which attribute and which value are we going to create a split and",
    "40-60": " then the data that that satisfies the condition goes to the left and the data that doesn't satisfy the condition goes to the right and imagine that now we have we started with 100 examples that we kind of throw from the top of the tree uh 10 of them go to the left and 90 of them go to the right so now I can think that that I have um an again the",
    "60-80": " very similar problem right I have a training data set now on the left hand side of 10 examples and I need to decide what to do one option will be to say look okay I will put a decision note here I will decide which is the next feature next feature to split on what is the value and that would create the left sub three and the right sub three or I",
    "80-100": " could decide that I only see 10 training examples that is very little let me actually go and create a prediction note here so let me stop building the tree on this side but let me create a prediction node and here is my you know prediction node predicting the kind of the best prediction possible 42 right on the other hand um now I need to take the",
    "100-120": " right hand side of the tree and decide what what I want to do do on this side for example I could either say okay let's just create another prediction note here and be done with creating the tree or I can say let me create another split note right so I create let's say another split node using some way to decide how to do this I need then to find another um variable on which I want",
    "120-140": " to split and another um value so this creates another condition which then further goes and takes this data set of 90 examples and splits them to the left hand side and to the right hand side or to the left branch and to the right Branch so now imagine that you know this split uh evenly uh puts 45 of the",
    "140-160": " training examples to the right and 45 um to the other side and now I we can continue recursively building this process right the set of 45 training examples we need to decide what to do with them do we create a split or not um imagine we create another split so which means we would now create um another",
    "160-180": " condition pick another feature we could actually go pick the same feature again as we had above and this way keep doing the three and what you see is that by with every split we kind of also split our data set to the smaller and smaller chunks right and the idea is that the whole process keeps repeating until until we want to decide that it's been",
    "180-200": " enough and we want to stop and create a prediction node so if we think about this basically the basic operation is that we imagine that we are currently um residing in some node of the three let's call this node G and um let this subg be the data that reaches um this node G",
    "200-220": " right as we take our training data set on the top we kind of Let It Drop throughout the tree and we are asking what is the subset of data that reaches our node G then the central decision we have to make is do we continue building the tree at this node G or not right and if we say yes we continue then the next",
    "220-240": " question is which variable or which feature and which value do we create a split upon right so what will be splitting here what is the condition on which we make the split and kind of send some some data to the left and some data to the right and if the answer to our question whether do we continue building the three is no if that's the answer",
    "240-260": " then the question is how do we make a prediction right we need to create this hexagonal as we draw it predictor node and we need to decide how are we predicting the Y in this case right so if the answer is yes let's keep building the tree we need to figure out how are we creating the split node if the answer is no we have to figure out how to build",
    "260-280": " a prediction Noe um so the way the decision Tre is built basically is this very simple um recursive construction where we have a function we can think of it as build sub tree and all this function does is the following it says for a given data set that that arrives into this particular subtree let us",
    "280-300": " figure out what is the best feature and the best value to split upon right so this is the definition of the split and then what is the data that goes to the left branch of the split what is the data that goes to the right branch of the split now we say okay if for the left hand side some stopping Criterion is met then um",
    "300-320": " let's create a prediction node um and otherwise let's go and continue building the tree using the using the data that goes to the left hand side and we do the same thing with the right hand side right we say are we satisfied with all the data that came into the sub tree if this if there is some stopping criteria that says yes then we say okay let's",
    "320-340": " build a prediction out otherwise let's continue building the tree right so at every step all we have to do is find the split and then check whether this stopping Criterion is met and if it's met we build a prediction node so our hexagonal node otherwise we keep building the tree by recursively calling the build sub three function so this is",
    "340-360": " the basic idea how to do this so what I will do next is I will slowly go through these um uh important functions one two and three and address um how do we Implement them and how do we think about them and in particular we will start with the first function which is find best split so the question is how do we find find a good split in the data which",
    "360-380": " means how do we identify the feature and the value on which to split so the question is how do we split and the idea is when I say how do we split is how do we pick the attribute and the value that that that gives us a good split so what we have to do is we have to measure how or create a Criterion or a measure that",
    "380-400": " tells us how how successful is splitting on a on a given value and now when I say how successful it something is I need come to this notion of Information Gain right if we think about intuitively our idea is that whenever I create a split I want to do it if we think about classification let's go kind of back at",
    "400-420": " predicting whether somebody's wealthy or not right I want to create a very a split in such a way that all the wealthy people go to the left and all the not wealthy people go to the right so the basically the idea is that I would try to find this magic condition that sends everyone to the left and sends kind of everyone that's not wealthy to the right",
    "420-440": " the the of course on in real data we won't be able to do this but we would like to know for every feature how close to this ideal um condition does it does it does it bring us and here is where the notion of Information Gain Is is arriving so what Information Gain tells us is tells us how much a given attributes attribute tells us about the",
    "440-460": " class y right so what is kind of the the the information about class y that is stored in a given attribute or feature X to understand the concept of Information Gain here is how we will think about it so we say that we want to have a quantity that we call Information Gain",
    "460-480": " of Y given X and the way we think about this we think that there is a let's say that there are two people that that that that can communicate over a phone or over a binary line and what what the person of the L on the left to the right to do it would like to tell the person on the right um um who is Rich so now",
    "480-500": " for example if a person on the right knows nothing about um the properties of the people then we have to for every person we have to tell this this person on the right um who is Rich and who is not now what the Information Gain tells us is to say how many bits on the average would would of communication would we save if if actually both ends",
    "500-520": " of the line would know uh the attribute X so imagine that um both ends of the line know that right now we are only talking about uh old people that have that have spent lots of time in tech industry for example this way then communicating who is Rich or not would require much less information because we would already",
    "520-540": " have a high prior that old people in Industry are Reach For example right so the idea is in some sense how much more in communic in communication would we save if we if we both know the value of this attribute X so let me now slowly derive how how we do Information Gain and this with this will also make it",
    "540-560": " clear what do we mean by it so given that we now know how to create a split on categorical attributes right so for categorical attributes we want to do the information gain now the question is how do we find the best split in other cases for example for continuous variables if variable is continuous then the way we",
    "560-580": " find the best split is to compute what is called the impurity right so the idea will be that that we want to create we have a data set D that come inside the node and then it gets split to the data set on the left and the data set to the right right so the idea is that um data data set on the left Union data set on the right equals the the whole data set",
    "580-600": " right so something comes inside the node D comes in and then this sub L goes to the left and this subr goes to the right then what I can do is I can compute the variance of all the all the examples in D multiplied by the size of D and then from that I subtract the the sum of the",
    "600-620": " variance um of the left data set plus the variance of the right data set of course both scaled by the sizes and the idea is if this um difference is high what this means is that initially the data set D had very high variance afterwards um the the the",
    "620-640": " resulting variance after we split was very small right and when I say the variance of D I simply mean what is the variance of the target variable right so it's the average value of y minus the I value of y squared summed up and taken the average so this is what I mean by variance and the goal is that the idea is that at the beginning I have lots of",
    "640-660": " variance after the split each individual um data sets so the left and one data sets have uh small small variances and the idea is I want to pick the the feature on which to split and the value such that this impurity or difference this difference in variances is as large",
    "660-680": " as possible so now that we know how to uh this create a split the next question is when do we decide to stop right so if we decide to create a split and we have categorical variables we want to use Information Gain if we have um real valued or integer variables we want to",
    "680-700": " use this decrease in in impurity or maximize that difference in variances now the question is when do we decide to stop so here we are basically having many possible different turistic options to do this for example one option is to say I will stop and create an um a prediction node when the leaf is pure",
    "700-720": " for example we can ask aha the variance of the y y values in that leaf is small so we want to create a prediction node we could uh another possible solution would be to say I will stop building the building the tree in that when the number of examples that comes into the given node is small so for example less",
    "720-740": " than 10 or in practice we would want to use both of these rules and stop building as soon as one of them is satisfied right if we get very few training examples arrive to a given um node this means we have very little data there so it's better stop building the tree if we have the data that comes to a",
    "740-760": " given note to be very pure so if everything is of the same class then again we should stop building the tree and just make a prediction so this is about the second question and the last question is how do we make a prediction I already discussed this a bit right once we decide that we want to make a prediction note for example for regression it's very easy we can just predict the majority um the majority",
    "760-780": " value or for example we can predict the average y of all the data that is in the leaf um what we could also do is we could take all the data that is in the leaf and then build a very small linear regression model right so the idea would be that um I take all the data that arrives into into the tree and then I",
    "780-800": " build a regression model that fits all the data that falls into that individual tree leaf um for classification what is most often done is to say let's predict the most common value of y right basically let's predict the majority value uh that is in the class or in the leaf of the",
    "800-820": " tree"
}