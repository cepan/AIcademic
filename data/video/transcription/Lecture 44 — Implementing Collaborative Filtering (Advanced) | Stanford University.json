{
    "0-20": " now let's turn to the task of actually implementing collaborative filtering and look at the complexity of the implementation the most expensive step in implementing collaborative filtering is the step of finding the que most similar users or the que most similar items in the neighborhood of an item or a user remember what what we had to do",
    "20-40": " to find the que most similar item was to take the item and compute its similarity with respect to every other item and this required us to compute for example a cosine or a centered cosine distance between the item of interest and every other item if you actually did this at every step we'd have to actually look at",
    "40-60": " every entry in the utility matrix and so the complexity of doing that turns out to be the size of the utility matrix you now since utility matrix can be really very large because there are many many users millions of users and millions of items clearly it's impractical to do this at runtime so what you really have",
    "60-80": " to do is to pre-compute pre-compute for every item the set of other similar items or for every user the set of similar users but even in my EP computation can take a really long time suppose there are n n n items and we are doing item two item collaborative",
    "80-100": " filtering for each item we'd have to compute its similarity to every other item and so the complexity of doing that is a product of the number of items and the size of the utility matrix now since utility matrix can be really large and the number of items can be really large the product of these two is clearly a very large number and so",
    "100-120": " even a naive pre-computation can can be too expensive to do in practice so how do you deal with this now in previous lectures we looked at a way of doing this and that technique those are the techniques for new neighbors search in high dimensions for example locality of",
    "120-140": " sensitive locality sensitive hashing we can use something like lfh to take an item and quickly find a set of new neighbors to that item or take a user and find a set of neonate as to that user and do that in advance in practice you might do something like this you know once in a day for it for",
    "140-160": " example or once every few hours you can also use clustering to the group users and items into into smaller clusters and thereby speed up the process by restricting the search to within a cluster as opposed to into in the entire set of items our users and finally we can use a set of techniques called a",
    "160-180": " dimensionality reduction methods which we'll cover in an upcoming lecture what are the bandages and disadvantages of collaborative filtering the biggest advantage of collaborative filtering is and it works for any kind of item it doesn't matter whether the item is books or music or videos or news articles or people collaborative filtering this",
    "180-200": " works for any kind of item without requiring any feature selection which is actually the biggest advantage of collaborative filtering because it turns out to be a really tough problem to find the right set of features for something as complicated as a movie or or or a piece of music and this it sort of also",
    "200-220": " explained the huge popularity of collaborative filtering because it sort of obviates this need for feature selection for complex things such as images and movies and music and so on that said collaborative filtering also has a number of disadvantages the first of these is cold start for collaborative filtering to work you need enough users in the system who have rated enough",
    "220-240": " items remember we need to given an item we have to find a set of other similar items or given a user we refined a set of other similar users but if there are not enough users in the system it's hard to find a match the second problem is is one a sparsity even when we do have enough users in the in the in the system",
    "240-260": " because there are so many items typically there are millions of users and millions and millions of items most users have not rated most items and therefore the user ratings matrix is very very sparse indeed and it's hard to find you know a user a pair of users",
    "260-280": " have a clear eight in the same set of items right so remember is supposed wanted to predict the rating for user X and an item I I need to find other users who also rated item I it might be really hard to find a such set of users because of sparsity the next problem is the",
    "280-300": " first data problem suppose we add a new item to the catalog right it's you cannot make recommend this new item to anybody because the new item doesn't have any ratings and esoteric items for instance that are actually you know",
    "300-320": " there may be a few people who really love the item but the number of people who like the items really small then to have a smaller number of ratings and it's hard to make recommendations for those items too and the final problem of collaborative filtering is a tensor of popularity bias let's take a really",
    "320-340": " popular movie or book like Harry Potter now lots of people tend to give high ratings to such a popular a popular book or movie and collaborative filtering will therefore tend to recommend a popular book or movie to to almost",
    "340-360": " everyone in the system regardless of whether they'd actually liked it or not just because the neighborhood of any item will include popular items at Amazon we used to call this problem the Harry Potter effect and the Harry Potter effect need not be a bad thing recommending popular items generally works out well but it's you know the",
    "360-380": " problem is if every item that's recommended is a popular item the popular items can completely crowd out the unique recommendations that can be made for a specific user and you have to take special steps to avoid the popularity bias from washing out the specific recommendations now that we've",
    "380-400": " seen some of the difficulties with collaborative filtering we can design hybrid methods to overcome those difficulties for example we can add content based methods to collaborate a filter we can add item profiles to deal with a new item problem and make recommendations of new items to users or we might you know take new users",
    "400-420": " and use demographic information about new users to build synthetic profiles for them and that's deal with the new user problem another approach is to implement two or more different recommender systems and combine their predictions perhaps using the linear model for example you could use a global baseline recommender and combine that",
    "420-440": " with collaborative filtering and let's see how to do this so here's a problem we'd like to estimate Jos rating for the movie The Sixth Sense the problem is that Joo has not rated any movies similar to The Sixth Sense according to",
    "440-460": " our measure of similarity and so using the collaborative filtering formula that we saw in the previous slides we can make no prediction for Jos waiting for the movie The Sixth Sense this is a problem that arises from the sparsity of the rating matrix so here is an approach called a global baseline approach and",
    "460-480": " it's actually very very simple suppose in our in our system the mean movie rating is three point seven stars three point seven if the average rating for across all movies and all users and we observe that the sixth sense the average rating for six cents is zero",
    "480-500": " point five stars above the mean movie rating people like the six cents on average greater you know more than the average movie we also notice that Joe rate zero point two stars below the average rating for other users right this is not remember this is not Joe's",
    "500-520": " rating for the sixth sense of for any specific movie but if you take the average of Joe's rating Joe turns would be a tough writer and his average rating is 3.5 stars are supposed to the mean rating of three point seven stars and so Joe's rating dos rating in general or zero point two stars below average now",
    "520-540": " we can use these three numbers to come up with the baseline estimate for the user Joe and the movie six cents we start with a mean rating which is three point seven notice that the six cents is zero point five stars above average and then we subtract the zero point two because Joe's rating is on",
    "540-560": " average zero point two stars below the average rating and we predict that Joe will give the six cents four stars notice that in this case we have not used any specific information about the movies yo has rated or they don't need you to have rated any movies",
    "560-580": " similar to six cents in order to make this prediction all we need is to have enough users and who have actually rated the sixth sense and so that we can actually compute an average rating for for the you know for the six cents now",
    "580-600": " what we'd like to do is actually combine this global baseline rating with collaborative filtering so let's let's look at an example so the global baseline estimated that Joe Blow will give the six cents four stars now suppose we use a collaborative filtering a nearest neighbor approach and we actually find out that Joe didn't like",
    "600-620": " the movie science and science actually happens to be a movie that's very similar to The Sixth Sense it's actually the same director and so and let's say the similarity between science and six cents if is one point four we find out that Joe didn't like science and in fact",
    "620-640": " two rated science one star below his average rating for all movies so now we can combine the global baseline estimate and the collaborative filtering refinement and come up with the final estimate so since the global baseline estimates that Joe will rate the sixth sense four stars whereas the",
    "640-660": " collaborative filtering gives it a negative one below his average rating so we can just add those two ratings and predict that Joe will grade the six tens three stars so notice that this approach actually takes a linear combination of two independent classifiers or global",
    "660-680": " baseline classifier and the collaborative filtering classifier and takes some of those you know it takes the thumb of those predictions and if you wanted we could have waited these predictions in different ways as well in this case we waited both of those are equal so finally here's the formula that you can",
    "680-700": " use to implement the the combination between the global baseline and a collaborative filtering let's define s IJ to be the similarity of items I and J and given an item I we are going to find its K nearest neighbors and we are only",
    "700-720": " going to find the K nearest neighbors that have also been rated by user X and our goal is to estimate the rating for user X an item i and above here i've given the the simpler formula that we had which is just a weighted average rating for all the items in the",
    "720-740": " neighborhood ni x now we are going to add in the global baseline idea and here's what the new formula looks like we are going to take estimate the rating RX i to be the sum of a baseline rating and the collaborative filtering refinement b a.b x i here is the",
    "740-760": " baseline estimate and the baseline estimate for user x and item i is itself the sum of three components mu which is the overall mean movie rating in the system bx which is the rating deviation of user x which is just the average rating that user X gives across all",
    "760-780": " movies that he has rated minus mu + bi similarly the rating deviation of movie hi and if you add those three up you get B X I which is a baseline rating for user X and item I and then we are going to add in the collaborative filtering piece which is the same as a formula we",
    "780-800": " had before the weighted average weighted by a similarity of item I and item J in the neighborhood however instead of using our XJ which is the rating of user X for item J we're just going to subtract out the baseline piece and only look at the the the deviation in the rating for the for",
    "800-820": " the item from the baseline since you've already added the baseline piece we don't want to double count it in the second piece here as well so you subtracted out the the baseline ratings from the collaborative filtering piece and this gives us the final formula that combines collaborative filtering and the",
    "820-840": " baseline approach"
}