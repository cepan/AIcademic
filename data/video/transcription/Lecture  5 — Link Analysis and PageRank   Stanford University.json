{
    "0-20": " so today we will start with the with a new topic and we will start looking at the analysis of large graphs and in particular we will talk about link analysis and pagerank so here is the idea so so far this is how the class fits together and we are starting with a new topic with a new set of data that is the graph data and in this module of the",
    "20-40": " class we will look at link analysis methods like pagerank and simrank where we look at Community detection with where the idea is that we want to find clusters of nodes in the network and then we will also look at spam detection where the idea is that we want to identify nodes that are spam spam nodes in the graph so those are the three",
    "40-60": " modules for the graph data section if we think about graphs graph graphs are everywhere in a sense that for example social networks uh Facebook Twitter and things like that can very naturally be represented as graphs graphs in a sense of as a set of nodes and a set of edges or connections or",
    "60-80": " intersections between them another set of data points that also can be represented as graphs are social media networks for example here in this graph what I'm showing you is a is an illustration of the structure of the United States blogosphere around presidential election in 2004 and what",
    "80-100": " you see is basically these two clumps in these networks these two communities and they basically correspond to the two political parties in the United States system and you see how this class the nodes in one cluster dance link to the other clusters and there is some number of cross-linking between the two so there is some amount of polarization in a sense another another set of data that",
    "100-120": " can naturally be represented those networks are the networks of information so for example in this in this case what we are seeing here is a map of science so here every node is a is a different journal and or a different conference and now the edges between these journals or publication venues mean that one",
    "120-140": " Journal is citing the other Journal So based on this citation Network between journals we can basically visualize how different disciplines of Science and subfields of science how they are relating to each other of course internet is another case where that can be studied as a graph so here we have",
    "140-160": " computers or a routers talking to each other and again this can be represented as a dynamic network of nodes which represent computers or routers and then let's say physical links or between between these machines and those are the edges of the network of course kind of the technological networks are also the",
    "160-180": " oldest example of graphs people have been studying so for example the the field of graph Theory goes goes back to 1700s when Euler posed this problem about the Seven Bridges of of koenigsberg where the idea is that we want to cross um at some point and travel each Bridge only once and the question is can that",
    "180-200": " be done and this can be formulated as a graph problem and the examples of other technological networks for example are power grids Road networks water distribution networks and so on and it's it's important for us to understand the structure of these networks to detect failures to detect disease outbreaks or",
    "200-220": " contaminations and so on um another example of a big part of kind of networks is is the web right so web itself can be represented as a graph and what we will do today we will kind of focus on the structure of the web graph and we will develop methods that allow us to learn something about the the pages on the web so the first question",
    "220-240": " is how do we represent a web as a graph we will represent web as a directed graph so in our graph nodes will be will correspond to web pages so every web page will be will be a node in this graph and now we will have directed links between these web pages that correspond to hyperlinks right so if I have my example here I have a set of",
    "240-260": " four web pages and now this web pages contain hyperlinks so in this case a particular a particular web page points to uh to another to another page via a hyperlink so we can use now these hyperlink relationships to create a network right so here is a small example if I show you a bigger example you could",
    "260-280": " think of the University website as a big giant graph of web pages citing or referring to each other via the use of hyperlinks right so we just represented the web as this network the question is how is the web organized the way people tried to approach organizing the web was to human",
    "280-300": " naturally curated by humans so for example Yahoo back in 1996 their original idea was to take all the web pages on the web and manually categorize them into a set of categories so for example here I have a screenshot of the web page and you see that the top category was for example Arts there was",
    "300-320": " business there was education and each of these categories had further subcategories so the idea was to take every web page and categorize it into into this giant hierarchy of course time showed that the web was growing far far too quickly so this did not scale so the next way how to organize the web and how to kind of find things on the web is",
    "320-340": " the web search and this is what kind of what we use today and what is interesting in terms of the web search is that there is Rich literature in particular the field of information retrieval that covers the problem of how do we find a document in a large set of documents right so in our case of the web we can think of every web page as a",
    "340-360": " document the whole the whole web is one giant Corpus of documents and our goal is to find a relevant document to a given query in this huge set however the traditionally the information retrieval field was interested in finding these documents in a relatively small small collections of trusted documents so for",
    "360-380": " example like newspaper collections or paper patent Collections and so on however the web is very different the the difference is first that the web is huge and the second thing is that web is full of untrusted documents random things spam unrelated things and so on so the",
    "380-400": " the big question on the web is which web pages on the web should we trust which web pages are kind of legitimate and which are which are fake and irrelevant and this is what we will be looking at today is how do we identify set of relevant or trustworthy web page web pages in this huge web graph so when you are doing the web search",
    "400-420": " there are two that there are two challenges right so as I mentioned the first challenge is who do we trust on the web right how do we know which are which web pages are legitimate and which web pages are for example spam or somehow fabricated on the web the idea here is that we will use the structure of the link web graph to understand uh",
    "420-440": " these things so the idea is kind of the trust trustworthy web pages will link to each other and we will build on this idea to exploit it to be able to identify the pagerank algorithm and then the other problem that happens on the web is that sometimes queries can be rather ambiguous for example you can ask what is the best answer to a query",
    "440-460": " newspaper and there is really kind of no no good answer to this query and the the goal here if we want to identify all the good newspapers on the web is to again look at the at the web structure of the of the structure of the web graph in order to identify the the set of pages",
    "460-480": " or a set of newspapers that are linking to each other and again get get the result out of the structure of the web graph so these are the two challenges we will address in today's lecture the way we can address both of these challenges is to basically realize that the web as a graph has very rich structure so one thing that we can do is",
    "480-500": " we can try to think of this problem abstractly as a way to rank nodes of a big graph so basically we would like to compute a score or an important score of every node in this web graph and the idea is that some nodes will collect lots of links so they will have high importance and some other nodes will have a small number of links or links",
    "500-520": " from untrusted sources so they will have low importance so that's the that's the thing we want to compute so in order to compute the importances of the nodes in a graph there are several approaches to this broadly these approaches are called link analysis because we are analyzing The Links of",
    "520-540": " the web graph to to compute an important score of a node in a graph so the first approach we will look at it's called pagerank and this is really the algorithm that was that was invented and that behind the initial implementation of the Google search engine then we will take a look at also at another algorithm that is called hubs and authorities here",
    "540-560": " the idea is that we have two types of web pages in a web graph we have the web pages that are called hubs and we have web pages that are kind of called authorities that are good authorities for given topics and then we will look at some extensions of these algorithms first in terms of topic specific or what is also called as personalized page rank",
    "560-580": " and we will also use these ideas and apply them to web spam spam detection where basically spammers may want to manipulate the structure of the web graph in such a way to to make some web pages to to seem important even though they are not so basically boost the importance of some of the web pages"
}