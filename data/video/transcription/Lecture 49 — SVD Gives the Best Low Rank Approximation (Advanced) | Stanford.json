{
    "0-20": " and what does SVD do is the following if you take our Matrix a represented exactly as a product of matrices U Sigma and V transpose before we said we can always do this but now we want to do dimensionality reduction which basically means with means we take U Sigma and V but we take part of U and cross it out",
    "20-40": " we take part of the sigma Matrix cross it out and we take part of the Matrix we transpose and cross it out so basically now we represented everything instead of using R columns or r o simply we represent it using k k rows in K Columns of v and U we obtain a new Matrix B now",
    "40-60": " that we multiply everything together and the idea is that b is the best approximation of a So What by best means is that the frobinius norm the difference between a and b is smallest possible right so um in in a case a and b are as close to",
    "60-80": " each other as possible given that we want to represent the data points with a small number of coordinates K so to be very precise here is here is what the theorem says the theorem says let's take our input Matrix a and do an SVD on it so it means we let's represent it as a product of matrices U",
    "80-100": " Sigma and V transpose where Sigma is the Matrix of um singular values that are sorted in the decreasing order then let's define this new Matrix B that is simply U times s times V transposed where what is important to note is now that Matrix S",
    "100-120": " is simply a diagonal matrix where the first K entries of s are simply the corresponding singular values from Matrix Sigma and the rest is zero so now if we if we take if we take this new product of soulmate I have the Matrix U I have the Matrix V transpose from before but now I basically took Sigma",
    "120-140": " and changed it into s then I obtained this new Matrix B and the idea is that um this Matrix B is the the best reconstruction of a in a sense of the forbidius norm so the way I can think of this is the following so I can think that b is simply a solution to the to",
    "140-160": " the question what is the best Matrix B that I can find that minimizes the forbidius norm where I want the rank of B to be equal to K so K is the number of Dimensions we allow ourselves the number of coordinates and what we want to find is the best B that most Faithfully represents the data represents the Matrix a and what what do we learn from",
    "160-180": " SVD is that if I'm given Matrix a represented as U Sigma V transpose then I want to do dimensionality reduction all I need to do is cross out a number of singular values cross out rows in a and cross out some row Cindy now I",
    "180-200": " multiply this matrices together and I'll get i'll get a new new Matrix that is as close to the original one as possible so the reason why doing this works is the following right so what secure value decomposition it's also called the Specter the composition of a matrix and all this says right is what we know so",
    "200-220": " far is that we take our original Matrix a and represent it as a long and thin Matrix U the diagonal matrix Sigma and the Matrix we transpose so the reason is y is kind of zeroing out the low singular values what is the best thing we can do and the way we can learn this or at least see why is that a good thing",
    "220-240": " to do is the following we basically say let's take our Matrix a that has n rows M columns and if you do the spectral the composition of it this basically means that we can represent it as a summation of K terms where we are taking Sigma 1 which is the first singular value the U1 the the first left singular Vector times",
    "240-260": " V1 transpose this is the first right singular vector and then you know plus the second singular value the second left singular Vector the second right singular vector and so on and so forth and as I said before we are assuming um that first we already know that singular value Sigma are greater than",
    "260-280": " zero and we are also assuming now that they are sorted so you know the question is why setting small sigmas to zero the right thing to do and the way we think about this is the following right we already know that vectors UI and VI their unit length right so because the the matrices u and v are column orthonormal which means that every Vector has a unit length so this",
    "280-300": " basically means that when I take a product of u and v and then multiply that with Sigma basically the sigma scales them right so now if I want to have small error I want to I can kind of drop vectors that have small importance that basically have small Sigma so this is why zeroing small sigmas actually works and produces the least possible",
    "300-320": " amount of error and this is kind of an informal argument we could also give a proof but this is the the intuition why doing dimensionality reduction in a way of setting small Sigma to zero actually works so of course now the question is how many dimensions to keep right so if I take my Matrix a I Do I Do It singular",
    "320-340": " value decomposition and I obtain at the end some R which is the rank of the Matrix um singular values and singular vectors and are in in kind of the worst case the r can be the the smallest of uh M and N right kind of the smallest of the number",
    "340-360": " of columns or rows of Matrix a then the question is how many how many when I'm doing dimensionality reduction how many dimensions K should I keep or how many singular values uh Sigma should I keep and the good rule of a thumb is that I should keep as many so that my the I preserve 80 to 90 percent of the energy and the way I can do this is the",
    "360-380": " following basically I Define the total energy to be the sum of the squares of the singular values and now what I want to do is I want to ensure that the sum over the singular values I to K Sigma I squared radius at a singular values I keep dividing this by the sum of singular values I up to r r these are",
    "380-400": " all the singular values I get I want this ratio to be to be around 0.8 or 0.9 and this is how many singular values I want to keep so that I still keep kind of 90 percent of the energy of course what we also have to talk",
    "400-420": " about is what is the computational complexity of SVD and to compute SVD there are standard linear algebra algebra tools and basically Computing SVD in some senses um cubic in the size of the data so it's the complexities n times M squared or n times M squared whichever of the two is",
    "420-440": " less right so in some sense it's it's quadratic in the in the in one dimension and linear in the in the other dimension so what are the conclusions so far so what we know so far is the following we know that we can take any Matrix a any real valued Matrix say represent it as a product of three three special matrices",
    "440-460": " U Sigma and we transpose we know that this um decomposition is unique we know that we can think of Matrix U as a user to concept Matrix we can think of U Matrix v as a movie to concept Matrix and we can think of the Matrix Sigma the singular values as the strength of each concept the other thing we we learned is",
    "460-480": " how do we do dimensionality reduction the way we do dimensionality reduction is by simply setting single uh the smallest singular values to zero which also means that we are kind of removing the corresponding left and right singular vectors to zero how many dimensions we we pick we pick the them",
    "480-500": " the enough singular values such that the total energy that 80 to 90 percent of all the energy is preserved energy defined as being the sum of the squared singular values and the other thing we we saw from the data is in that singular value is able basically to pick pick up or identify linear correlations in the",
    "500-520": " data so it's able to basically identify Dimensions along which the data is spread out the most"
}