{
    "0-20": " we have seen one use of locality-sensitive hashing where the underlying data is a collection of sets and similarity means Jaccard similarity but there are other approaches that share the same goal of focusing our search on the pairs that are likely to be similar we're now going to see some of these variations later we'll examine",
    "20-40": " general techniques and definitions of similarity other than Jaccard similarity and we'll see that LSH is really an approach to problems rather than than a particular algorithm our first example of using LSH concerns a problem called entity resolution and this kind of problem we're given a collection of records each record provides some",
    "40-60": " information about an entity typically entities are people but they could be companies physical locations events or any number of things the entity resolution problem is to determine which sets of records refer to the same person and to merge these records into one record that tells everything about that entity the problem is way more",
    "60-80": " complicated than it looks for a for example it is typical that records about people include the name of the person so it looks like it should be no problem at all to group them in two sets that represent the same individual but in a large collection of Records there will be people with the same name so grouping by name will merge records for different people and worse the same",
    "80-100": " person may have the name written in different ways in different records some records will have their middle initials no there's not a person's nickname who may appear in one place and the formal name in another like Sue and Susan and of course misspellings occur which makes names look different even if they are intended to be identical and we often",
    "100-120": " can compensate for these discrepancies by using other information in the records for example two records may have similar names but identical phone numbers or identical addresses that's when the problem becomes really interesting I'm going to tell you a real story of how I used LSH to get a big",
    "120-140": " consulting fee after I retired from Stanford I took a job consulting for some lawyers they were dealing with a lawsuit involving two companies that I'll call a and B okay Company B had a service and Company A agreed to use its customer base to find customers for Company B what the",
    "140-160": " companies took the squabbling and the deal was eventually cancelled since B was serving many of the customers that AIG had sent them a was owed fees with these customers and sued to get those fees unfortunately neither company had bothered to modify their records than",
    "160-180": " the Kate whether a customer had been part of this deal they could have created a record we sent this guy to B and B could have added a bit to their record saying this guy came from a but neither did so they had to pay me to figure out how many customers appeared in the databases of both companies to",
    "180-200": " set the scale of the problem each company had about a million records that might represent a customer that AIG had provided to B that's a tiny database by today's standards but notice that there are a trillion pairs of records one from a and one from B that might be the same person it is way too expensive to examine and evaluate a trillion pairs of",
    "200-220": " records each record from either company had a name address and phone number but often these were different even for the same person in addition to typos and the sorts of variations in name we discussed earlier there are many other sources of difference people would move and tell",
    "220-240": " one company their new address but not the other area codes would change even though the rest of your phone number remained the same people would get married and changed their name in all these cases one company might track the change and the other not so our first step was to devise a measure of how",
    "240-260": " similar records were we gave a hundred points each for identical names addresses and phone numbers so 300 was the top score interesting the only 7,000 pairs of records received this top score although we identified over a hundred and eighty thousand pairs that were very likely the same person then we penalize",
    "260-280": " differences in these three fields completely different names addresses or phones got zero score but small changes gave scores close to a hundred for example if the last names were the same but there was a small spelling difference in the first names like like this then the score for the name would",
    "280-300": " be ninety if the last names were the same but the first name is completely different the score for the names would be fifty we scored all candidate pairs of records and reported those pairs that were above a certain threshold as matches one of the subtle points is how",
    "300-320": " we set the threshold without knowing ground truth that is which pairs of Records really were created for the same individuals notice that this is not a job you can do with machine learning because there's no training set available and we'll talk about how we did this soon okay so as I mentioned we",
    "320-340": " can't afford to score all trillion pairs of records okay so I devised a really simple form of locality-sensitive hashing to focus on the likely matches here we used exactly three hash functions one had a bucket for each possible name the second had a bucket for each possible address and the third",
    "340-360": " had a bucket for each possible phone number now the candidate pairs with those placed in the same bucket by at least one of these hash functions that is a pair of records was a candidate pair if and only if they agreed exactly in at least one of the three fields did we lose some pairs surely we did",
    "360-380": " because there would be some pairs of records that had small differences in each of the three fields and these would never become candidates for scoring we actually did a hand sampling of records and estimate that there were about twenty five hundred pairs of records that we missed but that's not bad",
    "380-400": " compared with a hundred and eighty thousand that we found and finding those extra 2,500 would probably have cost more than they were worth to either company you may have been puzzled by my remark that we hashed to one bucket for each possible name since there are in principle an infinite number of possible names but we didn't really scratch two",
    "400-420": " buckets rather we sorted the records by name and then the records with identical names appear consecutively in the list and we can score each pair with identical names after that weary sorted by address and did the same thing with",
    "420-440": " records that had identical addresses and then finally we repeated the process by sorting by phone number we should that we should observe that another approach was to follow the strategy we use when we did LSH for signatures we could hash to say several million buckets and compare all pairs of records within one",
    "440-460": " bucket that would sometimes cause us to look at pairs of records with different names that happen to hash to the same bucket but if the number of buckets is much larger than the number of different names that actually appeared in the data then the probability of collisions like this is very low now remember that we scored each candidate pair of records",
    "460-480": " but suppose a pair gets a score like 200 out of 300 indicating a good deal of similarity but not perfect similarity do these records represent the same person well turns out a score of 200 made it very likely that the records represented the same person but how could we tell for sure we devised the way to calculate",
    "480-500": " the probability that records with a score X represented the same person and it's worth telling about because it can be used in other circumstances as well even though the data we used was very specific to the problem at hand first remember that there's a gold standard 7,000 pairs of identical",
    "500-520": " records that we could assume represented the same person for these pairs we looked at the creation dates at companies a and B it turns out that there was a 10 day lag on average between the time the record was created by company a and the time that the same",
    "520-540": " person went to Company B to be two in this service on the other hand in order to reduce further the pairs of Records we needed to score we only looked at pairs of records where the a record was created between between 0 and 90 days before the B record now if you take a random a record in a random B record whether a record happens to have",
    "540-560": " been created between 0 and 90 days before the B record you'll get an average delay of 45 days these records are almost certain to represent different people because they were chosen at random so let's look at a pool of matches say those will score 200 some will be valid matches and their average",
    "560-580": " difference in creation dates will be 10 others will be false matches and they will have an average difference in creation dates of 45 suppose that within this pool the average difference is X a little math tells you that the fraction of matches that are valid is 45 minus X",
    "580-600": " all divided by 35 so for example if x equals 10 then this fraction is 1 which makes sense since attendance the difference that the gold standard provides if x equals 20 then we would expect that 5/7 of the matches are valid that makes sense 5/7 of the matches will have an average",
    "600-620": " difference of 10 and 2/7 of them will have an average difference of 45 so the weighted average of the averages is is 20 so we tried to convince the lawyers that they should go into court with a claim of the fraction of each of the pools that had average delay is less",
    "620-640": " than 45 even though we couldn't tell which pairs in each pool were valid and which were not but the lord\u00eds toll is not to even try because no judge or jury would understand the argument but you understand it don't you well while we use the creation date field in records the idea generalizes to use any field",
    "640-660": " that was not involved in the locality-sensitive hashing all we need to know is that the value in this field will be closer when the records represent the same entity than when they represent different entities that should be the case almost always okay for a concrete example suppose records represent individuals and they have a height field we can assume that",
    "660-680": " if the records represent the same person the average difference in Heights will be zero or perhaps more precisely the difference will be the average measurement error which we can determine if we have some gold standard of records that we know represent the same person this different substitutes for the difference ten days in our example but",
    "680-700": " if to record represent different people then the average height difference will be the average difference for random people we can determine this difference by picking a relatively small number of pairs of records at random and determining the difference in Heights and those two records this difference plays the role of forty five in our example"
}