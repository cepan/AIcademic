{
    "0-20": " welcome back to mining of massive data sets in the previous um lecture we looked at collaborative filtering approaches for recommender systems in this uh section we're going to look at how to evaluate a recommend a system to make sure that it's doing a good job let's look at an example suppose we have a set of uh users and movies um",
    "20-40": " users on the y axis movies on the x-axis and here's utility Matrix um that gives um you know some where some of the rating values are known uh and some are unknown and ratings are on a scale from 1 through 5 now the common evaluation methodology",
    "40-60": " is to take a piece of the Matrix um and treat that as a test set now we know these ratings but we're going to withhold them from the algorithm that we're going to use so we'll call this the test data set or the withheld ratings um and we'll for the purposes of the um of the algorithm these are going",
    "60-80": " to be treated as the same as unknown ratings or the blank ratings but in fact we know what these ratings are so we can use our algorithm to predict these ratings and then compare them against the actual ratings and see how good the algorithm performed so the uh the the trick is to",
    "80-100": " compare the predictions against the withheld known ratings or the test Set uh T and the most common measure uh is uh is is a measure called the root mean square error or rmsse for short um and it's very simply defined um you know uh as you know as the um some of the",
    "100-120": " squares of the deviations uh from the actual ratings and the predicted ratings so in this case rxi star is the actual rating for user X and item I rxi is the rating that's predicted by by our algorithm uh we're going to take the difference of those two and square it and some of those squares across all the",
    "120-140": " withheld ratings and divide by the total number of withheld ratings which is n and just take the square root of that so this is the root mean square error or or rmsc and it's the most commonly used measure to evaluate a um a collaborative filtering system now while rmsc is a very very",
    "140-160": " simple measure it does have a few few problems and one of the one of the most common problems is that there this narrow Focus accuracy or rmsc sometimes misses the point of why we Implement uh recommend recommender system in the first place remember the goal of recommender system is not really to predict the uh the rating of a user for",
    "160-180": " a given item but to recommend items to a user that the that the user might buy or might view or might listen to um and therefore sometimes when you predict just the just the um items with the highest um scores or the highest similarity uh you might end up with problem s the first problem we run into",
    "180-200": " is a problem of prediction diversity which means that all the predictions are too similar to each other for example let's say the user liked the first Harry Potter movie now the set of uh most similar uh items to that or the set of items to the best scores might be the the other Harry Potter movies so all the predictions might therefore be Harry",
    "200-220": " Potter movies which is uh you know rather non- diverse set of recommendations and might not introduce the user to any new and interesting items the second problem is one of context now the the user might uh be uh the same user might be operating uh in",
    "220-240": " different context and might want different items in those in those contexts for example uh let's say a user is traveling to uh to to Patagonia uh he might end up shopping for lots of travel books on Patagonia then once he once he's back from Patagonia U this the set of his simar the set of recommended",
    "240-260": " items will include you know more books on Patagonia which the user is not interested in anymore because the user is in a very different context finally the order of predictions also turns out to be hugely important for example when there's a series of books or movies you want to predict you want to recommend uh items that are",
    "260-280": " earlier in the sequence U ahead of items that are later in the sequence note also that in practice we only care to predict high ratings right so uh We've sort of set up this whole problem of um predicting the uh rating of a user X for an item I but in",
    "280-300": " practice we don't really care uh what the rating of user X for item I is if that rating value is low and the user really doesn't like the item we only going to recommend items to the user that the user really cares about and so we only care to uh predict high ratings not to predict low ratings and the rmsc that we you've sort of defined the root",
    "300-320": " means square error actually doesn't distinguish between high ratings and low ratings um it only looks at the difference between the actual ratings and uh the predicted ratings so in fact uh we might come up with a method that works well in practice uh that recommends really good items to users but actually uh makes bad errors on items that the user doesn't really like",
    "320-340": " now this algorithm might work really well in practice but its root mean square error evaluation will look really bad so an alternative method that uh that actually captures this idea uh is to use a metric called Precision at top K so let's say the user um uh we look at",
    "340-360": " the withheld ratings of the user but only look at the U the highest uh K ratings in the withheld set for a given user and see whether our algorithm can predict a large fraction of those items so the position of top K is just a percentage of the predictions uh that",
    "360-380": " are in the user top K withheld ratings"
}