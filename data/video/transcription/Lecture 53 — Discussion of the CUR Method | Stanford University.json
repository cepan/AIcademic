{
    "0-20": " to demonstrate the um the pros and cons of the methods let me talk about this very quickly so why is CR method good first CR method is very easy to um to interpret right because the singular values and singular vectors right so the rows and Columns of C and R are simply the actual data points right they're actually rows and Columns of our dat",
    "20-40": " data Matrix um so it's very easy to interpret them because it's actual data another good point is that our XIs vectors right so again vectors in C and R so columns in C rows in R they are they are actual data points so if the data original data is sparse these vectors are also sparse um one drawback",
    "40-60": " though is that we have duplicate rows and columns in a sense that um rows that are very long will be sampled multiple times but this last last drawback is very easy to um to get around and I'll show you an example of how we can do it right so in some sense what is the difference between the two",
    "60-80": " the difference is that if we have a set of data points in a cloud like this a singular Vector will be kind of the best point to project but what a CR Vector will be you know will be some actual data point so it will be a bit off but it everything will still work so this is what we see from the CR decomposition um so right now the question is how do we",
    "80-100": " get rid of the duplicates right so the idea is very simple right the idea is kind of let's throw them away but then let's scale in some sense multiply the original row and column by the square root of the number of times it appeared in our original data right so if I have Matrix a i compute my CR decomposition",
    "100-120": " imagine that I have a certain column here sampled multiple times then I would only keep one copy of that column but I would have to multiply it with the square root of um D where the D is the number of times that column actually appeared in my data and I would do the same with r and I would then construct The Matrix uh U the same way as we",
    "120-140": " talked about so everything still works everything is great so how do CR and SVD compare to each other so in both cases we assumed that uh Matrix a is our given input data Matrix which is very big and let's think of it as sparse so it has lots of um nonzeros um in it and then",
    "140-160": " what the way we thought about um SVD was that we have matrices u and v they are big but Danse and Matrix uh Sigma is sparse and very small okay what is C CR also has a that is uh huge but sparse but what is now important is that C and",
    "160-180": " R are also big but sparse um and now Matrix U is dense but small so the difference is in in SVD Sigma is sparse but small in CR our kind of corresponding um Matrix U is dense but it's still very small so kind of it",
    "180-200": " doesn't matter but the big difference is that U and uh sorry C and R are sparse while in in SVD they are dense and just to give you an idea how much uh this makes a difference in practice let me tell you about a simple experiment okay so what we will do is we will take the dblp data set so dblp is a biographies",
    "200-220": " of all the computer scientists right so for every computer scientist there is a set of papers they published so what we can do is for example we can create this Matrix of author to conference Matrix we can think of it as a big thin Matrix where columns are different conferences and every row is a different user right so this is a different scientist and",
    "220-240": " every column is a different conference and then the entry simply tells me how many papers did a given author or a given scientist publish at a given conference so in terms of this Matrix we have around 400,000 authors 400,000 computer scientists and we have around 3500 100 different columns okay and this",
    "240-260": " Matrix is very sparse which means that most most computer scientists don't publish at most conferences right on out of these 3,000 conferences most computer scientists maybe publish at five or 10 or of them and the rest they they will never publish at right so what do we want to do now is we want to reduce the dimensionality of this data right so",
    "260-280": " what do we want to kind of what are the things we want to understand when we do this dimensionality reduction we want to understand how much time does it take to do the dimensionality reduction what is the Reconstruction error and how much space do we need to store to store the new uh the new data points and the new dimension the new uh basis vectors and",
    "280-300": " so on so let's look at the experiment so this is done from the paper by Sun and futas from 2007 and here are the two graphs right so the first one is the accuracy the accuracy simply means what is the Reconstruction error right so um one means there is no reconstruction error and zero means we have lots of reconstruction eror error right blue is",
    "300-320": " SVD and um the black and the red are CR CR with duplicates CR without duplicates and the X the y- axis here is the space ratio how much space do we need to store the the basis vectors and so on and what you see for example in this case because",
    "320-340": " our data is very sparse is that to achieve the same accuracy let's say to achieve accuracy of 02 um C needs much less storage than what SVD needs so in some sense SVD needs less vectors but these vectors are dense so we need to store every possible coordinate of them while CR IG vectors or singular vectors",
    "340-360": " if you think of them this way are sparse and we only need to store the nonzero ones right so for the same accuracy we need much less uh storage for CR and of course if you also think about the computational time here is the computational time that we need to achieve a given accuracy using SVD versus computational time to achieve",
    "360-380": " given accuracy even CR given CR and again what we see is that for for a fixed accuracy CR is much faster than um SVD of course what is um important here is that in in most cases the number of actual vectors or the dimensionality of the reduced space to which we are pro",
    "380-400": " proje pro projecting in SVD will be smaller than in CR but the amount of space and computation will will be much better um in CR so with this we are done with discussing the DI the topic of dimensionality reduction and the application where we will really need all these techniques in terms of the singular value decomposition and",
    "400-420": " thinking about matrices and representing them in low dimensional space and in terms of the concepts that appear in these matrices will be in terms of recommander systems where we will be given a big uh rating Matrix of users to movies and we will want to identify basically exploit the correlations the same way as we were doing in our movies to user example in order to identify",
    "420-440": " which movies to recommend to which user so this is what we will um look at in a couple of next lectures"
}