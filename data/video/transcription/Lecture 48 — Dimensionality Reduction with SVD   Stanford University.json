{
    "0-20": " so far we saw our singular value decomposition on this small users to movies example where what we saw was basically we took this original matrix and we were able to represent it as a product of three matrices and there we talked about the sci-fi concept the Romans concept and then there was this also the third column that we kind of",
    "20-40": " had very low strength right the third concept that had very small strengths that we kind of brushed under the rug and didn't really talk about so what I want to do now is actually talk about how do we do the dimensionality reduction in a sense how do we discover that our movie data really had only two real kind of strong concept and the last third concept was more like noise and it",
    "40-60": " was okay to to kind of remove it from our analysis and discussion so the idea is how do we what is SVD really doing and how do we think about it in terms of dimensionality reduction and what SVD is really trying to do it basically in some sense it gives us the best acess to project on so what do we mean by this is",
    "60-80": " that the the best means that the sum of the squared projection errors is minimized okay so in some sense we want we want a small set of axis such that we if we represent our data in terms of that of that axis we get the minimum reconstruction error and the simple example how to see this would be in this",
    "80-100": " two dimensional example where imagine every different axis is a separate movie and we have the users ranking movies and every point is now a user and the x position of this point is how much they rated use movie one and the Y position is how much the rated movie two and assume that our data lies in this kind",
    "100-120": " of in this kind of shape so then if we think of our feet and say okay we are only given one coordinate to be able to represent this data so not two coordinates but I only give you one coordinate what is the best axis along which you want to represent this data so for example in this case this would be the best axis and now every data point",
    "120-140": " we can represent as a single number which is simply the position or the projection of a given data point on on this line so for example the the data point the time just drawing the data point here would project to this line to this particular location and now my goal is that when I",
    "140-160": " now represent these two-dimensional points simply by the by their position on along my red line the sum of the squared errors of their locations so basically the distance between its true position and the in the position among the line should be minimal and this is exactly what SVD does",
    "160-180": " so what SVD does is finds the best vectors or axis on which to project the data such as the reconstruction error is minimized so let me give you an example of what do I mean by this and now for example also the question is given is this two dimensional data how do I discover the best the best axis on which to project and how do I really do dimensionality reduction how do I",
    "180-200": " discover the position of the point on this given line so the idea is the following we are given our matrix a and we want we represent it as a product of three matrices u Sigma V transpose where we think of V as a movie - concept matrix and you as a user - concept",
    "200-220": " matrix so what this means immediately that we see that V is a movie - concept matrix which means that for example if you want to ask what is this vector V 1 along which it is the best to represent the data that is simply the first row of our matrix V transposed right so this is exactly the vector that represents the",
    "220-240": " the axis of the highest variation so I I have still my our old example of users two movies represented and the SVD of this thing and now the question is how do we do dimensionality reduction so for example the way we do dimensionality reduction is that we can think of the",
    "240-260": " whole system the following that our first tried singular vector gives us the location axis on which we want to project and then the this corresponding singular value tells us the variance along that given dimension or the spread of the values along that given dimension so in our two dimensional case here the",
    "260-280": " values are really spread around the first singular vector they are spread around a bit the second singular vector a bit less and they are not really spread around the third singular vector so the strength of the imports importance of that vector is very small so now what we can think of we can think",
    "280-300": " of the locations the following so so far I talk to you what defines the axis now the next question is what defines the positions or coordinates of the points in this new space and the way we can think of that is that we simply take the matrix U and multiply it with C with Sigma right and this gives us the coordinates of the points in on this",
    "300-320": " projection axis right so basically how do they map down to our lines right so for example if I compute given my matrix a I compute the product of U times Sigma here is the here is now the position of every of every user in this new new space where for example the first vector",
    "320-340": " simply tells us what is the location of every user along along the first right singular vector and for example here you see that the values are very quite a lot along the first one they value quite a lot along the second one right the range is very high but for example the third one everything is kind of around zero or the variation on this along the third",
    "340-360": " column is much smaller which is why the third concept had had a very small weight so now given that we have our matrix a and again the singular value decomposition of it the question is how do we really do dimensionality reduction right so so far I just showed you how we",
    "360-380": " can represent the original data points in this new in this new projected space but the question is how do we really do dimensionality reduction and that turns out to be very simple all basically we need to do is we need to go and set a given set of singular values to 0 and basically take the smallest if we want to preserve K dimensions then basically we take R - kasing smallest singular",
    "380-400": " values and set them to 0 so for example in our case here if we want to do dimensionality reduction from this three dimensional space to a two dimensional space all we need to do is take this small the small singular value set it to 0 which in some sense means that now we also take the third column of you and the",
    "400-420": " third row of me transpose and set them to zero so the way we can do dimensionality reduction is to now take you the new Sigma and the new V transpose where we took the last singular value the last singular vector and the last right singular vector we",
    "420-440": " set all of those to 0 so if we would now for example go and take take the three new matrices that that are now smaller right they only have two columns in two rows and multiply this thing together here is the matrix we would obtain right so here's our original matrix a here is our new matrix a call it a prime order",
    "440-460": " let's give it a name B and what you notice is that a and B are very similar to each other so what do I mean by similar to each other is that if I take a given element for example this number 5 here and I compare it to this value here to the same axis in do the same cell in matrix B we see that the",
    "460-480": " difference is very small right or for example I can take this zero here correspond it compared it to the corresponding element in B and again I see that difference is very small right so what we basically did is we took in the previous slide we took our original matrix a representative did SVD and we",
    "480-500": " were able now to exactly reconstruct it now we actually took removed a few call last column from you and the last column from E and we removed the singular value now we represented everything as a as a as a smaller set of matrices so these matrices are now smaller if we multiply",
    "500-520": " the three matrices together now the new matrix we obtained was very similar to the original matrix a and when I say very similar we can quantify the similarity of two matrices using what is called the Frobenius norm right and the Frobenius norm of two matrices is simply the sum of the differences of their",
    "520-540": " entries right so if I say I have matrix a I have matrix B so their distance the Frobenius norm is simply I take a summation over all the entries I take the difference of the entry use some demo square that up some those entries together and take a square root and that's my distance"
}