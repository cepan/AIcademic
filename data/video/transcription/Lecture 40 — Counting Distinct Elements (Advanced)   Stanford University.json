{
    "0-20": " our next topic regarding the processing of streams is how we estimate the number of distinct elements in a stream without storing everything we have seen the technique is known as the flour land Martin algorithm will also see a generalization of the technique to computing moments of a stream which are essentially polynomials and the number",
    "20-40": " of occurrences of each of the elements that appear in the stream that sounds confusing but we'll make the definition clear before long so we have a stream of elements chosen from some Universal set of possible elements we'll assume that there are n elements in the set but n",
    "40-60": " might be extremely large for example the number of ipv6 addresses that could exist or the set of urls crawled by Google all we want to do is know how many different elements have appeared in the stream if not too many elements different elements have been seen we can",
    "60-80": " store all of them with an index like a hash table so we can tell whether an element arriving in the stream is new or has been seen before that requires a great deal of space if either the number of different elements is large or we are maintaining counts for a large number of streams in our case we can't maintain what we need in main memory and storing",
    "80-100": " the setters or sets on disk will slow us down too much here are some example applications for this problem we might be crawling a website and want to know how many different words appear at that site curiosity isn't a great motivation",
    "100-120": " but doing this count might tell us something about whether the site is an artificial site constructed by a spammer when the spammer has to create a great number of pages without doing much work they might use copies with the same page over and over which would lower the number of distinct words below what one would expect for the total size of the site or they might pick random pages",
    "120-140": " from the web which would mean the site has no coherent topic that would cause the number of distinct words to be unexpectedly large a major websites like to advertise the number of distinct users who have visited the site or used particular features of the site in the most recent",
    "140-160": " day week or month suppose now we are crawling the web we can't afford to follow links from every page we visit the job would never end so we have to cut off the search at some point there will be some pages we know exists because we found links to them but we",
    "160-180": " don't know what is on them or what links exist on those pages so which pages should we crawl and which should we not bother to crawl well heuristic essentially a simple approximation to the PageRank it's the count the number of end links to each page that we know about we then choose to crawl pages that have the highest number of in links this is an",
    "180-200": " example of an application where the sets we're counting a rather small most pages a few in links however there are very many pages that need to be counted so compressing the list of URLs with links to a given page is a good thing to do we're going to explore the case where",
    "200-220": " the set or sets we need to count are large enough that we cannot explain tane the sets explicitly in main memory we can't ever get exact counts if we don't store the entire set of elements seen so we'll look for the next best thing a way to estimate the count in a way that converges to the true answer as we allocate more space to estimating each",
    "220-240": " count the algorithm for estimating counts that we will cover is called the flajolet Marton algorithm after the inventors so to start let's pick a hash function that takes stream elements as its argument and returns a bit string whose length is sufficiently large that",
    "240-260": " there are more possible results of the hashing than there are elements that might appear in the set that is we need at least log n bits if there are n elements in the universal set if a is a possible stream element define our of a to be the length of the tail of",
    "260-280": " the hash value H of a that is the number of trailing zeros in the bit string H of a defined capital R to be the maximum value of R of a that we have seen in the stream so far that is cap R is the largest number of trailing zeros in the",
    "280-300": " hash function H applied to each of the elements we've seen in the stream the estimate of the number of distinct elements that we make from these calculations is 2 to the R this may seem ridiculous because our estimate is always a power of 2 and surely not every stream has a number of distinct elements that is a power of 2 but we're going to",
    "300-320": " use several hash functions and get several different values of capital R by combining them in the right way we can in principle get any count as our estimate of the number of distinct elements there is intuition why this idea it gives a good estimate the first notice that the number of times an element repeats in the stream has no",
    "320-340": " effect on the value of R because each time we hashed the same value we get the same tail of zeroes the value of R depends only on the number of distinct elements in the stream the probability that the hash value for any given element ends in i's eros goes down exponentially with i so when we increase R by 1 we need to double the number of",
    "340-360": " different elements that have a good chance of seeing I plus 1 zeros at the end of some hash value that is why raising 2 to the power equal to the longest tail of zeros is a reasonable estimate for how many different elements we've seen the next slide tries to make this idea formal first the probability",
    "360-380": " that the hash value of an element ends in iron more zeros is exactly two to the minus I that is for I equals 0 there is probability 1 that the tail has at least 0 zeros for I equals 1 there is half a chance that the last bit is 0 for I equals 2 the chance is 1/4 that the last",
    "380-400": " 2 bits are 0 and so on so suppose the number of distinct elements seen in the stream so far as M here is the formula for the probability that are the length of the longest tail is at least I to see why I remember two",
    "400-420": " to the minus I is the probability a tail has at least a zeros so 1 minus 2 to the minus I is the probability that a tail does not end in as many as I zero and if we have M independent hashing x' of different elements we raise this",
    "420-440": " probability to the M power to get the probability that no tail is as long as I finally 1 minus that is the probability that some element has a tail at least as long as I to see why - to the R is generally close to em look at the",
    "440-460": " formula for the probability that R is at least I we're only interested in the case where I is large so 2 to the minus I is tiny compared with 1 I claim that this is approximately this where E is",
    "460-480": " the base of natural logarithms the proof is similar to the one we just gave regarding the throwing of darts two targets and I'm not going to repeat it here so the first case is where two to the I is much greater than M we don't expect to find a tail as large as I among M hash values to see the math",
    "480-500": " start with the formula for the probability that R is at least I and again that's that's this since M times two to the minus I is small when two to the AI is much bigger",
    "500-520": " than M we can estimate this exponential again that by the first two terms of its Taylor expansion remember that e to the X is 1 plus X plus x squared over 2",
    "520-540": " factorial plus X cubed over 3 factorial and so on if X is much less than 1 only the first two terms are significant and that is what we have done here where we",
    "540-560": " replaced the exponential by the first two terms of the expansion but the ones cancel and the plus of course becomes the minus becomes a plus and that leaves",
    "560-580": " us with M over 2 to the I and since we assume two of the i's much bigger than m the conclusion is that there is very little probability that our the largest value of I that we found among the tail lengths is such that 2 to the our our estimate of M is much larger than M",
    "580-600": " itself on the other hand suppose 2 to the I is much smaller than M then M times 2 to the minus I is large and arrays to a large negative power that's this is small so the probability of",
    "600-620": " seeing a tail of length at least I is close to 1 our conclusion is that two to the R is almost always near em not much too big and not much too small unfortunately reasoning about the small",
    "620-640": " probability of an over or under estimate of em doesn't tell the whole story the fact is that the expected value of two to the R is infinite in principle although the fact that there is an upper limit on the length of a tail the number of bits in the hash value that means the expected value is not really infinite but some much too large number the",
    "640-660": " argument for infinite expectation is that we as we move from R to R plus 1 the probability of getting a tail that large halves but the value of 2 to the our doubles as a result each value of R up to the maximum impossible telling contributes the same amount to the",
    "660-680": " expectation to deal with the infinite expected value we need to use a large number of independent hash functions and thus get many samples of values for R we need to do that anyway since one value even if it were a good estimate would not be exact and only by combining many",
    "680-700": " estimates can we be reasonably sure we're close to the truth so we need to combine the samples of R that we get it's not obvious how we do that for example if we take the average then one unusually large value will distort the average too much and another option is to take the median the median lets us",
    "700-720": " ignore really large or small values but the trouble with medians is is that they are always one of the values in the set whose median you're taking and this set is a collection of powers of two so the result would always be a power of two",
    "720-740": " here's the way we combine averages and medians to get an estimate that is not biased to the high end and which will converge to the exact answer if we take enough samples that is we use enough different hash functions and compute the maximum tail length for each here's the way we combine averages and medians to get an estimate that is not biased to the high end and which will converge to",
    "740-760": " the exact answer if we take enough samples that is we use enough different hash functions and compute the maximum tail length for each we're going to partition the samples into small groups the group should be of size around log in at least log in where n is the size of the universal set then within a group",
    "760-780": " we take the average and among all the averages of the groups we take the median average and the result then will be an unbiased estimate of M the number of different elements in the stream I",
    "780-800": " want to move on to a generalization of the problem of counting distinct elements it's called estimating moments and the count of distinct elements will as we shall see turn out to be the 0th moment of the stream so as before let's",
    "800-820": " imagine we have a stream whose elements are chosen from a universal set of n elements and let the il iment occur M sub I times in the stream so far then the caithe moment of the stream is the sum of the K powers of all the M sub i's",
    "820-840": " here are the first three moments of a stream and the meanings the 0th moment is the sum of each M sub I raised to the 0th power the 0th power of anything except 0 is 1 so we are actually counting the number of distinct elements that have appeared so far on the stream that is the 0th moment is the problem",
    "840-860": " that the flajolet Martin algorithm solves the first moment is the sum of the M sub i's that is the sum of the count of the number of occurrences of all the elements that's just the length of the stream we can count the length of the stream with a single counter that we increment once per input this is easy",
    "860-880": " much easier than the other moments and no estimation is needed the second moment that is the sum of the squares of the M sub i's is sometimes referred to as the surprise number and it gives a measure of how uneven the distribution of elements in the stream is so here's",
    "880-900": " an example of computing the second moment and what the surprise is all about well suppose that a hundred elements have arrived so far on the stream and that these elements are divided among 1111 different values what would be unsurprising is if they all appeared approximately the same number of times the best we can do in that",
    "900-920": " regard is to have one appear ten times and the others nine times each the sum of the squares of these counts is ten squared plus ten times 9 squared which is a hundred plus ten times 81 and is",
    "920-940": " equal to nine hundred and ten that would be the lowest possible surprise number for the stream with this number of different elements now what would be really surprising is if one of the eleven numbers appeared 90 times and the other tendon appeared once",
    "940-960": " each the sum of the squares with accounts in this case would be 90 squared plus 10 times 1 squared which is 80 100 plus 10 or 81 10 that is the",
    "960-980": " largest possible surprise number in this situation I'm now going to introduce a technique due to Alan matheus and zaga D for estimating a moment of a stream it works to compute any moment but we'll",
    "980-1000": " talk only about the second moment the method involves keeping track of the value of many different random variables X as the stream grows each random variable is analogous to recording the maximum number of zeros in the tail using a fixed hash function like we did for the fletcher lynn martin algorithm and that's for the fledgling martin",
    "1000-1020": " algorithm each random variable requires storage of an integer preferably in main memory so we're limited in how many variables we can compute for each stream so let's see how we manage one random variable we can manage as many as we can afford in the same way of course using",
    "1020-1040": " different random numbers for each so let n be the length of the stream scene so far and it's going to grow as time goes on surely but right now it has some particular value for the random variable X we need to pick a random place in the stream to start so that any starting",
    "1040-1060": " point is equally likely to be chosen this choice introduces the randomness if we have many random variables that will be independent because for each we choose a random starting point independently of the others so let a be the element found at the chosen starting point the value of random variable X is",
    "1060-1080": " in the current stream length times twice the number of occurrences of a we find in the stream since the randomly chosen starting point and then minus one notice that a Shirley occurs at the time chosen but may occur many times after that occurrence is before the chosen time do not count an important point is",
    "1080-1100": " that even though X is defined this way we do not have to change the value of X each time n increases by one we store in separately and it can be used to compute the value of each of the random variables if we need it what we actually store for X is the element a and the count of occurrences of a since the",
    "1100-1120": " randomly chosen starting point this way when an input arrives at the stream we can leave almost all the variables unchanged we only have to change those for which the element being counted at the is the element that just arrived on this slide we're going to argue that the",
    "1120-1140": " expected value of a variable considering all the possible starting times exactly equals the second moment of the stream first remember that the second moment is the sum over all elements a of the square of the number of times a occurs now here's the formula for the expected",
    "1140-1160": " value of variable X there are n possible starting points each equally likely will average the value of x that is computed for each of these starting times the 1 over N is for taking the average and everything else is the sum over all",
    "1160-1180": " possible times T of the value 2 that is computed when time T is chosen remember that when T is chosen as the start time the value of x is in this times twice the number of occurrences that of that same element in the stream from then on",
    "1180-1200": " and then Maya finally minus 1 here we have rewritten the formula for the expected value of x by grouping all the x from 1 to n according to the symbol a that is found there so we can sum over all symbols a the 1 over N and",
    "1200-1220": " the N our constants as far as the summation is concerned so we carry them over and guess what they cancelled now",
    "1220-1240": " the term for a given symbol a involves several different times T in the stream each time will give a different value for twice the number of a is minus 1 the first term 1 represents the time when the last day arrives then the count will be one that is twice the count is 2 and then minus 1 leaves us with 1 the 3",
    "1240-1260": " represents the next two last time that a occurs and then the count will be 2 double it to make 4 and subtract subtract 1 to leave 3 we continue like that for each possible time and a appears and we get all the odd integers in turn finally the largest count we can",
    "1260-1280": " get is when the time T is the first time a appears then the count will be M sub a the full number of times a appears we double it and subtract 1 you can show that the sum of all the odd integers up",
    "1280-1300": " to 2 M sub I minus 1 is M sub I squared it's an easy induction and I'm not going to do it here but for example if M sub a is 4 then 1 plus 3 plus 5 plus 7 equals",
    "1300-1320": " 16 which of course is 4 squared as I mentioned we want not only the correct expected value for a variable we want to know that as you use more and more variables the average of their estimate of the moment will converge to the true value I'm just going to tell you that's",
    "1320-1340": " the case you combine them as for flagellate martin estimates group into small groups take the average of the groups and then the median of the averages there's a small problem we need to fix though we treat it as a constant but in fact the stream is always growing",
    "1340-1360": " and n is therefore a variable so one consequence of and being a variable is easy to fix in fact we mentioned it before we store in once and increment that each time a new element arrives in the variable X we store only the count if we ever need the value of x we double the count subtract",
    "1360-1380": " one and then multiply the result by the current value of n however the tricky part is how we manage to keep the fixed number of variables representing random choices of positions with each position from 1 to n equally likely even as n grows that is if we're keeping K random",
    "1380-1400": " variables then whatever n is we want each starting time to have been selected with probability K over N so here's how we make sure that at all times each of the end positions is chosen with probability K over N the technique is called reservoir sampling by the way to",
    "1400-1420": " get started each of the first K positions in the stream has chosen that makes sense because K over N is K over K or 1 before we reach the caithe position we're not really sampling since the best we can do is to choose each position with certainty but now n is bigger than",
    "1420-1440": " K so not every position can be chosen so suppose the nth element arrives prior to this there were n minus 1 positions in the stream and each was chosen with equal probability and that probability is K over N minus 1 the nth element arrives we know the enthis to be chosen",
    "1440-1460": " with probability K over N so let's generate a random number and do that for the recently just arrived position if the decision is not the choose position in then no changes are made to our selection of K positions but if you decide to pick position and then select",
    "1460-1480": " one of the current K positions at random and toss it from the set of positions we know the enthalpy of arenda being chosen but how about the first n minus 1 positions their probability can be calculated as shown there are 2 cases",
    "1480-1500": " either the nth position was chosen or not if it is not chosen it is not chosen then with probability n minus K over n each of the first n minus 1 positions was previously chosen with probability K",
    "1500-1520": " over N minus 1 in the case where the enth position is not chosen if some previous position had been chosen then it will still be chosen so we multiply by this factor but there is another term we have to add to the probability is the product of three factors first is the",
    "1520-1540": " factor K over and representing the probability that the nth position is chosen now in order for one of the first n minus 1 positions to remain chosen it must have been chosen previously that happens with probability K over N minus 1 as we mentioned and it must not be",
    "1540-1560": " thrown out it will not be thrown out with probability K minus 1 over K now I'll let you do the math but the expression does indeed simplify the K over N so all n positions now have exactly the same probability of being chosen"
}