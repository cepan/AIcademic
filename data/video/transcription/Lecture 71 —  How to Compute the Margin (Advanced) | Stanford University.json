{
    "0-20": " having defined the support Vector machines optimization problem now we want to solve it so what does this mean we want to find the vector W and the bias parameter B so here is our optimization problem right as as we said before we want to find this W and B such that the length of w is small and also",
    "20-40": " the sum of the slack penalties is small and the slack um penalty parameter C is trading off between the length of the margin and the misclassifications we are making so our goal is to find B andw basically solve the above optimization problem a standard way how to do this would be to say use a solver a solver is",
    "40-60": " nothing else than a mathematical package or a piece of mathematical Sol uh software where we are writing down the the structure of our optimization problem and the uh the software is able to solve it for us um if we would look at our optimization problem um the the",
    "60-80": " the optimization problem requires a quadratic solver why quadratic solver because here uh W is Multiplied with with with itself which means this is a quadratic objective function so we would have to find have a software that solves quadratic um optimization problems and this software is called quadratic soft",
    "80-100": " quadratic solware the problem of course is that these solers are are generally um slow because they are meant to be to be built for General quadratic problems while given that this is a support Vector machine we can use the structure of the problem to come up with a much better solution so the idea is that",
    "100-120": " solvers that are kind of off the shelf software are very inefficient for large scale data and we have to build a custom solution that will optimize our svm optimization problem so the question is how do we do that so our goal is to estimate W and B and the way we will do this is to we will come up with an",
    "120-140": " alternative approach imagine that we want to minimize this function f of B and W okay so there is some abstract function that has two parameters and we want to find this the value of these two parameters B and W such that the function is minimized what is the what is the function the function is very simple this is simply the the one half",
    "140-160": " sum over all the coordinates over the all the dimensions of the um squares of the values of w so this is the first part the margin maximization part of our optimization problem and then we have the second part which is this empirical loss part where we say plus the slack panel TC um times the sum over all our",
    "160-180": " training examples right 1 to n and then the value of the slack penalty which is um zero if we classify correctly or one minus our um distance from the boundary so this is the the optimization problem right so we want to find W and B that minimize our function f so what we",
    "180-200": " notice now is that this is a very very nice function and one ways to optimize nice smooth functions is to is to compute their gradient right so one way to optimize convex functions and our our function in fact is convex is to use what is called gradient descent which basically means is that we given given a",
    "200-220": " function now of some abstract parameters Z right in our case z is W comma B um what the idea is the following if I have Z and on the y- axis I plot f for Z all that we want will want to do is we will want to start at some uh point we will",
    "220-240": " want to compute the derivative or the gradient of that function at a given data point and then we will want to make a small step in the direction of the gradient that will give us to the new value we will re reevaluate the value of the gradient there and again make a small step in the direction reverse to the gradient and we if we keep iterating",
    "240-260": " this eventually we will get we will minimize the function and get into the value of the function so we will use the same idea and the same intuition to optimize our svm optimization problem um and the function f so our goal as we said is to minimize function f so what we want to do is",
    "260-280": " first we will want to compute the the gradient or the derivative of our function uh F and the way we will describe the the deriv is with this kind of upside down triangle right so our idea is that we want to compute the gradient with respect to WJ and we will label the gradient um um as this kind of",
    "280-300": " triangle um and then which coordinate or which dimension do we want to compute the gradient um with respect to so the idea is gradient of coordinate J is simply our F of B uh taken gradient with respect to variable WG WJ if we compute the gradient",
    "300-320": " um all other W's um are just constants and and and and their der and their derivatives are zero so the only the only thing that survives is WJ because we have WJ squ derivative of that is two WJ then conveniently conveniently we have this one half here which cancels the two and all that is left is WJ and",
    "320-340": " then if we also take the the we also have to take the gradient of the of the empiric loss so what we are saying here is this is the slack penalty times the sum of all the data points um the empirical the gradient of the empirical loss evaluated for",
    "340-360": " WJ so how do we compute the derivative of the empirical gradient so if um our classification is correct if if which means we correctly classified the training example then the the value of the derivative is zero and if we misclassified then this is simply the J",
    "360-380": " coordinate of training example I times the value or the class of that training example so what this now means is that we we were able to compute very simply using basically high school math what is the gradient of our objective function for a given Dimension or coordinate",
    "380-400": " J so now that we have the gradient we want to use our what is called the gradient descent method which is a very simple method we will start um at some at some location um X and all we are doing then is to say let's evaluate the gradient uh for a",
    "400-420": " given for a given Dimension so we will iterate over all the dimensions we will evaluate the gradient at that given Dimension by simply um going through all the all the data compute the sums introduced on the previous slide and then we are updating that coordinate of our Vector W uh in the direction of the is reverse to the gradient one thing",
    "420-440": " that we have to keep in mind is that we have this parameter AA which is what is called the learning rate parameter which basically tell us how big step are we going to make um basically when we compute the gradient how much are we going to move in the direction that is the opposite of the gradient um one",
    "440-460": " thing to note that in this case and this is a big problem is that Computing the gradients takes linear time right so Computing the gradient takes linear time why is that because here we have to go over all the data all the training data to compute the value of the gradient so this means if we want to have to do let's say 100 steps of the grade in",
    "460-480": " descent we have to scan over our our our training data 100 times which means that this this will be this method will be super slow when we have large amounts of data and the question is can we do this faster can we kind of still do gradi and descent while while not needing to Traverse over the whole day data set to",
    "480-500": " to do one step of the gradient descent so our next uh question will be how how do we speed up this method and in order to speed this method we will use what is called stochastic gradient descent right so before we were just talking about gradient descent or what is also known as badge gradient descent now we will use the idea of stochastic gradient",
    "500-520": " descent and the idea here will be that kind of instead of evaluating the gradient over all the examples in our data set we will we simply evaluate the gradient at each individual example so the idea is that the that the gradient for for coordinate or Dimension J and training example I is",
    "520-540": " simply the WJ plus C * the the gradient for that given training training example I so how this changes our optimization problem is that now we have two for Loops where the first Loop goes over all the data second one goes over all the",
    "540-560": " coordinates and we simply say let's evaluate the gradient at coordinate J for a given data point and then let's move in the core in the direction of the gradient at that individual data point so what does this mean is that our um calculation of gradient is now very fast right we just are evaluating gradient at",
    "560-580": " a single data point and we are making a step but because this is this is this takes very little time we'll be able to make many many uh small steps and the method that that that does this is called stochastic gradient what's the difference between stochastic gradient descent and gradient descent",
    "580-600": " the distance the difference is in in a sense that gradient gradient decent computes the exact version of the gradient kind of what is the what is the the direction in which we move towards the valy stochastic gradient descent we can think of it as a noisy version of gradient descent right so one one potential idea would be imagine that we want to minimize this um",
    "600-620": " three-dimensional function where um I have the dimensions X and Y and the dimension Z is kind of inside the slide and I want to converge here to the bottom of this of this value if I were to use cor gradient descent at every step I would move closer along this red",
    "620-640": " line however if I would be using stochastic gradient descent sometimes my gradient will be very noisy because I'm evaluating it using just one TR data point so I may my my optimization might kind of dance around but eventually I hope I will reach the the minimum so while using gradient descent we are kind",
    "640-660": " of guaranteed that at every step as we are making more and more iterations the value of the objective function in our case this would be w b is steadily decreasing using stochastic gradient descent um we the whole thing is much noisier however the benefit of stochastic gradient desent is that",
    "660-680": " Computing it is much easier so we will be able to do many many many more steps than what stochastic what gradient descent is able to do in the same time period so now let me give you a quick example of how stochastic gradient descent support Vector Machine Works in practice um and this is an example um",
    "680-700": " from uh from real life where the idea is that we want to use what is known as reers reuter Corpus volume one so this is a a a set of uh articles from writers where every article is categorized into a different topic so the idea is what we want to do is we want to predict a",
    "700-720": " category of every newswire article right whether it talks about sport politics international news and so on so in this data set we have 7 more than 700,000 training examples or documents and what we will do is we will represent every of these training thousand um 800,000 training examples with a 50,000",
    "720-740": " dimensional Vector which basically means that we will have one feature or one dimension per word we will remove words that are um very common we will also remove words that are very rare um and we will keep the rest of the words um we want to use this 700,000 documents to figure out what is",
    "740-760": " a good value of w andb and then we will want to evaluate um our predictions on a small test set of 23,000 documents so there are three interesting questions that we may want to ask first question is does stochastic gradient descent really minimize our objective",
    "760-780": " function right so how successful is stochastic gradient descent as a method for finding the minimum of our objective function um f of WB second thing is how long right how much processor time how much wallclock time does it take for the stochastic gradient to find the minimum of the function and and then of course the question is after we find this W how",
    "780-800": " well how well does this value of w generalize to the Future data right what is the error on the training data set so to answer the first question how how um successful and how long does it take um sastic gradient descent to find the value the minimum value of uh F so",
    "800-820": " standard support Vector machine would for example take 23,000 seconds uh fast version of support vector machine would take 66 seconds while for example the the version of support Vector machine that that we introduced the stochastic gradient descend based um svm would take 1.4 seconds to find um W and B that",
    "820-840": " minimize F so that's the S the answer to the second question how long does it take answer to the first question is what is the the value of the objective function right how good of a solution do we find and for example what you find that the values of the solutions are",
    "840-860": " pretty much the same so all all of the three different variants of support Vector machines are able to find the same value of the of the objective functions they were all equally successful in minimizing the the function f and then what is the error on the training set all of in all these cases the error is about the same so what do we conclude on this we conclude",
    "860-880": " that support Vector machine using stochastic gradient descent took only 1.4 seconds so it was like more than uh 10 times faster than any any other method while at the end achieving um the same performance"
}