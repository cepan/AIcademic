{
    "0-20": " we're now going to study carefully the matter of matrix multiplication we'll talk only about multiplication of square matrices although the best way to do multiplication of matrices of any rectangular shape follows the pattern we'll discuss and makes a good exercise to test your understanding well first apply the theory we develop",
    "20-40": " to give the best matrix multiplication algorithm that uses a single MapReduce job however we'll go beyond this theory to explore an algorithm for using a sequence of two MapReduce jobs to multiply matrices and this algorithm although more complicated turns out to be superior to the one job method whenever communication is the significant cost I would not like to",
    "40-60": " talk about a very common problem matrix multiplication now we best use MapReduce for this problem I'm going to cover both upper bounds the best algorithms and lower bounds of proof using the mapping scheme the idea that the family of algorithms I talked about is essentially",
    "60-80": " the best possible I'm going to talk only about the problem of multiplying square matrices but the same ideas apply from rectangular matrices as well and even to an extent to the extreme case of matrix vector multiplication refer to the two matrices being multiplied as a and B and",
    "80-100": " their result is C we need a way to refer to the element of row I and column J of matrix a and I'll use a sub I for that and likewise I'll use B with row and column subscripts to refer to the element of matrix B in that row and column and C sub scripted the same way",
    "100-120": " for elements of matrix C here's the definition of matrix multiplication see I K is the sum over all J from 1 to N of the product of a IJ and B JK we actually discussed the relationship between inputs and outputs for matrix multiplication earlier as an example",
    "120-140": " where an output requires lots of inputs in particular the inputs for the problem are the two N squared elements a I J and B JK and the outputs are all the cia's and CI k depends on all the inputs a IJ and bj k for any j from 1 to N that is",
    "140-160": " CI K depends on the 2 n inputs those in rows I of a and column K of B here's a picture of how you compute CI k the green point is computed from an entire row of a that's the blue row and an",
    "160-180": " entire column of B the yellow column there is something special about how inputs relate to outputs for matrix multiplication we can exploit it in the following way if one reducer covers both outputs eik and sees a FG then we can swap the rows of the two outputs and",
    "180-200": " deduce that this reducer also covers CI g and c FK the reason is that in order to cover CI k this reducer must have among its inputs the entire row of a and then the entire column K of B likewise",
    "200-220": " since it covers CFG the reducer must have all the row F of a and column G of B from row I and column G it has all it needs to compute C IG and from row F and column K it has all it needs to compute C F K we can apply this reasoning to any",
    "220-240": " outputs one reducer covers if a reducer covers any output in row I of matrix C and it also covers any output in column K of C then it has what it needs to compute CI k as a result we can find for any reducer a set of rows and a set of columns of C such that the reducer",
    "240-260": " covers every output that is in the intersection of these rows and columns here's a suggestion of what one reducer might cover it has these four rows of a as inputs and it also has these seven",
    "260-280": " columns of B as inputs and as a result can cover the twenty eight outputs indicated by the red dots in order to get a lower bound on replication rate as a function of reducer size Q we need an upper bound on the number of outputs that one reducer with Q inputs can cover first of all there's no point in",
    "280-300": " providing a reducer with only part of a row or column so we can assume that to maximize the number of covered outputs we choose some number of rows of the first input matrix a and some number of columns of the second Matrix B if the total number of matrix elements input to a reducer is Q then there can be only Q",
    "300-320": " over N input rows or columns it's an easy calculus application to prove that if the sum of rows and columns is fixed we maximize the number of covered outputs by choosing an equal number of rows and columns that as we cover a square of the output matrix so how many outputs can we cover we choose any Q",
    "320-340": " over to n rows of the first matrix in any Q over to n columns of the second Matrix and together they yield a square of side Q over 2n or Q squared over 4 n squared outputs we use the upper bound on outputs to get a lower bound on the",
    "340-360": " replication rate we need to cover N squared outputs since that is the number of elements of the result matrix we just argued on the previous slide that if Q is the number of inputs and reducers is allowed to have then each reducer covers at most q squared over 4 n outputs divide this by this",
    "360-380": " and you get four and forth over Q squared which is the number of reducers that are necessary now we note that the",
    "380-400": " total number of inputs summed over all the reducers is Q / / reducer times this as the number of reducers or this is for n to the fourth divided by Q we also know that the total number of inputs is",
    "400-420": " 2 and squared N squared for each of the two argument matrices when we divide this by 2n squared we get the replication rate over here in the",
    "420-440": " extreme case where Q equals 2n squared and one reducer can take the entire to input matrices and compute the product in the usual serial way no replication of inputs is necessary and R equals 1 at the other extreme we can use a separate",
    "440-460": " reducer for each output this reducer needs one row in one column that is Q equals 2 n is the smallest possible Q for which the product can be computed as a single MapReduce job thus every row of the first matrix must be sent to n reducers one to match it against each of the N columns of the second matrix and",
    "460-480": " similarly for the columns of the second Matrix each must be sent to n different reducers thus the replication rate in this case is in each of these two examples matches the 2n squared over Q lower bound there is in fact a MapReduce way of performing matrix multiplication",
    "480-500": " using the minimum possible replication for any Q within the range 2n to 2n squared we do have to assume Q divides 2n squared however in order for the bound to be exact we divide the rows with the first argument matrix into g groups of n over G rows and we do the same with the columns of the second Matrix and for each pair of",
    "500-520": " groups one from each matrix we have a reducer that produces all the outputs that can be computed from these groups that is there are G squared pairs of groups and Q is the number of elements in n over G rows and n over G columns that is 2n squared over G the",
    "520-540": " replication rate is G since each element must be sent to G reducers one for each group that needs to be matched against the group of rows or columns we solve for G in terms of Q and we get R equals",
    "540-560": " to N squared over Q here is a picture of what one reducer does it gets a group of rows the blue and a group of columns the yellow with these rows and columns it is capable of computing a little square of the output matrix shown in green to",
    "560-580": " compute the entire output you need G squared reducers each one covering one of the little squares of the output and needing an entire group of rows and a group of columns as input and now want to talk about a different approach to doing matrix multiplication",
    "580-600": " by MapReduce it turns out to be at least as good as the algorithm we just discussed as far as communication cost is concerned but it uses not one MapReduce job but two that is what we discussed so far when we divide rows and columns into groups is the best one job way to multiply matrices but if we use",
    "600-620": " two MapReduce jobs and sequence we can do better for the first MapReduce job we divide both input matrices into small rectangles a reducer gets the elements in two of these rectangles one from each input matrix no reducer can produce any",
    "620-640": " output completely but it contribute to a square of outputs the second job simply sums the contributions to each output from the various reducers of the first job thereby producing the correct output elements here's what we see a typical reducer of",
    "640-660": " the fear first job does it gets an input that is a rectangle of the first matrix this rectangle is twice as high as it is wide that turns out to be the way to minimize the total communication used by the two jobs together we won't prove this but it is indeed the case that cap",
    "660-680": " IB the set of row indices defining the blue rectangle and cap J the set of column indices now look at the yellow yellow rectangle in the second Matrix B where only you know we only need a reducer for this pair of rectangles if the rows of the yellow rectangle J",
    "680-700": " have the same indices as the column indices of the blue rectangle and we've called the set of indices J the rectangles of the second matrix are twice as wide as they are high well use K is the indices of the second of the",
    "700-720": " set of columns for the yellow rectangle and when we multiply the blue and yellow rectangles we contribute to the outputs in the green square of the result matrix C the Green Square has cap I as it said of row indices and cap K as it said of column in the season that is what you get for the element in row I and column",
    "720-740": " K of the result matrix C is the sum over all little J in the set of common indices of the two rectangles that is the set cap J of the product of a IJ and b JK that's not enough to get the entire",
    "740-760": " sum for the output element CI k but if we divide the columns of the first matrix and the rows of the second matrix into groups",
    "760-780": " then there will be a lot of blue rectangles with the same set of rows I and there'll be a lot of corresponding",
    "780-800": " yellow rectangles each rectangle rectangle pairs with the corresponding yellow rectangle like this and they each contribute to the same green rectangle",
    "800-820": " so all the second MapReduce job does then is to some corresponding contributions to the output cik from the different set of indices cap J now I want to describe the first job in more detail begin by picking a number G the",
    "820-840": " groups will divide both rows and columns into groups but the groups will have slightly different sizes I'm going to assume that 2 G divides in and will divide the first matrix a into rectangles that are in over G rows by n over 2 G columns like the blue rectangle",
    "840-860": " on the previous slide that is the rows of a are divided into G groups and the columns into 2 G groups will do something similar but slightly different for the second matrix B here the rectangles consist of n over 2 G rows and in over G columns of B like the",
    "860-880": " yellow rectangle in the previous slide that is B's roads or rows are divided into two G groups and as columns are divided into D G groups we don't have to constrain which rows are caught our columns go into which groups with one exception the groups of columns of a must each have a set of indices that is the same set as the set of indices for",
    "880-900": " some group of rows of B that condition will be satisfied if we do the obvious partitioning in two groups put the first n over 2 G columns of a into the first group the next n over 2 G columns into the second group and so on then do the same thing with the rows of B I'll also assume we do the obvious partition for",
    "900-920": " the rows of a and the call b-but there we do not we use groups of size n over G rather than size n over to gene we want to reduce it for each pair of rectangles one from a and one from B as long as they share the same set of",
    "920-940": " indices cap J for the columns of a and the rows of B we need a name for the reducer and I'll use ijk you can think of I as either the set of rows of a involved in the first rectangle or as the name for that group of rows and similarly K is the set of columns of B",
    "940-960": " involved in the second rectangle and J is both the set of indices for the columns of the first rectangle and the set of rows for the second rectangle again remember that the only time we have to multiply two rectangles is when they share the set of indices cap J other pairs of rectangles have no intersection between the columns of the",
    "960-980": " first rows and the rows of the second and therefore their elements are never multiplied together to get any output element here's the picture of the two rectangles multiplying to contribute to a square of the output elements the dimension of",
    "980-1000": " each rectangle in terms of the parameter G is shown it is important to remember that when we multiply the blue and yellow rectangles we don't get the complete matrix product in the green area we get a contribution to those output elements and there are two G different values of the set of indices cap J that can be paired with the same",
    "1000-1020": " set I in the same set K each of these triples correspond to one reducer at each of these reducers contributes to the output elements in the same green area the function of the second MapReduce job as we have said is to sum these contributions to get the correct value for each output element now I want",
    "1020-1040": " to lay out what the mappers and reducers actually do in the first job before I do I want to use a convention lowercase IJ and K will represent individual row and column indices the lie is a row of the first matrix a little J is both a column of a and a row of the second Matrix B",
    "1040-1060": " and a little K is a column of B then I want to use the corresponding capital letters to represent the groups into which these rows or columns fall that is group of rows capital I is always the group of A's rose to which the row little I belongs and so on the mappers",
    "1060-1080": " will handle elements of both matrices a and B let's see what it does when given an element a IJ first it will generate G key value pairs the keys will be capital IJ K for any group capital K but capital I and J are the particular groups into",
    "1080-1100": " row which row little I and column little drive along the value will have enough information for the reducer to figure out what is going on it needs to know a that is that this is an element of input matrix a rather than B it needs to know I and J and so it knows where the L of",
    "1100-1120": " this element fits into the matrix a and it needs to know the value a IJ for elements of B the mapper does something very similar it also generates G key value pairs but now groups J and K are fixed and only group I can vary over the",
    "1120-1140": " G possible values for that group and the value tells the reducer this is an element of B it's located at position I and J and that its value is b JK producer for key ijk takes all the elements that is received and sorts them by the row little I've a and the column",
    "1140-1160": " little K of B that row column pair gives us the output element to which the contribution is to be made then sums over all little J the product of a IJ times b JK will call the result X sub IJ K notice that I and K are row and column",
    "1160-1180": " respectively while capital J is a group and job two is really simple the mappers take each X IJ K and turn it into one key value pair the key is I K and that",
    "1180-1200": " is the index of the output element that X IJ K contributes to and the value is X IJ K itself and what do the reducers do the reducer for I and K simply sums all the associated values these are the contributions to the output element CI k I want to talk about whether using one",
    "1200-1220": " MapReduce job or two is likely to give better performance for matrix multiplication the answer is a little bit of a surprise at least it was to me first of all let's talk about computation time we'll look at communication cost after that I claim that that the same arithmetic is done by either method you need to multiply each",
    "1220-1240": " a IJ with each B JK in both methods do each of these multiplications exactly once and you need to sum the N terms that give you each output L and both methods do these sums wants although they group the terms in different orders now there's a general bias against using more than one MapReduce job if one will do because",
    "1240-1260": " setting up a job requires some overhead and also one often has to wait for the last task associated with a job to finish so having two jobs forces us to wait twice rather than once however as we shall see the difference in communication cost me can be so great that it strongly indicates for the two job method so I want to do the",
    "1260-1280": " comparison of the communication cost using one or two MapReduce jobs we already know what it is for one job as a function of the reducer size Q we deduced that the replication rate was 2 N squared over Q and there are 2n",
    "1280-1300": " squared input so the total communication is the product of the replication rate and the number of inputs or 4n to the fourth power divided by Q we need to analyze the to job method we'll do it in terms of the parameter G the number of groups of rows of the first matrix later will relate G and Q so we can get a comparable function for communication in",
    "1300-1320": " terms of n the matrix size and Q the reducer size drop 2 is easier to analyze first we'll assume that the mappers for the second job execute where the data is so there is no communication of data between jobs the communication cost of the second job is 2 and squared times G",
    "1320-1340": " it is the product of three factors N squared over G squared is the number of squares into which the result matrix is partitioned when G is the number of groups and each square has G squared",
    "1340-1360": " elements and two G reducers contribute to each square that explains where the two and squared G comes from now let's figure the communication cost of job one there are 2n squared input elements and",
    "1360-1380": " recall that each input is sent to G different reducers where G is the number of groups that means job one uses 2n squared G communication exactly the same as the second job incidentally that was no accident we picked the two-two-one aspect ratio",
    "1380-1400": " of the rectangles exactly so the two jobs would require the same communication and this total communication is a minimum if we had done the more natural thing of dividing input matrices into squares we would require about six percent more total communication so the total communication of the two jobs is for N squared times G",
    "1400-1420": " now we need to relate G and Q so we can get rid of G and express the communication in terms of N and Q only in the first job each reducer gets two rectangles each is n over G by n over 2g which explains this term and the factor too is there because",
    "1420-1440": " there are two rectangles input to every reducer so Q is N squared over G squared or reversing the relationship G is n over the square root of Q when we replace G in the total communication we",
    "1440-1460": " get for n cubed over the square root of Q remember that for the single job method the communication is for n to the fourth over Q if Q equals n squared those are the same but for Q smaller than N squared the two job approach wins out for example if Q is to n the minimum",
    "1460-1480": " possible for the one job method then the one-drop method takes communication to n cubed while using two jobs keeps the communication to two square roots of two n to the 5 halves"
}