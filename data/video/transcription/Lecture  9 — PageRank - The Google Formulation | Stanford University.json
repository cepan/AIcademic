{
    "0-20": " so so far we looked at the PageRank formulation we looked at the linear algebra formulation and we now looked at the random walk kind of intuition of PageRank and we in the in the last slide of previous lecture we just said that the under certain conditions this PageRank reactor will be unique so now",
    "20-40": " the question is what are these certain conditions that matrix M has to satisfy in order for the PageRank to exist and to be unique and now what we will learn is basically we learned the real the Google formulation of the of the PageRank algorithm so what we know so far is that the importance of a page J",
    "40-60": " in in a web graph is simply the sum of the importances of pages I dot point to it and when when we sum these things together we divide them by the out degree of of the of the source page and we what we established is that this equation that I have here is simply can",
    "60-80": " be written as a matrix equation so now there are three questions we need to answer first does this m equal R equals M times R does this converge second does it converge to what we want and third are our results reasonable so what we'll do next is we'll answer these questions one by one so here is the first question",
    "80-100": " does this converge imagine a very simple graph only two nodes and node eight points to node B & B points back to a and now imagine we want to run our power iteration and as I as I showed in the in the last slide we said that the PageRank vector is unique and it the stationary",
    "100-120": " distribution will always be reached regardless of how we initialize our initial vector so now imagine I we initialize our our vector R R at R at time 0 to be simple to have to have two values to have value 1 on the coordinate a and have value 0 and coordinate B now",
    "120-140": " when we are multiplying M times R what will happen is the score of a gets passed to B and score of B gets passed to a so at the next time step the coordinates will flip and now when we multiply again the coordinates will flip again right so what we see here is that we will never converge all the all that is happening here is that the score of one gets",
    "140-160": " passed between a and B and score of zero gets passed between B and a so it seems that our our PageRank computation will never converge this problem is called the spider trap problem I will explain it a bit more later so so far it seems that our method for computing PageRank",
    "160-180": " the way they find it so far doesn't really work so we looked at the spider trap problem that I'll talk more about later and here is another problem with the current formulation of PageRank and this is called the dead-end problem and here the the thing is even a simpler graph than what we had before the breaks our algorithm so let's consider a very",
    "180-200": " simple graph two nodes one edge kind of couldn't be simpler and let's think of the same initialization vector as we had before so a starts with score one B starts with score zero so what what happens in this case is in the first multiplication with matrix M the score the scores gets flipped but now what",
    "200-220": " happens in the second step of multiplication is basically the score of 1 gets lost right as a B is not able to pass the score to anyone else so the score get lost and we converge to this vector of zeros which is which is a problem so what we will do now is actually talk about these two problems",
    "220-240": " of spider traps and dead ends in in more detail and develop solutions for them so to summarize there are two problems with how we defined PageRank so far the first problem is the problem of dead ends so basically the idea is that dead ends are these web pages that have no outgoing links so what will happen is that",
    "240-260": " importance of these pages will leak out right the idea is that basically whenever a web page receives its page rank score and then there is no way for a web page to pass this PageRank score to anyone else because it has no out links this page rank score will leak out from our system and at the end the page rank scores of all the web pages will be",
    "260-280": " zero as we saw in the previous example so that's what is called the dead end problem and then the spider trap basically the idea is that here out links from webpages can form a small group so the idea is that the if you think of a random walk interpretation of PageRank basically the random worker will get trapped in a small part of the",
    "280-300": " web graph and then the random Locker will get kind of indefinitely stuck in that part and at the end those pages in that in that part of the graph will get very high weight and every other page will get very low weight so this is called the problem of spider traps so what we will do next is we actually",
    "300-320": " develop a solutions to both of these problems so let's look at the two of the problems that we that we just discussed in a bit more detail so first is the problem of spider traps on a bit more complicated graph so here is a variant of the graph we have from our initial investigation of PageRank the three nodes Y a and M and in this case M is a",
    "320-340": " spider trap which means M has this self loop so whenever a random worker gets to know them it basically gets stuck to it in this infinite loop because there is no other way out of know them and all that and Walker can do is infinitely walk this self loop from M so now think",
    "340-360": " of the stochastic matrix M that we have here on the right and the question is what happens to the power iteration as as we run it and multiply our R with the matrix M so here is the example I have my vector R I initialize",
    "360-380": " it as we as we said to one over the size of the graph so one third on every component and I start multiplying with matrix M what will happen is that at the end here is the result we will obtain basically the importance of node M will be 1 and the importances of both other nodes will be 0 if you think about the",
    "380-400": " the random walk interpretation of PageRank this result is very much expected right if we think of a random walker browsing this 3 node graph and we ask after lots and lots of time where do we think the random Walker will be basically the random Walker will be stuck at no",
    "400-420": " with probability 1 what this means is basically wherever the random worker starts for some time the random worker will be able to walk between Y nodes y na but as soon as the random walker crosses the edge to M it will be stuck in this infinite loop and will never be able to move anywhere else so this means that basically all the PageRank scores",
    "420-440": " or will bet will be concentrated at node M which is what exactly what happens so in some sense in this case the page ranks the PageRank very nicely converged to some vector but it converge to something that doesn't make much sense right so all the all the importance gets concentrated into into this single node",
    "440-460": " and both nodes Y and a have importance of zero so this is the problem of spider traps is that they lead to results that are not intuitive or not what we want so now how to solve the problem of spider traps is to slightly modify our random",
    "460-480": " walk way of thinking about PageRank right so the way Google solved the solution to the spider traps is to say that at each step the random walker has two choices with some para with some parameter beta with some probability beta the random walker will follow the the outgoing link at random so the same",
    "480-500": " as the random walker was doing before but with some remaining probability the random walker will randomly jump to some other random webpage right so the way we can think of this now is that we have a random surfer that whenever a random surfer arrives to a new page flips a coin with and if the discount says yes the random Walker will pick another link",
    "500-520": " at random and walk that link and if the coin says now the random Walker will randomly teleport basically jump to some other random page on the web right so this means that the tallip that the random surfer will be able to jump out or teleport out from a spider trap within only a few time steps right after",
    "520-540": " a few time steps the the coin will say yes let's teleport and the random surfer will be able to jump out of the out of the trap so if you think about this in terms of the graph here is our graph from before with node M being the spider trap so what we can think of it of this now is that we have these additional links that",
    "540-560": " basically have a with small probability the random surfer can teleport out of of any node at any given time so this means spider traps are no problem anymore so everything is good the problem we still have is the dead ends so let's understand the dead dead ends problem a",
    "560-580": " bit better so the problem with dead ends was the following was basically that these are the pages that have zero out degrees so the PageRank score does not get distributed to any other page in the graph because they don't point to anyone else so going back to our three three node graph example in this case node M",
    "580-600": " is the dead end because the out degree of node n is zero okay so now what we see if you look at the structure of our matrix M the first thing we notice is that our matrix M is not stochastic anymore right so our columns don't sum to one in particular the columns of for node M do",
    "600-620": " not sum to one the reason for that is because no dam has zero out degree so it will be all zeros in that column of matrix M so if we look at our set of equations what we used to have before was now is that basically the the N does not are of m does not does not appear in",
    "620-640": " any of the equations and now if we start with we would go and run our power iteration here is here is basically what happens we again start with the vector of one thirds kill keep multiplying with M in the first iteration second iteration third iteration and after a",
    "640-660": " while all the our vector basically converges to all zeros so basically it would say that all web pages in this graph can be importance of zero which is again not what not what we want and basically the problem is that whatever is the PageRank score of off node M no dam is not able to pass this PageRank",
    "660-680": " score to any other node in the network so that PageRank score kind of leaks out of our system so the question is how do we of the the problem you just observed with that 10v dead ends so the the way we solve the problem is to basically say the following what we say is that if a node has no outgoing links then when we",
    "680-700": " reach that node we will teleport with probability 1 so this basically means that for example whenever whenever we reach node M we will always jump out of it and uniformly at random and teleport somewhere else so if you think what does this do to our stochastic matrix m and the column corresponding to know them",
    "700-720": " what happens is the basic now column 1 will have will have values of 1 over 3 for all all the all its entries what does this mean is basically whenever a random self surfer comes to N it teleports out and with probability 1/3 lands to any any other node in the graph so this is again the way using the",
    "720-740": " random jumps or random teleports how we solve the problem of dead ends"
}