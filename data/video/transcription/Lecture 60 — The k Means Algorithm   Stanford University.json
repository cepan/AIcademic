{
    "0-20": " hello everyone welcome back to mining of massive datasets we're going to continue our discussion of clustering today by looking at the k-means algorithm recall that we previously looked at another mean way of doing clustering called hierarchical agglomerative clustering now that algorithm work really well in terms of producing clusters but unfortunately it doesn't work very well for large datasets because of its",
    "20-40": " computational complexity the k-means algorithm or the k-means family of algorithms actually is a way of addressing that problem of creating algorithms that are computationally tractable and work with very large datasets now the k-means algorithm assumes a euclidean space and a euclidean distance",
    "40-60": " and the first step is to pick k in the number of clusters now for now let us assume that we pick a number k and say that's the number of clusters that we finally want to at the end alka i'll show you how to actually pick this this value k for now let's just assume that K is a given now we're going",
    "60-80": " to initialize our K clusters by picking one point per cluster for example we could just pick K points at random one and assign one point to each cluster and that would be the one way of picking the points and later on we'll examine other ways of doing this much better now that",
    "80-100": " we have a cluster is populated with these K points picked at random here's how we going to proceed we're going to go through all the points in our data sets and for each point we're going to place it in the cluster to Hooves centroid it's closest so we're going to find the cluster with the closest",
    "100-120": " centroid to the data point and then we're going to assign that data point to that cluster and we're going to do this for each data point now when we assign a whole bunch of new data points to a cluster the centroid of the cluster might change because of all the new data points that have been added to the cluster so we're going to update the",
    "120-140": " locations of the centroids of each of the K clusters by taking into account the new data points that I've been added to those clusters now once we do this we will find that the centroids of the K clusters moved and now a point was close to one cluster might be closer to another cluster so we're going to go through and reassign all points to their closest",
    "140-160": " centroid and sometimes this moves points between the clusters now notice that we may have to do point you know steps two and three again and again and it is exactly what we're going to do we're going to repeat steps two and three until convergence and what we mean by convergence is that the case centroids don't move any further and the points",
    "160-180": " don't move any further across the across the centroids at that point we have a stable k-means clustering so let's look at an example so here's a set of points and we're going to arbitrarily say that K is equal to two so we're looking to find two clusters in this data set now",
    "180-200": " in round one I'm at random going to pick two points since k equal two and call those a centroid with two clusters let's say I pick these two points so the the point that I've highlighted with the in pink here and the point that I've highlighted in blue here as the two centroids Iping these totally",
    "200-220": " arbitrarily and at random now what I'm going to do is I'm going to go through all the data points and for each data point I'm going to assign it to the centroid that it's closest to so observe that this point for example here is close to the red cluster so I'm going to assign it to the red cluster whereas",
    "220-240": " this point here is closer to the blue cluster than to the red cluster so I'm going to assign it to the blue cluster and you know when I go through all the data points and do this here is the clustering that I end up with these two points here end up in the you know in the pink cluster here and this set of",
    "240-260": " points are closer to the the blue point here and so they end up in the blue cluster so that's round one of k-means clustering now what I'm going to do now is because I've created these two new clusters I'm going to compute their new centroids and when I compute the new centroids the the centroid of the pink",
    "260-280": " cluster moves over a little bit to the left to this new position here and the centroid of the blue cluster moves to that new position now because I have new centroids I'm going to go through in round two I'm going to go through each data point again and assign it to its closest centroid and when I do that observe that point for example this",
    "280-300": " point here is now closer to the to the pink cluster than it is to the blue cluster and so it moves from the blue cluster to the pink luster okay and so we have this new clustering at the end of round two now that gives a new clustering I'm going to recompute the centroids once again and I recompute the",
    "300-320": " centroids the the centroids migrate further and because centroids are migrated further I have to recompute the the assignment of points to the centroids again and when I do that these two points here then are closer to",
    "320-340": " the the pink luster so they move to the to the pink luster and and we end up with the clustering shown here at the end of round three okay now as it turns out in this case round three is the last round at this point of the centroids and the points don't migrate any further and",
    "340-360": " we have a stable clustering so what we have here is the clustering the k-means clustering for k is equal to 2 4 for this data set observe that I started from a completely random assignment of you knows of centroids and I actually migrated the centroids to where they",
    "360-380": " finally ended up now an important question here is how do we pick the right value of K in the previous example we arbitrarily pick K is equal to 2 and that actually turned out to be the right value for that data set because you can see that there are roughly two different clusters of data points here but a",
    "380-400": " general how do we know what the right value of K is up front so that we can pick the number of clusters up front so since we don't know the right value of K the obvious answer is to try different values of K and see what looks good now the question nowhere to decide what do you mean by looks good one obvious answer is to look",
    "400-420": " at the average distance of points from the centroid as K increases all right and let's see let's look at an example to see what we mean here so here's here's an example with a bunch of data points and they've actually picked K is equal to two because if pick k equal to we have two clusters and you can see",
    "420-440": " that that's a centroid of the first class turn you can see there's a lot of points there are very far away from the centroid in this case right now if K is too small as in this case then there are going to be lots of points that are a large distance away from the centroid so the average distance from the centroid is going to be fairly large now amid K",
    "440-460": " equals three and as it turns out this is the right value for this data set and you can see that the distances to the centroid shrink quite a lot when I went from k equal to 2 k equal 3 that splits the top fluster into two and that",
    "460-480": " shrinks the average value or the or the average distance of centroid significantly right and the and but if I increase K further let's say I make K equal 4 and that splits that cluster further now that does shrink the average distance of centroid but not as much as when we went from k equal to 2 k equal 3",
    "480-500": " alright so when we make K 2 large it does decrease the average in the centroid but not by not as much now you can see this very clearly if I plot a graph right if I plot a graph where the x-axis is K the number of clusters and the y-axis is the average distance to the centroid you can see that as K",
    "500-520": " increases the average distance to the centroid keeps falling but at some point it's it's you know it is there's an e in the curve and the average distance to the centroid falls only very slowly the obvious thing to do is to pick a value of K that's close to the knee of the curve where you get a more you know fairly low distance to the average",
    "520-540": " distance of the centroid without too high a value of K so in this case the best value of K is the one that that's shown in this in the picture the final question we need to address with k-means clustering is the picking of the K initial point to initialize the K clusters in the",
    "540-560": " examples that we've seen so far we've picked that the initial K points and are completely at random now that worked out well in our example but in general it may not work out quite so well we might for example pick K points that all happen to be in the same cluster in which case the final clustering won't reflect the actual clustering of the data right or we might pick points that",
    "560-580": " are outliers that are not near any of the real clusters so the the final clustering depends on the initial K points that you pick whether so it's important that we pick the right K points to start the clustering from the first approach to picking the initial K",
    "580-600": " points is sampling so remember the data set is really large and so we can't actually run a complicated algorithm like hierarchical clustering on it but what we can do is to sample the data and take a smaller sample of the data and then using the sample of the data we can run another algorithm like hierarchical agglomerative clustering which we",
    "600-620": " covered in the last lecture and we can run that algorithm untii we obtain K clusters and then we can pick a point from each of the K clusters for example we could pick the point that closes to the centroid for each cluster and we could call those our initial K K centroids another approach is to not",
    "620-640": " resort to another clustering algorithm but to just pick a dispersed set of points points that are as far away from each other in the data set as possible so one approach to this is to first pick a the first point entirely at random now the second point we pick to be the point that's as far away from the first point",
    "640-660": " as possible in the data set the third point we pick to be the point that is as far away from both point one and point two as possible in general we pick you know once we picked the first few points we pick the next point to be the one whose minimum distance from the already selected points is as large as possible and then we repeat this until you pick K",
    "660-680": " points so this ensures that the key points that we pick at as far apart or as dispersed as possible you know in the in the data set and that gives us a better chance of finding the right clustering let's",
    "680-700": " consider the complexity of k-means clustering now each round we have to examine each input point exactly once we have to find the closest centroid to that point and assign it to that centroid now since there are n points and there are K centroids right we have",
    "700-720": " to compute the distance of each point from each centroid so the algorithm is order K times n for n points and K clusters now each round is is is this of complexity K times n now this is actually not bad because you know when n is very large and K is fairly small we",
    "720-740": " have a linear algorithm in n each town is actually linear in n but the real problem is that the number of rounds to convergence can be really really large there is actually no theoretical limit on the number of rounds - for the algorithms to convert to converge and the number of rounds can be really really large so the algorithm could",
    "740-760": " spend actually a really long time getting to getting to convergence so the question is if the data set is really really large and we don't want to go through this large number of rounds can we actually do something like k-means clustering in a single pass over the data because we have really large data",
    "760-780": " set and we don't want to scan it multiple times we're going to answer this question in the next segment"
}