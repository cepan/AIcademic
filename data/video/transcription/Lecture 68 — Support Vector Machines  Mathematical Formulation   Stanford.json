{
    "0-20": " so now given that we know what we want to do the question is how do we define the support vector machines classifier so intuitively our goal is to separate kind of pluses from minuses right we want to separate spam email from non-spam email and the way we will do this is the following we will think that we are given a set of training data examples right this is just a set of pre",
    "20-40": " labeled emails right every email has a set of features plus the corresponding label whether it's spam or not and the way we can think of this is that for every example right for every training example for every email we have the feature vector that is real valued and where the squares of the features sum to one so it has a Euclidean length one and",
    "40-60": " simply we can think that in in this case these features tell me whether certain words are present or not present in the document and the Y is simply the class label whether this is spam or not and then as we said before the way we will do our classification we will take this weight vector W and do a product with",
    "60-80": " the feature vector X to make the classification so now the question is right what is the best line in this case that separates out pluses from minuses in the line as I will demonstrate later is actually defined by the way the weight vector W and for example in this",
    "80-100": " small and small case that I have here there are many possible lines that separate pluses and minuses so the question is which among these three different lines is the best one so in order to decide which of the three different lines are separating hyperplanes is best to use let's look at",
    "100-120": " this simple example so we have non spam and spam and we have our separating line between spam and non-spam now the intuition we would like to build here is that the distance from the separating line tells us how sure or how confident are we in a prediction for example if we",
    "120-140": " ask how confident are we that this email a is not spam that would be very very easy it's very far away from the decision plane while for example for this for this email see we are not entirely sure right it's very close to decision boundary so it could as well be on the other part and be part of the minor labeled as - so in some sense what",
    "140-160": " we want to the intuition we want to explain here is to say that the the confidence in our classification is the distance of the point from the separating hyperplane so what we want to do now is we want to find the hyperplane that has what is known as largest margin right so what we want to do is to find the line that separates pluses from",
    "160-180": " minuses the most so here is a simple example the way we define the margin we define the margin as a distance of the closest example from the decision boundary right so for example on the case on the Left if this is our decision boundary then the margin of this",
    "180-200": " decision boundary is very small right very quickly we hit - we hit a data point while for example using the same data here is a different decision boundary that has much larger margin right now here margin gamma is much bigger meaning the distance to the closest data point is much larger than",
    "200-220": " on the case on the left the reason why we define the margin in a way that we say what is the distance to the close of the closest data point and maybe not what's the average distance or something like that is because this kind of makes sense in intuitively and also there is a mathematical or machine learning theory",
    "220-240": " that says that this is the right way to go about this and to define the margin as the distance of the closest example from the decision boundary so now we will ask why is maximizing margin gamma a good idea and to do this we really want to first compute what is what is the value of gamma how can we compute",
    "240-260": " the value of the margin so the way we will reason about this is that first means a simple fact from linear algebra and it's a fact about the dot product right so if you think about what is the dot of the dot product between vectors a and B then the value of the dot product is simply the length of vector a times the length of vector B times the cosine",
    "260-280": " of the angle between the two vectors so here is a simple picture illustrating this right I have a vector B I have a vector a and then the way I can think of the dot product is simply that I take a and project it down to B and then the length of this projection projection is simply length of a time's",
    "280-300": " the cosine of the angle between the two vectors and now I take this thing and multiply it with the length of B so how can we think about the dot product or the inner product between two vectors we can think of it in a sense of what is the length of the projection of vector a down to the vector B and we measure this in the",
    "300-320": " units of the length of vector B so this is important because we can now go and actually compute what is the margin of a given of a given data point so let's think about it the following right we have our line we'll call it capital L we simply define this line as W times X",
    "320-340": " plus B equals 0 and we all do also this this cross here is the coordinate origin then another thing we have is that let's have this vector W that simply basically defines the line and it has two coordinates w1 w2 right now let's pick",
    "340-360": " some data point a that has coordinates X sub a one and X sub a two right now what our goal is to ask is what is the margin of this data point how far is this data point from the separating plane so how do we compute that let's first think of the location of the a simply as a vector",
    "360-380": " pointing from the coordinate origin up to the point a and simply let's think of define another point M that is a point somewhere on the line it doesn't matter it's an arbitrary point on this line L ok so now the question is how far what is the distance between the line",
    "380-400": " and the point a so what we would like to do is we would like to find out what is the distance between the point a and the line the green line L and the way the way we compute this distance is simply this is kind of the distance between the point a and the point H that is kind of closest to the point a and H",
    "400-420": " to be on the line so the way we can think of this is the following right so if you think about what is the vector between a to M right the the way we compute this Brown vector is to simply say this is a minus M so we get we get the vector and now we have to multiply this vector with W right in a sense that",
    "420-440": " if you think about this multiplication this will simply say if we project if you project our vector a minus M on W what is its length so this is this is all all all okay and exactly as we want but now we can start and expand the difference of vectors a and M so if we",
    "440-460": " do this we get X by 1 minus X sub m times W 1 plus X sub 2 minus X sub M 2 times W 2 and now if we distribute the W inside there is one important thing we have to remember right we remember that that point M belongs to a line so how is",
    "460-480": " the line defined the line is defined up here which basically means that x times W X sub M times W 1 times X sub a X sub M 2 times W 2 equals minus B so if we take these terms then and sum them together their value should be minus B so this means that kind of these terms",
    "480-500": " cancel out and they can be replaced with the value of B so the overall the overall value is X sub a 1 times W 1 plus X sub a 2 times W 2 plus B given the fact that we mentioned below now we can look at what is the value of this expression and this expression is exactly W times a plus B which basically",
    "500-520": " means that the distance between the point a and the line L is simply the vector W multiplied by the coordinates of point A plus this offset term B so what did we just do we see that the margin or the distance is exactly the dot product of the vector W that is",
    "520-540": " perpendicular to the line times the location of that point so now that we know what is the value of the margin we can already start formulating the problem of how do we find the best vector W how do we find the separating separating plane that has the largest margin so the way we do this is very",
    "540-560": " simple right we already know how we will be making predictions we simply take our vector W multiply it went by X and B to it and then based on the sign of off this value we either make a positive prediction or a negative prediction so now we also just computed what is the",
    "560-580": " confidence in our prediction right confidence is simply the value the value of this whole term right and now so in some sense for if we want to predict what is the our confidence in prediction of the i data point kind of what is our margin for data point i our gamma of data point i is simply the confidence in",
    "580-600": " our prediction times the class of that prediction the reason why we are multiplying here with y i is very simple because if our confidence is negative and the class is really negative this means our overall confidence will be something positive and if the two parts",
    "600-620": " of this expression have nice matching signs then our then our confidence will be negative which means we made a miss classification okay but now given this in mind we can simply say what do we want to do we want to find find W right we want to find the separating hyperplane we want to find the decision boundary subject over all our training data the margin of that given data point",
    "620-640": " is as large as possible so we want to find W where the smallest smallest margin is as large as possible so we can write this down as an optimization problem and and say that our goal is to find gamma such that for every training",
    "640-660": " example the the margin of the straining example is at least gamma right we want to find the largest possible margin such that all the all the training examples have margin greater than gamma and by fine by finding such gamma we are implicitly finding the W that achieves this large margin so very simply what is",
    "660-680": " support vector machine doing as we have done it right now it's saying I want to find the work W such that it maximizes the margin where here is a set of constraint that simply say the margin is simply the smallest value where every every our confidence in every prediction is at",
    "680-700": " least that value so to summarize what we learned so far about the support vector machine we learned that the way the method works is that we want to find a separating plane or the decision boundary that maximizes the margin we said that this is good both according to intuition as well as according to theory in particular the theoretical argument",
    "700-720": " is based on what Nick Sherwin Lincoln's dimension and also it works well in practice so the optimization problem that we to find the W that we identified so far simply says we want to find W such that our confidence in prediction over all the training example is at",
    "720-740": " least gamma so now the whole the whole exercise will be how do we go and find such W that maximizes our value of the margin the value of gamma"
}