{
    "0-20": " we're now going to take up a particular problem that has a very non-trivial solution we assume the stream elements of bits zeros are 1 and we want to know how many bits in the last n or 1 if we can store the most recent n bits we could use a solution like the one discussed for averages in fact the algorithm would be even simpler however",
    "20-40": " we're going to address the situation where n bits don't fit in main memory perhaps n is a trillion or and as a reasonable number but there there are so many streams that we can't store complete windows for all if we want exact answers then we can show that it is impossible to do anything better than to store the entire window however what",
    "40-60": " is interesting is that we can store on the order of the square of log and bits where n is the window size and still answer queries about the count of ones with answers that are off by at most a small factor as small as we like if we do enough work the problem we're going",
    "60-80": " to discuss is this we're given a stream of zeros and ones at any time we have to be prepared to answer a query of the form how many of the last K bits were one here K can be any integer from 1 up to some large upper limit n we can",
    "80-100": " surely do this if we use a window size and and store the last n bits when a new it arrives we throw away the oldest bit in the window since it can never again be useful to answer one of these queries but one disadvantage of this approach is",
    "100-120": " that answering one query requires that we examine K bits since K can be quite large and both inputs and queries may be arriving very rapidly that may be time we cannot afford another potential problem is that we may not be able to afford the space as we just mentioned we could be trying to handle a large number",
    "120-140": " of streams or n could be so large that even one window does not fit in main memory both these concerns suggest that we should consider a method that uses less than n space and it also allows us to answer queries about the last cabe it's much faster than um the order of K it turns out that",
    "140-160": " we can't get an exact answer to queries without using n bits in the window but we can get close using much less space than o of n and also much less time than o of K we're going to introduce the right algorithm with a discussion of something that seems like it should work but doesn't quite our goal is to be off",
    "160-180": " by no more than a factor of two and estimating the number of ones in the last K bits so we will summarize blocks of the stream those blocks will have exponentially increasing length that is 1 2 4 8 16 so on and the summary of a",
    "180-200": " block will be that simply the count of the number of ones in that block when we want to know the count of ones for the last K bits we can find some blocks that lie wholly within the last K bits and we add up their counts it is only the last block the one furthest back in time that gives us pause we don't know how many of",
    "200-220": " its ones are within the last K bits so we have to guess but if we created these exponentially growing blocks for all time units then there would be as many blocks of length one as there are bits in the window as well as blocks of size 2 4 8 and so on so that saves us nothing",
    "220-240": " instead we have to drop blocks if their left end that is the end that is earliest in time coincides with the left end of a larger block and we also drop a small block of there's a larger block completely to the right that is later in the stream as a result you never have",
    "240-260": " more than 2 blocks of any one size so here is an example of the blocks we might retain at some time the 5 rows of blocks are of length 1 2 4 8 and 16",
    "260-280": " okay there are two blocks of length one the more recent has a count of zero because it consists of a single zero that's this while the other has a count of one because it consists of a single",
    "280-300": " one okay here's a block of length two it has a count of one because it represents zero one that is these two bits notice that we've previously deleted the block of length one that would go here because",
    "300-320": " it begins at the same point as the block of length two above it also all other blocks of length one are deleted because they have a block of length two completely to their right we also show a second block of length to its count as",
    "320-340": " two because it represents this one one there are two blocks of length four and they have counts of two and three they represent well this guy represents this",
    "340-360": " sequence 0 0 1 1 so it has two ones this represents 1 0 1 1 and therefore it gets a count of 3",
    "360-380": " we see one block of length 8 its count is 4 now let's see because it represents these 8 bits and notice that that the",
    "380-400": " count for a second block of length 8 is not needed because we can figure out it has 6 ones since that's a 10 that 6 is the difference between the number of ones in this block of length 16 and that block of length 8 or that is 10 minus 4",
    "400-420": " equals 6 so if this block existed it would have surely have 6 once okay now suppose we get a query for how many ones",
    "420-440": " there are the most recent 28 bits we can add up the counts of certain blocks some little blocks at the right end and some bigger blocks going to the left we want to pick blocks so that each of the most recent 28 bits is covered by exactly one of the blocks we choose so we pick this",
    "440-460": " block of length 1 ok this block of length 2 this of length 4 we don't want this block of length 8 because we have this block of length 16 and that's still all within the last 28 so so far we have",
    "460-480": " covered 23 bits and we know that among them the number of ones is 0 plus 1 plus 2 plus 10 which is 13",
    "480-500": " okay but what do we do about the oldest five bits we that is there these bits now if we could see the bits we would know that they're zero zero one zero one and therefore they have two ones but we don't see them all we see is that they",
    "500-520": " are part of this block of 16 and we know that block has a count of six okay but we can't tell how many of those six are in the most recent five positions again we don't ever get to see this anymore now if we could see them of course we",
    "520-540": " know there were two and that the right answer is 15 but we need to estimate without seeing how many ones there are in this region okay if we guess that half the count of the block that is 3 6",
    "540-560": " divided by 2 in this case is is in the blood and the region we we don't see then we would guess 16 and that's only off by 7% so that's not even bad we could even try a proportional guess that",
    "560-580": " is say we know that there are 6 within 6 divided by 16 well 6 divided by 16 is the probability that any given bit is 1 in in this in the range represented by this by this block of 16 and we know",
    "580-600": " that we have to count five of them so that's 30 divided by 16 which is roughly 2 and so if we guess 2 and out of that we're to get 15 and that happens to be right on the mark even though we didn't get to see the the the 5 bits that we",
    "600-620": " wanted to count those the strategy has a lot to recommend it first it stores only the square of log n bits I might comment that we use log we use this expression log",
    "620-640": " super to n to mean the square of log n okay this is a common expression you don't want to write it as log n squared because that's a really to law to log n which is not what we mean so I should if",
    "640-660": " you've never seen this notation before again putting the square above the log means that you're actually squaring the whole thing the squaring log in okay now as I said it's okay square is storing",
    "660-680": " square of log n bits is not that bad okay it's much less than n for for for large and so if n is a billion then log squared n is about 900 now why do we need only on the order of log squared n bits well first of all if the window",
    "680-700": " size is n bits we never need any blocks of length greater than n an account up to n can be stored in log base two of n bits and how many counts do we need well they're only log base two n block sizes from 1 1 2 4 8 16 and so on up to the",
    "700-720": " largest power of 2 there's no law or no larger than n so we never store more than 2 blocks of any size and as a result we need to store at most 2 log n counts of at most log n bits each and that's 2 log squared n",
    "720-740": " another good thing is that after each bit we do a limited amount of work we have to create a new block of length one for each of the lengths one two four eight and so on we may have to drop a block of that length or we may have to combine two blocks of one length into",
    "740-760": " two blocks of the next larger length but that means that most order log and work total since there are log n sizes and the error is frequently small it can't be bigger than the count of the biggest block the one that is only partially in the region we're counting there's a",
    "760-780": " problem with the scheme however when the ones are distributed evenly among all the regions of the stream the number of ones in the ambiguous region can't be more than half the total number of ones in the region we want to count so our error is limited to 50% but look what happens if all the ones in the region we",
    "780-800": " want to counter at the left end and in particular are counted only by a block that is partially within the desired region then the true count could be anything from 0 up to the full count of that block anything we guess could be wildly wrong and we'll never know whenever going to discuss a similar",
    "800-820": " algorithm that preserves the good and avoids the problem with uneven distribution of ones we still divide the window into blocks but instead of letting each block cover a fixed segment of the stream we'll let each block cover a fixed number of ones the sizes of the blocks will still be limited to the powers of 2 that is 1 2 4 8 and so on",
    "820-840": " but the notion of the size of a block changes now the size of a block will be the number of ones so we'll have blocks of size 1 to represent segments in the stream that have a single one blocks are twice that size we'll represent two ones and any number of zeros and then there will be blocks representing four ones in",
    "840-860": " any number of zeros and so on the advantage of this scheme is that if there are a few ones in the recent stream the block size is covering that region will stay small they will cover large parts of the stream while their size number of ones remains limited I decided",
    "860-880": " to call the algorithm I'm going to describe the D GIM algorithm the initials refer to the four guys who invented this algorithm algorithm my ear that or iris Yanis pur Indyk and Rajeev Motwani and in fact this is a good time to stop and remember Rajeev Motwani who",
    "880-900": " died shortly after this algorithm was published he along with unison Indyk is also responsible for locality sensitive hashing which forms a major part of this course like our earlier attempt at an algorithm D GIM stores on the order of log squared n bits to represent one n",
    "900-920": " bit window there's an absolute guarantee of no more than 50% error and the answer to any query and if 50% is too much you can reduce the error to anything greater than zero the algorithm becomes more complicated on the number of bits you",
    "920-940": " need to store grows although the number of bits remains proportional to log squared n it's just the constant factor that grows in inverse proportion to the desired error bound to begin the story we need to introduce the idea of a timestamp every bit that arrives in the stream gets a timestamp you might think",
    "940-960": " that we need an arbitrary number of bits to represent the timestamp since there's no limit on how long the stream can be but it's really only necessary to represent timestamps modulo n the window size that is we can divide the timestamp by n and take the remainder the NIT",
    "960-980": " effect is the timestamps start out at 0 1 and so on up to n minus 1 and then go to 0 again 1 2 and so on regardless of where the window is in the stream it's n bits will all have different timestamps we're going to partition the window of",
    "980-1000": " length and into buckets each bucket is represented by a record and records can be stored in on the order of log n bits as we shall see we only need on the order of log n buckets to represent the windows on the order of log squared and bits of physis the record contents are",
    "1000-1020": " the following the timestamp of its end the most recently arrived bit as we mentioned will record timestamps modulo n so we need log n bits to represent the timestamp the number of ones between beginning and end of the segment we call",
    "1020-1040": " this count of ones the size of the bucket however the number of ones in the segment represented by a bucket must be a power of two that explains why we only need log log n bits to represent the count of ones we can store the logarithm",
    "1040-1060": " of the count instead of the count itself since we know that log base two of the count must be an integer the count itself can't be higher than n so its logarithm can't be higher than log base two event since the logarithm is an integer I and we only need log I bits to represent the I in binary log log and",
    "1060-1080": " bits of physis it really doesn't matter much since we still need order log n bits in the record for the bucket just to store the timestamp of its end the partition into buckets must obey the following rules there must be one or two",
    "1080-1100": " buckets of each allowed size up to the maximum size we need remember that allowed sizes are the powers of two no bit of the window is part of two buckets some zeros in the stream may not belong to any bucket it doesn't matter but",
    "1100-1120": " buckets can only increase in size as we go as we go back in time the most recent part of the window is represented by the smallest buckets when the end timestamp of a bucket is more than n time units in the past it no longer represents part of the window so we delete it from the set of buckets whose records are stored",
    "1120-1140": " here's a picture of what the partition of a stream into buckets might look like at some point the most recent two ones are in buckets of size one by themselves this here here further back the previous",
    "1140-1160": " two ones are grouped into a bucket of size two there might be two buckets of size two but they could also only be one as in this case then going further back in time we see the previous four ones in a bucket of size four and the four ones",
    "1160-1180": " before that are also in a bucket of size four then we see two buckets of size 8 and finally a bucket of size 16 the end timestamp of this bucket is still within the window of length n that's this",
    "1180-1200": " although its beginning is outside the window we still need this bucket but any previous buckets have a timestamp that is prior to the beginning of the current window so we have deleted their records so let's see how we manage the buckets as bits arrived in the stream the first",
    "1200-1220": " thing we're going to do is worry about whether we need to drop the oldest bucket we need to keep outside the bucket representation a counter the number of bits that have ever arrived in the stream however we only need this count modulo n so an extra log n bits is all we need when a new bit comes in increment that count of course if the",
    "1220-1240": " count which is n then we set it back to zero that's how modular arithmetic works now look at the end time of the oldest bucket if it's timestamp agrees with the current time then that timestamp is really the current time minus n since we're computing all timestamps modulo n",
    "1240-1260": " the entire oldest bucket is therefore out of the window and we delete its record but if the timestamp is anything else than the oldest bucket still has its end within the window so it remains what we do next depends on whether the bit that just entered is 0 or 1 if it's",
    "1260-1280": " 0 then we make no further changes to the set of buckets that was easy the current input is 1 we have some work to do but the work is at most logarithmic in the window size in first we create a new bucket for the new bit the size of the",
    "1280-1300": " bucket is 1 and it's ending timestamp is the current time there might have been one or two buckets of size one previously if there's only one now there are two and that's fine we're allowed to have one or two of any size but if they were previously two now",
    "1300-1320": " there are three we can't have three buckets of size one so we combine the oldest two into one bucket of size two combining consecutive buckets of the same size is easy we add one to the logarithm of the size and we take the",
    "1320-1340": " end time staff to the end time stamp of the more recent of the two so for example here are two buckets could be of consecutive buckets of any size say two to the X we combine them into one bucket of size 2 to the X plus 1 by simply this",
    "1340-1360": " bucket gets this ending time let's just copy it from here and we add one to the size which essentially says therefore it's does it the size is doubled and",
    "1360-1380": " then we just make that go away but our work might not be over if we had to create a bucket of size 2 we might now have three of that size so we combine the earliest two into one bucket of size four and the problem could ripple through the sizes if we just created a",
    "1380-1400": " third bucket of size four then we could have three buckets of size four we need to combine the earliest two into bucket of size eight and so on but because we're doubling the bucket size each time we passed the problem to the next level after your log and fix ups we've reached a bucket size as large as the entire window and there's never need for a larger bucket okay the rippling effect",
    "1400-1420": " therefore stops after at most log and rounds and each round may takes a constant amount of work so of log n is a guaranteed upper bound on the total time needed to process an incoming one usually the time required is much less and on the average it is constant",
    "1420-1440": " on the slide we'll see the changes that occur as bits into the system so here's the initial state of the window a one enters we create a bucket of size one for it this but now we have three",
    "1440-1460": " buckets of size one so we're going to have to combine the two earliest ones this one and that one okay so here we've done the combination what has happened",
    "1460-1480": " in terms of the records is that the record for this bucket is deleted the size for this record has changed from 1 to 2 and it's time stop has not changed it is therefore actually become this",
    "1480-1500": " record and notice that that one is really inside the record that the slide is not shown perfectly there now I'm showing what happens after another 1 0 1 arrives okay the first of these ones created this bucket and then the 0 came",
    "1500-1520": " in representing changed and then this next one arrives and now we get a third bucket of size 1 so that causes these 2 buckets that get combined into this guy and now we have three buckets of size 2",
    "1520-1540": " so we have to combine these two and by whether that one really belongs in the in them in the middle bucket of size 2 so we combine these two into a bucket of",
    "1540-1560": " size 4 and that made three buckets of size four so these guys got combined into that bucket of size eight but that was a third bucket of size eight so these buckets of size ain't got combined into that bucket of size 16 now",
    "1560-1580": " there can't be more buckets of size 16 there's this one but that extends beyond the end of the the window so we're done rippling changes to larger and larger buckets now I want to explain how to",
    "1580-1600": " query the system so suppose we want to know how many ones there are in the last K bits where K is any integer less than or equal to n the window size first thing we want to do is to ignore all buckets whose ending timestamp is earlier than K bits prior to the current",
    "1600-1620": " time those buckets are all outside the range and we want to count so they make no contribution start by summing the sizes of all the buckets except the oldest bucket that is still in the range we're interested in then add half the size of that bucket okay the reason we",
    "1620-1640": " only add half the oldest bucket size is that we really don't know how many ones from that bucket are still within the range of interest by guessing half we minimize the maximum error as we'll discuss on the next slide so here's why the estimate capping off by a factor of more than 50% from the true answer first",
    "1640-1660": " suppose that the oldest bucket in the range were interested in has sized two to the I we assumed half or two to the I minus one of its ones are among the most recent K bits to arrive the true number could be anything between one and two to the I so our error is upper bounded by two to the I minus one now what's the",
    "1660-1680": " smallest the true answer could be there's at least one bucket of each of the sizes less than two to the I that lies completely within the last K bits these account for at least one plus two plus four plus so on up to two to the I",
    "1680-1700": " minus one and that's two to the that sum is two to the I minus one now we add one for the one that is at the end of the oldest bucket that bucket has an ending timestamp that's within range and",
    "1700-1720": " buckets always end in a 1 so there's a there are at least two to the I ones within the range okay since our error is no more than two to the I minus one that error is at most 50% now we're not going to discuss the",
    "1720-1740": " extensions here but it is possible to modify the algorithm described to limit the error to any fraction we like greater than zero while still using only on the order of log squared n bits to represent all the buckets we need to represent the textbook describes how to do this"
}